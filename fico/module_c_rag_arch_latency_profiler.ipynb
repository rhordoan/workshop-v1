{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module C: RAG Architecture & Latency Profiler\n",
        "\n",
        "**Goal:** Meet the strict 1.5s SLA while maintaining accuracy.\n",
        "\n",
        "**Persona:** Solutions Architect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Module C initialized\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Widgets are optional; notebook works without them.\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "from rag_pipeline import (\n",
        "    RAGConfig,\n",
        "    Timer,\n",
        "    VectorIndex,\n",
        "    build_chunks_from_df,\n",
        "    chunk_text,\n",
        "    make_prompt,\n",
        "    normalize_rows,\n",
        ")\n",
        "from nim_clients import NIMClient, NIMConfig\n",
        "\n",
        "print(\"✅ Module C initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"HF_TRUST_REMOTE_CODE\"]=\"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workshop Setup Notes\n",
        "\n",
        "### Models\n",
        "- **Local generation/judge**: `LOCAL_GEN_MODEL` (default: `meta-llama/Llama-3.1-8B-Instruct`)\n",
        "- **Local embedding**: `LOCAL_EMBED_MODEL` (default: `sentence-transformers/all-MiniLM-L6-v2`)\n",
        "- **Local reranker**: `LOCAL_RERANK_MODEL` (default: `cross-encoder/ms-marco-MiniLM-L-6-v2`)\n",
        "\n",
        "### NIM endpoints (localhost)\n",
        "This lab expects NIMs to be running locally and reachable via HTTP.\n",
        "\n",
        "You can override endpoints/models using env vars:\n",
        "- `NIM_API_KEY` (optional)\n",
        "- `NIM_EMBED_MODEL`, `NIM_RERANK_MODEL`, `NIM_GEN_MODEL`\n",
        "- `NIM_EMBED_PATH` (default `/v1/embeddings`)\n",
        "- `NIM_RERANK_PATH` (default `/v1/rerank`)\n",
        "- `NIM_CHAT_PATH` (default `/v1/chat/completions`)\n",
        "- `NIM_BASE_URL` (default `http://localhost:8000`)\n",
        "\n",
        "### What is measured in the SLA waterfall\n",
        "- The waterfall measures **per-request latency** for query embed/retrieve/rerank/generate.\n",
        "- **Index building** (chunk embedding for the corpus) is cached and is not part of the SLA.\n",
        "\n",
        "### If widgets don’t work\n",
        "Scroll to the bottom for the **NO-WIDGET FALLBACK** cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Real Latency Waterfall (Local vs NIM)\n",
        "\n",
        "This phase runs a **real** RAG request and measures wall-clock time for:\n",
        "\n",
        "- Embed query\n",
        "- Retrieve (vector index)\n",
        "- Rerank\n",
        "- Generate\n",
        "\n",
        "Then it plots a measured waterfall and checks the 1.5s SLA.\n",
        "\n",
        "We run it in two modes:\n",
        "- **Local**: embedding + rerank + generation in-process\n",
        "- **NIM**: embedding + rerank + generation via localhost NIM endpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 69 docs from corpus_runs/llm_richer_n20_20251211_193028/fico_corpus_embedded.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e07fa92dd374a56895db1322f16324d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(Dropdown(description='Mode:', options=('local', 'nim'), value='local'), Button(d…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Load corpus from Module B run (single remaining run) ---\n",
        "RUN_DIR = Path(\"corpus_runs/llm_richer_n20_20251211_193028\")\n",
        "CSV_PATH = RUN_DIR / \"fico_corpus_embedded.csv\"\n",
        "\n",
        "if not CSV_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing corpus CSV: {CSV_PATH}\")\n",
        "\n",
        "df_docs = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Parse list-like columns stored as JSON strings\n",
        "for col in [\"tags\", \"allowed_roles\", \"allowed_tenants\", \"restricted_tags\"]:\n",
        "    if col in df_docs.columns:\n",
        "        df_docs[col] = df_docs[col].apply(lambda v: json.loads(v) if isinstance(v, str) and v.strip().startswith(\"[\") else [])\n",
        "\n",
        "print(f\"✅ Loaded {len(df_docs)} docs from {CSV_PATH}\")\n",
        "\n",
        "\n",
        "# --- Build chunk dataset ---\n",
        "# Use redacted body to avoid leaking sensitive lines during workshops.\n",
        "chunks = build_chunks_from_df(df_docs)\n",
        "\n",
        "# Respect chunk settings via widgets; default chunking above is coarse, so we'll rebuild chunks when chunk_size changes.\n",
        "\n",
        "def build_chunks(chunk_size: int, overlap: int):\n",
        "    out = []\n",
        "    for _, row in df_docs.iterrows():\n",
        "        doc_id = str(row.get(\"doc_id\"))\n",
        "        title = row.get(\"title\")\n",
        "        tenant_id = row.get(\"tenant_id\")\n",
        "        doc_type = row.get(\"doc_type\")\n",
        "        tags = row.get(\"tags\")\n",
        "        body = row.get(\"body_redacted\") or row.get(\"body\") or \"\"\n",
        "        parts = chunk_text(str(body), chunk_size=chunk_size, overlap=overlap)\n",
        "        for j, part in enumerate(parts):\n",
        "            out.append(\n",
        "                {\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": f\"{doc_id}::c{j:03d}\",\n",
        "                    \"title\": str(title) if title is not None else None,\n",
        "                    \"tenant_id\": str(tenant_id) if tenant_id is not None else None,\n",
        "                    \"doc_type\": str(doc_type) if doc_type is not None else None,\n",
        "                    \"tags\": list(tags) if isinstance(tags, list) else [],\n",
        "                    \"text\": part,\n",
        "                }\n",
        "            )\n",
        "    return out\n",
        "\n",
        "\n",
        "# --- Models ---\n",
        "# Local models\n",
        "LOCAL_EMBED_MODEL = os.environ.get(\"LOCAL_EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "LOCAL_RERANK_MODEL = os.environ.get(\"LOCAL_RERANK_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "# Text-only default (works with AutoModelForCausalLM). Override via env var.\n",
        "LOCAL_GEN_MODEL = os.environ.get(\"LOCAL_GEN_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "local_embedder = SentenceTransformer(LOCAL_EMBED_MODEL, device=\"cuda\")\n",
        "local_reranker = CrossEncoder(LOCAL_RERANK_MODEL, device=\"cuda\")\n",
        "\n",
        "# NIM client\n",
        "nim = NIMClient(NIMConfig())\n",
        "\n",
        "\n",
        "def embed_local(texts: list[str]):\n",
        "    emb = local_embedder.encode(texts, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "\n",
        "def rerank_local(query: str, docs: list[str]):\n",
        "    pairs = [(query, d) for d in docs]\n",
        "    scores = local_reranker.predict(pairs)\n",
        "    order = np.argsort(-np.array(scores))\n",
        "    return order.tolist(), scores\n",
        "\n",
        "\n",
        "_LOCAL_GEN = {\"tok\": None, \"model\": None}\n",
        "\n",
        "\n",
        "def ensure_local_generator():\n",
        "    if _LOCAL_GEN[\"tok\"] is not None and _LOCAL_GEN[\"model\"] is not None:\n",
        "        return _LOCAL_GEN[\"tok\"], _LOCAL_GEN[\"model\"]\n",
        "\n",
        "    import torch\n",
        "    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    trust_remote_code = str(os.environ.get(\"HF_TRUST_REMOTE_CODE\", \"0\")).lower() in {\"1\", \"true\", \"yes\"}\n",
        "\n",
        "    # Fail fast on known vision configs (e.g. GLM-4V) which cannot be loaded as a text CausalLM.\n",
        "    try:\n",
        "        cfg = AutoConfig.from_pretrained(LOCAL_GEN_MODEL, trust_remote_code=trust_remote_code)\n",
        "    except KeyError as e:\n",
        "        # Usually indicates your transformers version doesn't recognize a new `model_type`.\n",
        "        raise ValueError(\n",
        "            f\"Transformers in this environment doesn't recognize model_type={e!s} while loading LOCAL_GEN_MODEL={LOCAL_GEN_MODEL!r}. \"\n",
        "            \"Fix: (1) pick a simpler text model like TinyLlama/TinyLlama-1.1B-Chat-v1.0, or (2) upgrade transformers (pip install -U transformers), \"\n",
        "            \"or (3) set HF_TRUST_REMOTE_CODE=1 if the model requires custom loading code.\"\n",
        "        ) from e\n",
        "    except Exception as e:\n",
        "        raise ValueError(\n",
        "            f\"Failed to load config for LOCAL_GEN_MODEL={LOCAL_GEN_MODEL!r}: {type(e).__name__}: {e}\"\n",
        "        ) from e\n",
        "\n",
        "    if cfg.__class__.__name__.lower().startswith(\"glm4v\") or getattr(cfg, \"model_type\", \"\") in {\"glm4v\", \"glm-4v\"}:\n",
        "        raise ValueError(\n",
        "            \"LOCAL_GEN_MODEL appears to be a vision (VLM) model config, which can't be loaded via AutoModelForCausalLM. \"\n",
        "            \"Set LOCAL_GEN_MODEL to a text-only causal LM (e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0) or switch Mode to 'nim'.\"\n",
        "        )\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(LOCAL_GEN_MODEL, use_fast=True, trust_remote_code=trust_remote_code)\n",
        "    tok.padding_side = \"left\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LOCAL_GEN_MODEL,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=dtype,\n",
        "        trust_remote_code=trust_remote_code,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    _LOCAL_GEN[\"tok\"] = tok\n",
        "    _LOCAL_GEN[\"model\"] = model\n",
        "    return tok, model\n",
        "\n",
        "\n",
        "def gen_local(prompt: str, max_new_tokens: int, temperature: float):\n",
        "    import torch\n",
        "\n",
        "    tok, model = ensure_local_generator()\n",
        "\n",
        "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=float(temperature),\n",
        "            top_p=0.95,\n",
        "            max_new_tokens=int(max_new_tokens),\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "def build_index_for_mode(mode: str, chunk_rows: list[dict], use_faiss: bool = True):\n",
        "    texts = [r[\"text\"] for r in chunk_rows]\n",
        "\n",
        "    if mode == \"local\":\n",
        "        E = embed_local(texts)\n",
        "        E_norm = normalize_rows(E)\n",
        "        return VectorIndex(E_norm, use_faiss=use_faiss), E_norm\n",
        "\n",
        "    if mode == \"nim\":\n",
        "        # Batch to avoid request-size limits\n",
        "        embs, _ = nim.embed_many(texts, batch_size=64)\n",
        "        E = np.array(embs, dtype=np.float32)\n",
        "        E_norm = normalize_rows(E)\n",
        "        return VectorIndex(E_norm, use_faiss=use_faiss), E_norm\n",
        "\n",
        "    raise ValueError(f\"unknown mode: {mode}\")\n",
        "\n",
        "\n",
        "def run_rag(query: str, *, mode: str, chunk_rows: list[dict], index: VectorIndex, cfg: RAGConfig):\n",
        "    timer = Timer()\n",
        "\n",
        "    # Embed query\n",
        "    if mode == \"local\":\n",
        "        q_emb = timer.time(\"embed\", lambda: embed_local([query])[0])\n",
        "        net_t = 0.0\n",
        "    else:\n",
        "        embs, net_t = timer.time(\"network+embed\", lambda: nim.embed([query]))\n",
        "        q_emb = np.array(embs[0], dtype=np.float32)\n",
        "\n",
        "    q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
        "\n",
        "    # Retrieve\n",
        "    idxs, sims = timer.time(\"retrieve\", lambda: index.search(q_norm, cfg.top_k))\n",
        "    idxs = [int(i) for i in idxs]\n",
        "\n",
        "    candidates = [chunk_rows[i] for i in idxs]\n",
        "\n",
        "    # Rerank\n",
        "    if cfg.rerank_top_k > 0:\n",
        "        docs = [c[\"text\"] for c in candidates]\n",
        "        if mode == \"local\":\n",
        "            order, _ = timer.time(\"rerank\", lambda: rerank_local(query, docs))\n",
        "        else:\n",
        "            order, _ = timer.time(\"network+rerank\", lambda: nim.rerank(query, docs, top_n=min(cfg.rerank_top_k, len(docs))))\n",
        "\n",
        "        # NIM rerank returns indices into docs; local returns order\n",
        "        if isinstance(order, list) and order and isinstance(order[0], int):\n",
        "            reranked = [candidates[i] for i in order[: cfg.rerank_top_k]]\n",
        "        else:\n",
        "            reranked = candidates[: cfg.rerank_top_k]\n",
        "    else:\n",
        "        reranked = candidates\n",
        "\n",
        "    # Context packing\n",
        "    context = []\n",
        "    used = 0\n",
        "    for r in reranked:\n",
        "        t = r[\"text\"]\n",
        "        if used + len(t) > cfg.max_context_chars:\n",
        "            break\n",
        "        context.append(r)\n",
        "        used += len(t)\n",
        "\n",
        "    from rag_pipeline import Chunk\n",
        "\n",
        "    context_chunks = [\n",
        "        Chunk(\n",
        "            doc_id=str(r.get(\"doc_id\")),\n",
        "            chunk_id=str(r.get(\"chunk_id\")),\n",
        "            text=str(r.get(\"text\")),\n",
        "            title=r.get(\"title\"),\n",
        "            tenant_id=r.get(\"tenant_id\"),\n",
        "            doc_type=r.get(\"doc_type\"),\n",
        "            tags=r.get(\"tags\") if isinstance(r.get(\"tags\"), list) else [],\n",
        "        )\n",
        "        for r in context\n",
        "    ]\n",
        "\n",
        "    prompt = make_prompt(query, context_chunks)\n",
        "\n",
        "    # Generate\n",
        "    if mode == \"local\":\n",
        "        answer = timer.time(\"generate\", lambda: gen_local(prompt, cfg.max_new_tokens, cfg.temperature))\n",
        "    else:\n",
        "        answer, _ = timer.time(\"network+generate\", lambda: nim.chat(prompt, max_tokens=cfg.max_new_tokens, temperature=cfg.temperature))\n",
        "\n",
        "    # Timings\n",
        "    timings = dict(timer.timings)\n",
        "    if net_t and \"network+embed\" not in timings:\n",
        "        timings[\"network+embed\"] = float(net_t)\n",
        "\n",
        "    return answer, context, timings\n",
        "\n",
        "\n",
        "def plot_measured_waterfall(timings_s: dict, title: str):\n",
        "    # Normalize keys into ordered components\n",
        "    keys = [\n",
        "        \"network+embed\",\n",
        "        \"embed\",\n",
        "        \"retrieve\",\n",
        "        \"network+rerank\",\n",
        "        \"rerank\",\n",
        "        \"network+generate\",\n",
        "        \"generate\",\n",
        "    ]\n",
        "    labels = {\n",
        "        \"network+embed\": \"NIM:Embed (HTTP)\",\n",
        "        \"embed\": \"Embed\",\n",
        "        \"retrieve\": \"Retrieve\",\n",
        "        \"network+rerank\": \"NIM:Rerank (HTTP)\",\n",
        "        \"rerank\": \"Rerank\",\n",
        "        \"network+generate\": \"NIM:Gen (HTTP)\",\n",
        "        \"generate\": \"Generate\",\n",
        "    }\n",
        "\n",
        "    parts = [(k, float(timings_s.get(k, 0.0))) for k in keys if float(timings_s.get(k, 0.0)) > 0]\n",
        "    total = sum(v for _, v in parts)\n",
        "\n",
        "    fig = go.Figure(\n",
        "        go.Waterfall(\n",
        "            name=\"Latency\",\n",
        "            orientation=\"v\",\n",
        "            measure=[\"relative\"] * len(parts) + [\"total\"],\n",
        "            x=[labels[k] for k, _ in parts] + [\"TOTAL\"],\n",
        "            textposition=\"outside\",\n",
        "            text=[f\"{v:.3f}s\" for _, v in parts] + [f\"{total:.3f}s\"],\n",
        "            y=[v for _, v in parts] + [0],\n",
        "            connector={\"line\": {\"color\": \"rgb(63, 63, 63)\"}},\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.add_hline(\n",
        "        y=1.5,\n",
        "        line_dash=\"dot\",\n",
        "        annotation_text=\"SLA Limit (1.5s)\",\n",
        "        line_color=\"red\" if total > 1.5 else \"green\",\n",
        "    )\n",
        "\n",
        "    fig.update_layout(title=title, showlegend=False, yaxis=dict(title=\"Time (seconds)\", range=[0, max(2.5, total * 1.2)]))\n",
        "    fig.show()\n",
        "\n",
        "    print(f\"⏱️ TOTAL LATENCY: {total:.3f}s\")\n",
        "    if total > 1.5:\n",
        "        print(\"❌ FAILED SLA. Optimize configuration!\")\n",
        "    else:\n",
        "        print(\"✅ PASSED SLA.\")\n",
        "\n",
        "\n",
        "# -- CONTROLS (optional) --\n",
        "style = {\"description_width\": \"initial\"}\n",
        "\n",
        "w_chunk = widgets.IntSlider(value=900, min=200, max=2000, description=\"Chunk Size (chars):\", style=style)\n",
        "w_overlap = widgets.IntSlider(value=150, min=0, max=500, description=\"Overlap (chars):\", style=style)\n",
        "w_k = widgets.IntSlider(value=10, min=1, max=50, description=\"Top_K Chunks:\", style=style)\n",
        "w_rerank_k = widgets.IntSlider(value=5, min=0, max=20, description=\"Rerank Top_K:\", style=style)\n",
        "\n",
        "w_query = widgets.Text(value=\"Kubernetes incident runbook\", description=\"Query:\")\n",
        "\n",
        "w_mode = widgets.Dropdown(options=[\"local\", \"nim\"], value=\"local\", description=\"Mode:\")\n",
        "btn_run = widgets.Button(description=\"Run RAG + plot latency\")\n",
        "btn_compare = widgets.Button(description=\"Compare Local vs NIM (SLA)\")\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "# Cache indexes per mode + chunk settings so repeated runs are fast\n",
        "_INDEX_CACHE = {}\n",
        "\n",
        "\n",
        "def get_index(mode: str, chunk_size: int, overlap: int):\n",
        "    key = (mode, int(chunk_size), int(overlap))\n",
        "    if key in _INDEX_CACHE:\n",
        "        return _INDEX_CACHE[key]\n",
        "\n",
        "    rows = build_chunks(int(chunk_size), int(overlap))\n",
        "    index, _ = build_index_for_mode(mode, rows, use_faiss=True)\n",
        "    _INDEX_CACHE[key] = (rows, index)\n",
        "    return _INDEX_CACHE[key]\n",
        "\n",
        "\n",
        "def on_run_click(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        rows, index = get_index(w_mode.value, w_chunk.value, w_overlap.value)\n",
        "        cfg = RAGConfig(mode=w_mode.value, top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
        "        answer, context, timings = run_rag(w_query.value, mode=w_mode.value, chunk_rows=rows, index=index, cfg=cfg)\n",
        "        plot_measured_waterfall(timings, title=f\"RAG Latency ({w_mode.value})\")\n",
        "        print(\"\\n--- Answer (truncated) ---\")\n",
        "        print((answer or \"\")[:800])\n",
        "\n",
        "\n",
        "def on_compare_click(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        rows_l, idx_l = get_index(\"local\", w_chunk.value, w_overlap.value)\n",
        "        rows_n, idx_n = get_index(\"nim\", w_chunk.value, w_overlap.value)\n",
        "\n",
        "        cfg_l = RAGConfig(mode=\"local\", top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
        "        cfg_n = RAGConfig(mode=\"nim\", top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
        "\n",
        "        q = w_query.value\n",
        "\n",
        "        ans_l, ctx_l, t_l = run_rag(q, mode=\"local\", chunk_rows=rows_l, index=idx_l, cfg=cfg_l)\n",
        "        ans_n, ctx_n, t_n = run_rag(q, mode=\"nim\", chunk_rows=rows_n, index=idx_n, cfg=cfg_n)\n",
        "\n",
        "        print(\"=== LOCAL ===\")\n",
        "        plot_measured_waterfall(t_l, title=\"RAG Latency (local)\")\n",
        "        print(\"\\n=== NIM ===\")\n",
        "        plot_measured_waterfall(t_n, title=\"RAG Latency (nim)\")\n",
        "\n",
        "        total_l = float(sum(t_l.values()))\n",
        "        total_n = float(sum(t_n.values()))\n",
        "        print(\"\\n=== Comparison ===\")\n",
        "        print(f\"Local total: {total_l:.3f}s\")\n",
        "        print(f\"NIM total:   {total_n:.3f}s\")\n",
        "        print(f\"Delta (NIM-Local): {(total_n-total_l):+.3f}s\")\n",
        "\n",
        "\n",
        "btn_run.on_click(on_run_click)\n",
        "btn_compare.on_click(on_compare_click)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HBox([w_mode, btn_run, btn_compare]),\n",
        "    w_query,\n",
        "    widgets.HBox([w_chunk, w_overlap]),\n",
        "    widgets.HBox([w_k, w_rerank_k]),\n",
        "    out,\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Real Automated Eval (RAGAS)\n",
        "\n",
        "This phase builds a tiny evaluation set from the corpus and runs **real RAGAS** metrics.\n",
        "\n",
        "- It runs the same queries through **Local** and **NIM** modes.\n",
        "- It uses a **local Llama-3.1-8B-Instruct** judge (as requested) to compute metrics.\n",
        "\n",
        "Note: RAGAS will be slower than Phase 1 because it calls an LLM as a judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eed0797388f2474cb2de695c52fbc10f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Run RAGAS Eval (Local vs NIM)', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a833898afa46e2afec7d1cf83b5c3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Local generator/judge (cached) ---\n",
        "# Reuse the cached local generator from Phase 1 if it exists; otherwise define it.\n",
        "\n",
        "if \"ensure_local_generator\" not in globals():\n",
        "    _LOCAL_GEN = {\"tok\": None, \"model\": None}\n",
        "\n",
        "    def ensure_local_generator():\n",
        "        if _LOCAL_GEN[\"tok\"] is not None and _LOCAL_GEN[\"model\"] is not None:\n",
        "            return _LOCAL_GEN[\"tok\"], _LOCAL_GEN[\"model\"]\n",
        "\n",
        "        import torch\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "        tok = AutoTokenizer.from_pretrained(LOCAL_GEN_MODEL, use_fast=True)\n",
        "        tok.padding_side = \"left\"\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            LOCAL_GEN_MODEL,\n",
        "            device_map=\"auto\",\n",
        "            dtype=torch.bfloat16,\n",
        "        )\n",
        "        model.eval()\n",
        "\n",
        "        _LOCAL_GEN[\"tok\"] = tok\n",
        "        _LOCAL_GEN[\"model\"] = model\n",
        "        return tok, model\n",
        "\n",
        "\n",
        "def gen_local_cached(prompt: str, max_new_tokens: int = 180, temperature: float = 0.2):\n",
        "    import torch\n",
        "\n",
        "    tok, model = ensure_local_generator()\n",
        "\n",
        "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=float(temperature),\n",
        "            top_p=0.95,\n",
        "            max_new_tokens=int(max_new_tokens),\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# --- RAGAS eval ---\n",
        "# We create a small synthetic QA set grounded in context, then evaluate.\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def make_eval_questions(n: int = 6):\n",
        "    # Sample docs and generate grounded questions using the local model.\n",
        "    # We generate questions from redacted bodies to keep the lab safe.\n",
        "    sample = df_docs.sample(n=min(n, len(df_docs)), random_state=42)\n",
        "    rows = []\n",
        "    for _, r in sample.iterrows():\n",
        "        ctx = str(r.get(\"body_redacted\") or r.get(\"body\") or \"\")\n",
        "        ctx = ctx[:2000]\n",
        "        prompt = (\n",
        "            \"Create one question that can be answered ONLY from the provided context.\\n\"\n",
        "            \"Return ONLY the question.\\n\\n\"\n",
        "            f\"CONTEXT:\\n{ctx}\\n\\nQUESTION:\"\n",
        "        )\n",
        "        q = gen_local_cached(prompt, max_new_tokens=64, temperature=0.2)\n",
        "        q = (q.splitlines()[0] if q else \"\").strip()\n",
        "        if not q.endswith(\"?\"):\n",
        "            q = q + \"?\"\n",
        "        rows.append({\"question\": q, \"doc_id\": str(r.get(\"doc_id\"))})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def answer_with_mode(question: str, mode: str, chunk_size: int, overlap: int, top_k: int, rerank_k: int):\n",
        "    rows, index = get_index(mode, chunk_size, overlap)\n",
        "    cfg = RAGConfig(mode=mode, top_k=int(top_k), rerank_top_k=int(rerank_k))\n",
        "    answer, ctx_rows, timings = run_rag(question, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
        "\n",
        "    contexts = [c[\"text\"] for c in ctx_rows]\n",
        "    return answer, contexts, timings\n",
        "\n",
        "\n",
        "def run_ragas_eval(n_questions: int = 6, chunk_size: int = 900, overlap: int = 150, top_k: int = 10, rerank_k: int = 5):\n",
        "    from ragas import evaluate\n",
        "    from ragas.metrics import faithfulness, answer_relevancy\n",
        "\n",
        "    eval_qs = make_eval_questions(n_questions)\n",
        "\n",
        "    # Build dataset entries for both modes\n",
        "    # ragas>=0.4 expects: user_input, response, retrieved_contexts\n",
        "    records = []\n",
        "    lat_records = []\n",
        "    for q in eval_qs:\n",
        "        for mode in [\"local\", \"nim\"]:\n",
        "            ans, ctxs, timings = answer_with_mode(q[\"question\"], mode, chunk_size, overlap, top_k, rerank_k)\n",
        "            records.append(\n",
        "                {\n",
        "                    \"user_input\": q[\"question\"],\n",
        "                    \"response\": ans,\n",
        "                    \"retrieved_contexts\": ctxs,\n",
        "                    \"mode\": mode,\n",
        "                }\n",
        "            )\n",
        "            lat_records.append({\"mode\": mode, **timings})\n",
        "\n",
        "    ds = Dataset.from_list(records)\n",
        "\n",
        "    # RAGAS needs an LLM judge. Use local Llama via a simple langchain wrapper.\n",
        "    from transformers import pipeline\n",
        "    from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "    tok, model = ensure_local_generator()\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tok,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "        return_full_text=False,\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    # Evaluate\n",
        "    report = evaluate(ds, metrics=[faithfulness, answer_relevancy], llm=llm)\n",
        "    df_report = report.to_pandas()\n",
        "\n",
        "    # Aggregate by mode\n",
        "    print(\"\\n=== RAGAS report (by mode) ===\")\n",
        "    print(df_report.groupby(\"mode\")[[\"faithfulness\", \"answer_relevancy\"]].mean())\n",
        "\n",
        "    # Latency summary\n",
        "    df_lat = pd.DataFrame(lat_records).fillna(0.0)\n",
        "    print(\"\\n=== Latency summary (per-stage means) ===\")\n",
        "    cols = [c for c in df_lat.columns if c != \"mode\"]\n",
        "    print(df_lat.groupby(\"mode\")[cols].mean())\n",
        "\n",
        "    return df_report, df_lat\n",
        "\n",
        "\n",
        "btn_eval = widgets.Button(description=\"Run RAGAS Eval (Local vs NIM)\")\n",
        "out_eval = widgets.Output()\n",
        "\n",
        "\n",
        "def on_eval_click(b):\n",
        "    with out_eval:\n",
        "        clear_output()\n",
        "        # Keep small by default for workshops\n",
        "        run_ragas_eval(n_questions=6, chunk_size=int(w_chunk.value), overlap=int(w_overlap.value), top_k=int(w_k.value), rerank_k=int(w_rerank_k.value))\n",
        "\n",
        "\n",
        "btn_eval.on_click(on_eval_click)\n",
        "display(btn_eval, out_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== No-widget fallback: Phase 1 ===\n"
          ]
        },
        {
          "ename": "ConnectionError",
          "evalue": "HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea53a22ba60>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:169\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py:96\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py:86\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     85\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 86\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:395\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:234\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    233\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:181\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7ea53a22ba60>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:756\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 756\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py:576\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    578\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea53a22ba60>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Build indexes once\u001b[39;00m\n\u001b[1;32m      7\u001b[0m rows_local, index_local \u001b[38;5;241m=\u001b[39m get_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m900\u001b[39m, overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m rows_nim, index_nim \u001b[38;5;241m=\u001b[39m \u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m cfg \u001b[38;5;241m=\u001b[39m RAGConfig(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, rerank_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     12\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU latency impacts APR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[2], line 323\u001b[0m, in \u001b[0;36mget_index\u001b[0;34m(mode, chunk_size, overlap)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _INDEX_CACHE[key]\n\u001b[1;32m    322\u001b[0m rows \u001b[38;5;241m=\u001b[39m build_chunks(\u001b[38;5;28mint\u001b[39m(chunk_size), \u001b[38;5;28mint\u001b[39m(overlap))\n\u001b[0;32m--> 323\u001b[0m index, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_index_for_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m _INDEX_CACHE[key] \u001b[38;5;241m=\u001b[39m (rows, index)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _INDEX_CACHE[key]\n",
            "Cell \u001b[0;32mIn[2], line 159\u001b[0m, in \u001b[0;36mbuild_index_for_mode\u001b[0;34m(mode, chunk_rows, use_faiss)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorIndex(E_norm, use_faiss\u001b[38;5;241m=\u001b[39muse_faiss), E_norm\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnim\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Batch to avoid request-size limits\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     embs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     E \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    161\u001b[0m     E_norm \u001b[38;5;241m=\u001b[39m normalize_rows(E)\n",
            "File \u001b[0;32m~/fico/nim_clients.py:83\u001b[0m, in \u001b[0;36mNIMClient.embed_many\u001b[0;34m(self, texts, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), bs):\n\u001b[1;32m     82\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m bs]\n\u001b[0;32m---> 83\u001b[0m     embs, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     all_embs\u001b[38;5;241m.\u001b[39mextend(embs)\n\u001b[1;32m     85\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n",
            "File \u001b[0;32m~/fico/nim_clients.py:63\u001b[0m, in \u001b[0;36mNIMClient.embed\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     61\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39membed_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: texts}\n\u001b[1;32m     62\u001b[0m t0 \u001b[38;5;241m=\u001b[39m now_s()\n\u001b[0;32m---> 63\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m dt \u001b[38;5;241m=\u001b[39m now_s() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     65\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:677\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea53a22ba60>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ],
      "source": [
        "# ---- NO-WIDGET FALLBACK ----\n",
        "# If ipywidgets don't render/click in your environment, run these direct calls.\n",
        "\n",
        "print(\"\\n=== No-widget fallback: Phase 1 ===\")\n",
        "\n",
        "# Build indexes once\n",
        "rows_local, index_local = get_index(\"local\", chunk_size=900, overlap=150)\n",
        "rows_nim, index_nim = get_index(\"nim\", chunk_size=900, overlap=150)\n",
        "\n",
        "cfg = RAGConfig(mode=\"local\", top_k=10, rerank_top_k=5)\n",
        "\n",
        "q = \"GPU latency impacts APR\"\n",
        "\n",
        "ans_l, ctx_l, t_l = run_rag(q, mode=\"local\", chunk_rows=rows_local, index=index_local, cfg=cfg)\n",
        "plot_measured_waterfall(t_l, title=\"RAG Latency (local)\")\n",
        "\n",
        "print(\"\\n---\")\n",
        "\n",
        "ans_n, ctx_n, t_n = run_rag(q, mode=\"nim\", chunk_rows=rows_nim, index=index_nim, cfg=RAGConfig(mode=\"nim\", top_k=10, rerank_top_k=5))\n",
        "plot_measured_waterfall(t_n, title=\"RAG Latency (nim)\")\n",
        "\n",
        "print(\"\\n=== No-widget fallback: Phase 2 (RAGAS) ===\")\n",
        "# Small eval by default\n",
        "run_ragas_eval(n_questions=4, chunk_size=900, overlap=150, top_k=10, rerank_k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
