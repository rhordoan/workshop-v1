{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 — Observability for RAG (Traces-first, Cloud-Engineer-friendly)\n",
    "\n",
    "**Audience:** senior cloud engineers (strong distributed-systems intuition, new to AI).\n",
    "\n",
    "**Goal:** Learn to diagnose **latency**, **errors**, and **quality regressions** in an AI/RAG system the same way you debug microservices: with **traces + metrics + structured logs**.\n",
    "\n",
    "## What you’ll do\n",
    "- Start a local **Jaeger** UI and **OpenTelemetry Collector**.\n",
    "- Run a tiny end-to-end **RAG** request and see spans for:\n",
    "  - `rag.embed_query` → `rag.retrieve` → `rag.rerank` → `rag.pack_context` → `rag.generate`\n",
    "- Run a few “consulting-style” drills:\n",
    "  - **Tail latency** (p95 explosion)\n",
    "  - **Quality regression** (Top_K / Rerank_K tradeoffs)\n",
    "  - **Failure analysis** (timeouts, 502, retries)\n",
    "\n",
    "## Prereqs\n",
    "- Docker works (`docker ps`)\n",
    "- Ollama server running at `NIM_BASE_URL` (default `http://localhost:11434`)\n",
    "\n",
    "## URLs\n",
    "- Jaeger UI: `http://localhost:16686`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup + configuration\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from nim_clients import NIMClient, NIMConfig\n",
    "from rag_pipeline import (\n",
    "    RAGConfig,\n",
    "    Timer,\n",
    "    VectorIndex,\n",
    "    build_chunks_from_df,\n",
    "    chunk_text,\n",
    "    normalize_rows,\n",
    "    make_prompt,\n",
    "    clean_answer,\n",
    ")\n",
    "\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "NIM_BASE_URL = os.environ.get(\"NIM_BASE_URL\", \"http://localhost:11434\").rstrip(\"/\")\n",
    "JAEGER_UI = os.environ.get(\"JAEGER_UI\", \"http://localhost:16686\")\n",
    "\n",
    "# OpenTelemetry export target (OTLP/HTTP). This is what start_observability.sh exposes.\n",
    "OTEL_EXPORTER_OTLP_ENDPOINT = os.environ.get(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4318/v1/traces\")\n",
    "OTEL_SERVICE_NAME = os.environ.get(\"OTEL_SERVICE_NAME\", \"fico-rag-observability\")\n",
    "\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "print(\"NIM_BASE_URL:\", NIM_BASE_URL)\n",
    "print(\"OTEL_EXPORTER_OTLP_ENDPOINT:\", OTEL_EXPORTER_OTLP_ENDPOINT)\n",
    "print(\"OTEL_SERVICE_NAME:\", OTEL_SERVICE_NAME)\n",
    "print(\"JAEGER_UI:\", JAEGER_UI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability mental model (AI edition)\n",
    "\n",
    "Cloud intuition still applies:\n",
    "\n",
    "- **Logs** answer: *what happened?* (debugging details, errors, inputs/outputs)\n",
    "- **Metrics** answer: *how often and how bad?* (SLOs, p95/p99, error rate)\n",
    "- **Traces** answer: *where did the time go?* (end-to-end path across stages/services)\n",
    "\n",
    "### What changes in AI/RAG\n",
    "In RAG systems, **latency** and **quality** are coupled:\n",
    "\n",
    "- Bigger `top_k` can improve recall but also:\n",
    "  - increases rerank cost\n",
    "  - increases context size (slower generation)\n",
    "  - can reduce answer quality via noisy context\n",
    "\n",
    "### Golden signals (practical)\n",
    "Track these for any customer:\n",
    "- **Latency**: p50/p95/p99 (and stage breakdown)\n",
    "- **Traffic**: req/s, concurrency\n",
    "- **Errors**: rate + dominant class (timeouts, 5xx, invalid_request)\n",
    "- **Saturation**: GPU memory, queueing, throttling\n",
    "- **Quality proxy**: retrieval hit-rate, answer length, refusal rate, simple judge score\n",
    "\n",
    "We’ll build a tiny version of that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: Docker + start tracing stack (Jaeger + OTEL Collector)\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def _run(cmd: list[str], *, check: bool = False) -> subprocess.CompletedProcess:\n",
    "    return subprocess.run(cmd, check=check, text=True, capture_output=True)\n",
    "\n",
    "\n",
    "def docker_ok() -> bool:\n",
    "    try:\n",
    "        r = _run([\"docker\", \"ps\"], check=False)\n",
    "        if r.returncode != 0:\n",
    "            print(\"docker ps failed:\")\n",
    "            print((r.stderr or r.stdout or \"\").strip()[:400])\n",
    "            return False\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"docker not found\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def find_fico_root() -> Path:\n",
    "    \"\"\"Find the `fico/` directory regardless of notebook working directory.\"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    candidates = [here, *here.parents]\n",
    "    for c in candidates:\n",
    "        if (c / \"scripts\" / \"setup_workshop.sh\").exists() and (c / \"nim_clients.py\").exists():\n",
    "            return c\n",
    "        if (c / \"fico\" / \"scripts\" / \"setup_workshop.sh\").exists() and (c / \"fico\" / \"nim_clients.py\").exists():\n",
    "            return c / \"fico\"\n",
    "    raise RuntimeError(\"Could not find repo root (expected to find fico/scripts/setup_workshop.sh)\")\n",
    "\n",
    "\n",
    "FICO_DIR = find_fico_root()\n",
    "print(\"FICO_DIR:\", FICO_DIR)\n",
    "\n",
    "if docker_ok():\n",
    "    start = FICO_DIR / \"scripts\" / \"start_observability.sh\"\n",
    "    if start.exists():\n",
    "        print(\"Starting observability stack...\")\n",
    "        r = _run([\"bash\", str(start)], check=False)\n",
    "        if r.returncode != 0:\n",
    "            print(\"start_observability.sh failed:\")\n",
    "            print((r.stderr or r.stdout or \"\").strip()[:800])\n",
    "        else:\n",
    "            print(\"✅ start_observability.sh ran\")\n",
    "\n",
    "    # Basic UI reachability (doesn't guarantee traces are flowing, but catches common issues)\n",
    "    try:\n",
    "        ui = requests.get(JAEGER_UI, timeout=2)\n",
    "        print(\"Jaeger UI HTTP:\", ui.status_code)\n",
    "    except Exception as e:\n",
    "        print(\"Jaeger UI not reachable yet:\", type(e).__name__, str(e)[:120])\n",
    "else:\n",
    "    print(\"ℹ️ Docker unavailable: this notebook will still run, but traces will be printed to the console instead of Jaeger.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing setup (OpenTelemetry)\n",
    "\n",
    "We’ll emit spans to the OTEL Collector using **OTLP/HTTP**.\n",
    "\n",
    "- If Docker + the collector are reachable, traces show up in Jaeger.\n",
    "- If not, we fall back to printing spans (still useful for learning).\n",
    "\n",
    "### Trace hygiene (important for AI consulting)\n",
    "- Don’t put raw customer data into span attributes.\n",
    "- Prefer **sizes**, **counts**, **ids**, and **hashes** over raw text.\n",
    "- Use **events** for concise summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tracing\n",
    "\n",
    "Run the next cell once. It:\n",
    "- configures an OpenTelemetry tracer provider\n",
    "- instruments `requests` so outbound HTTP calls show up as spans\n",
    "- exports to Jaeger (via OTEL Collector) if reachable; otherwise prints spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenTelemetry setup\n",
    "\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "# Prefer OTLP/HTTP exporter if installed and reachable.\n",
    "_export_to_jaeger = False\n",
    "_otlp_exporter = None\n",
    "\n",
    "try:\n",
    "    from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "\n",
    "    _otlp_exporter = OTLPSpanExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT)\n",
    "    _export_to_jaeger = True\n",
    "except Exception as e:\n",
    "    print(\"OTLP exporter not available; falling back to console spans:\", type(e).__name__, str(e)[:120])\n",
    "\n",
    "resource = Resource.create({\"service.name\": OTEL_SERVICE_NAME})\n",
    "provider = TracerProvider(resource=resource)\n",
    "\n",
    "if _export_to_jaeger and _otlp_exporter is not None:\n",
    "    provider.add_span_processor(BatchSpanProcessor(_otlp_exporter))\n",
    "    print(\"✅ Exporting spans via OTLP/HTTP to:\", OTEL_EXPORTER_OTLP_ENDPOINT)\n",
    "else:\n",
    "    provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n",
    "    print(\"✅ Exporting spans to console\")\n",
    "\n",
    "trace.set_tracer_provider(provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Optional: auto-instrument requests so HTTP calls show in Jaeger\n",
    "try:\n",
    "    from opentelemetry.instrumentation.requests import RequestsInstrumentor\n",
    "\n",
    "    RequestsInstrumentor().instrument()\n",
    "    print(\"✅ requests instrumentation enabled\")\n",
    "except Exception as e:\n",
    "    print(\"requests auto-instrumentation not available (ok):\", type(e).__name__, str(e)[:120])\n",
    "\n",
    "print(\"If Jaeger UI is up, open:\", JAEGER_UI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented RAG (what we will trace)\n",
    "\n",
    "We’ll trace a **single request** through these stages:\n",
    "\n",
    "- `rag.embed_query` (NIM embeddings)\n",
    "- `rag.retrieve` (vector similarity search)\n",
    "- `rag.rerank` (NIM reranker)\n",
    "- `rag.pack_context` (context budget enforcement)\n",
    "- `rag.generate` (NIM chat completion)\n",
    "\n",
    "For consulting work, your job is to:\n",
    "1. **Locate the bottleneck stage** (most of the latency)\n",
    "2. **Decide if it’s expected** (token budget, queueing) or a bug\n",
    "3. **Propose a safe knob** (Top_K, Rerank_K, max_tokens, timeouts)\n",
    "\n",
    "### Hardware observability (the missing half)\n",
    "Traces tell you *where* the time went. Hardware signals tell you *why*:\n",
    "\n",
    "- **GPU VRAM used**: KV cache pressure, model residency\n",
    "- **GPU utilization**: compute-bound vs waiting/queueing\n",
    "- **Power + temperature**: power limits and thermal throttling (\"my p95 got slow because physics\")\n",
    "\n",
    "Next we’ll collect NVML snapshots during runs and attach a few key numbers to spans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware cockpit: GPU/CPU snapshots + a lightweight background watcher\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import threading\n",
    "\n",
    "\n",
    "def _read_meminfo() -> dict[str, float]:\n",
    "    out: dict[str, float] = {}\n",
    "    try:\n",
    "        with open(\"/proc/meminfo\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    key = parts[0].rstrip(\":\")\n",
    "                    val_kib = float(parts[1])\n",
    "                    out[key] = val_kib\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return out\n",
    "\n",
    "\n",
    "def cpu_snapshot() -> dict[str, float]:\n",
    "    mem = _read_meminfo()\n",
    "    # meminfo is KiB\n",
    "    mem_total_gib = float(mem.get(\"MemTotal\", float(\"nan\")) / (1024.0**2))\n",
    "    mem_avail_gib = float(mem.get(\"MemAvailable\", float(\"nan\")) / (1024.0**2))\n",
    "\n",
    "    try:\n",
    "        l1, l5, l15 = os.getloadavg()\n",
    "    except Exception:\n",
    "        l1, l5, l15 = float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"cpu_load1\": float(l1),\n",
    "        \"cpu_load5\": float(l5),\n",
    "        \"cpu_load15\": float(l15),\n",
    "        \"mem_total_gib\": float(mem_total_gib),\n",
    "        \"mem_avail_gib\": float(mem_avail_gib),\n",
    "    }\n",
    "\n",
    "\n",
    "def _bytes_gib(x: float) -> float:\n",
    "    return float(x) / (1024.0**3)\n",
    "\n",
    "\n",
    "def gpu_snapshot() -> dict[str, float]:\n",
    "    \"\"\"Best-effort NVML snapshot. Returns empty dict if NVML isn't available.\"\"\"\n",
    "    try:\n",
    "        from pynvml import (\n",
    "            nvmlInit,\n",
    "            nvmlShutdown,\n",
    "            nvmlDeviceGetHandleByIndex,\n",
    "            nvmlDeviceGetMemoryInfo,\n",
    "            nvmlDeviceGetUtilizationRates,\n",
    "            nvmlDeviceGetPowerUsage,\n",
    "            nvmlDeviceGetTemperature,\n",
    "            NVML_TEMPERATURE_GPU,\n",
    "        )\n",
    "\n",
    "        nvmlInit()\n",
    "        try:\n",
    "            h = nvmlDeviceGetHandleByIndex(0)\n",
    "            mem = nvmlDeviceGetMemoryInfo(h)\n",
    "            util = nvmlDeviceGetUtilizationRates(h)\n",
    "            pwr_w = float(nvmlDeviceGetPowerUsage(h)) / 1000.0\n",
    "            temp_c = float(nvmlDeviceGetTemperature(h, NVML_TEMPERATURE_GPU))\n",
    "            return {\n",
    "                \"gpu_vram_used_gib\": _bytes_gib(mem.used),\n",
    "                \"gpu_vram_total_gib\": _bytes_gib(mem.total),\n",
    "                \"gpu_util_pct\": float(util.gpu),\n",
    "                \"gpu_mem_util_pct\": float(util.memory),\n",
    "                \"gpu_power_w\": float(pwr_w),\n",
    "                \"gpu_temp_c\": float(temp_c),\n",
    "            }\n",
    "        finally:\n",
    "            try:\n",
    "                nvmlShutdown()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def system_snapshot() -> dict[str, float]:\n",
    "    out = {}\n",
    "    out.update(cpu_snapshot())\n",
    "    out.update(gpu_snapshot())\n",
    "    return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPUWatcher:\n",
    "    interval_s: float = 0.25\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._stop = threading.Event()\n",
    "        self._rows: list[dict[str, float]] = []\n",
    "        self._t0 = 0.0\n",
    "        self._th: threading.Thread | None = None\n",
    "\n",
    "    def start(self):\n",
    "        self._t0 = time.perf_counter()\n",
    "\n",
    "        def _loop():\n",
    "            while not self._stop.is_set():\n",
    "                t = time.perf_counter() - self._t0\n",
    "                row = {\"t_s\": float(t), **system_snapshot()}\n",
    "                self._rows.append(row)\n",
    "                time.sleep(float(self.interval_s))\n",
    "\n",
    "        self._th = threading.Thread(target=_loop, daemon=True)\n",
    "        self._th.start()\n",
    "\n",
    "    def stop(self) -> pd.DataFrame:\n",
    "        self._stop.set()\n",
    "        if self._th is not None:\n",
    "            self._th.join(timeout=2.0)\n",
    "        return pd.DataFrame(self._rows)\n",
    "\n",
    "\n",
    "print(\"Hardware cockpit ready.\")\n",
    "display(pd.DataFrame([system_snapshot()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and build a tiny chunk set (fast)\n",
    "\n",
    "RUN_DIR = Path(\"corpus_runs/llm_richer_n20_20251211_193028\")\n",
    "CSV_PATH = RUN_DIR / \"fico_corpus_embedded.csv\"\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing corpus CSV: {CSV_PATH}. Run the corpus generation notebook or check corpus_runs/.\")\n",
    "\n",
    "df_docs = pd.read_csv(CSV_PATH)\n",
    "print(\"docs:\", len(df_docs))\n",
    "\n",
    "# Build chunks using the helper, then re-chunk with smaller sizes for the lab.\n",
    "\n",
    "def build_chunk_rows(*, chunk_size: int = 900, overlap: int = 150) -> list[dict[str, Any]]:\n",
    "    out: list[dict[str, Any]] = []\n",
    "    for _, row in df_docs.iterrows():\n",
    "        doc_id = str(row.get(\"doc_id\"))\n",
    "        title = row.get(\"title\")\n",
    "        body = row.get(\"body_redacted\") or row.get(\"body\") or \"\"\n",
    "        parts = chunk_text(str(body), chunk_size=int(chunk_size), overlap=int(overlap))\n",
    "        for j, part in enumerate(parts):\n",
    "            out.append(\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}::c{j:03d}\",\n",
    "                    \"title\": str(title) if title is not None else None,\n",
    "                    \"text\": part,\n",
    "                }\n",
    "            )\n",
    "    return out\n",
    "\n",
    "chunk_rows = build_chunk_rows(chunk_size=900, overlap=150)\n",
    "print(\"chunks:\", len(chunk_rows))\n",
    "\n",
    "# NIM client\n",
    "nim = NIMClient(NIMConfig(base_url=NIM_BASE_URL, timeout_s=float(os.environ.get(\"NIM_TIMEOUT_S\", \"60\"))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (or reuse) a vector index for retrieval\n",
    "\n",
    "_INDEX_CACHE: dict[tuple[int, int], tuple[list[dict[str, Any]], VectorIndex]] = {}\n",
    "\n",
    "\n",
    "def get_index(chunk_size: int, overlap: int) -> tuple[list[dict[str, Any]], VectorIndex]:\n",
    "    key = (int(chunk_size), int(overlap))\n",
    "    if key in _INDEX_CACHE:\n",
    "        return _INDEX_CACHE[key]\n",
    "\n",
    "    rows = build_chunk_rows(chunk_size=int(chunk_size), overlap=int(overlap))\n",
    "    texts = [r[\"text\"] for r in rows]\n",
    "\n",
    "    # Embed passages (this is NOT part of the per-request trace; it's offline/index build)\n",
    "    with tracer.start_as_current_span(\"index.embed_passages\") as sp:\n",
    "        sp.set_attribute(\"chunk_size\", int(chunk_size))\n",
    "        sp.set_attribute(\"overlap\", int(overlap))\n",
    "        sp.set_attribute(\"num_chunks\", int(len(texts)))\n",
    "        embs, dt = nim.embed_many(texts, batch_size=64, input_type=\"passage\")\n",
    "        sp.set_attribute(\"embed_seconds\", float(dt))\n",
    "\n",
    "    E = np.array(embs, dtype=np.float32)\n",
    "    E_norm = normalize_rows(E)\n",
    "\n",
    "    with tracer.start_as_current_span(\"index.build\") as sp:\n",
    "        sp.set_attribute(\"num_vectors\", int(E_norm.shape[0]))\n",
    "        index = VectorIndex(E_norm, use_faiss=True)\n",
    "\n",
    "    _INDEX_CACHE[key] = (rows, index)\n",
    "    return _INDEX_CACHE[key]\n",
    "\n",
    "\n",
    "rows, index = get_index(chunk_size=900, overlap=150)\n",
    "print(\"index ready. chunks:\", len(rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrumented single RAG request\n",
    "\n",
    "@dataclass\n",
    "class RAGRun:\n",
    "    answer: str\n",
    "    answer_clean: str\n",
    "    timings_s: dict[str, float]\n",
    "    used_chunks: list[dict[str, Any]]\n",
    "    hw_samples: list[dict[str, Any]]\n",
    "\n",
    "\n",
    "def run_rag_traced(\n",
    "    query: str,\n",
    "    *,\n",
    "    chunk_size: int = 900,\n",
    "    overlap: int = 150,\n",
    "    top_k: int = 10,\n",
    "    rerank_k: int = 5,\n",
    "    max_context_chars: int = 6000,\n",
    "    max_new_tokens: int = 160,\n",
    "    temperature: float = 0.2,\n",
    ") -> RAGRun:\n",
    "    rows, idx = get_index(chunk_size=int(chunk_size), overlap=int(overlap))\n",
    "\n",
    "    timings: dict[str, float] = {}\n",
    "    t_total0 = time.perf_counter()\n",
    "\n",
    "    hw_samples: list[dict[str, Any]] = []\n",
    "\n",
    "    def record_hw(stage: str, sp=None) -> dict[str, Any]:\n",
    "        snap = system_snapshot()\n",
    "        snap[\"stage\"] = str(stage)\n",
    "        snap[\"t_s\"] = float(time.perf_counter() - t_total0)\n",
    "        hw_samples.append(snap)\n",
    "\n",
    "        if sp is not None:\n",
    "            # Keep span attributes small and numeric.\n",
    "            for k in [\"gpu_vram_used_gib\", \"gpu_util_pct\", \"gpu_power_w\", \"gpu_temp_c\", \"mem_avail_gib\"]:\n",
    "                v = snap.get(k)\n",
    "                if isinstance(v, (int, float)) and not math.isnan(float(v)):\n",
    "                    sp.set_attribute(k, float(v))\n",
    "        return snap\n",
    "\n",
    "    with tracer.start_as_current_span(\"rag.request\") as root:\n",
    "        record_hw(\"start\", root)\n",
    "        root.set_attribute(\"query_chars\", int(len(query)))\n",
    "        root.set_attribute(\"top_k\", int(top_k))\n",
    "        root.set_attribute(\"rerank_k\", int(rerank_k))\n",
    "        root.set_attribute(\"max_context_chars\", int(max_context_chars))\n",
    "        root.set_attribute(\"max_new_tokens\", int(max_new_tokens))\n",
    "\n",
    "        # 1) Embed query\n",
    "        t0 = time.perf_counter()\n",
    "        with tracer.start_as_current_span(\"rag.embed_query\") as sp:\n",
    "            sp.set_attribute(\"model\", nim.cfg.embed_model)\n",
    "            embs, _dt = nim.embed([query], input_type=\"query\")\n",
    "            q_emb = np.array(embs[0], dtype=np.float32)\n",
    "            sp.set_attribute(\"seconds\", float(time.perf_counter() - t0))\n",
    "            record_hw(\"embed_query.end\", sp)\n",
    "        timings[\"embed_query\"] = float(time.perf_counter() - t0)\n",
    "\n",
    "        # 2) Retrieve\n",
    "        t0 = time.perf_counter()\n",
    "        with tracer.start_as_current_span(\"rag.retrieve\") as sp:\n",
    "            q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "            idxs, sims = idx.search(q_norm, int(top_k))\n",
    "            idxs = [int(i) for i in idxs]\n",
    "            sp.set_attribute(\"candidates\", int(len(idxs)))\n",
    "            if len(sims) > 0:\n",
    "                sp.add_event(\n",
    "                    \"sims_summary\",\n",
    "                    {\n",
    "                        \"sim_min\": float(np.min(sims)),\n",
    "                        \"sim_p50\": float(np.percentile(sims, 50)),\n",
    "                        \"sim_p95\": float(np.percentile(sims, 95)),\n",
    "                        \"sim_max\": float(np.max(sims)),\n",
    "                    },\n",
    "                )\n",
    "            record_hw(\"retrieve.end\", sp)\n",
    "        timings[\"retrieve\"] = float(time.perf_counter() - t0)\n",
    "\n",
    "        candidates = [rows[i] for i in idxs]\n",
    "\n",
    "        # 3) Rerank\n",
    "        t0 = time.perf_counter()\n",
    "        with tracer.start_as_current_span(\"rag.rerank\") as sp:\n",
    "            sp.set_attribute(\"model\", nim.cfg.rerank_model)\n",
    "            docs = [c[\"text\"] for c in candidates]\n",
    "            order, _ = nim.rerank(query, docs, top_n=min(int(rerank_k), len(docs)))\n",
    "            sp.set_attribute(\"reranked\", int(len(order)))\n",
    "            record_hw(\"rerank.end\", sp)\n",
    "        timings[\"rerank\"] = float(time.perf_counter() - t0)\n",
    "\n",
    "        reranked = [candidates[i] for i in order[: int(rerank_k)]] if rerank_k > 0 else candidates\n",
    "\n",
    "        # 4) Pack context\n",
    "        t0 = time.perf_counter()\n",
    "        with tracer.start_as_current_span(\"rag.pack_context\") as sp:\n",
    "            used: list[dict[str, Any]] = []\n",
    "            total_chars = 0\n",
    "            for r in reranked:\n",
    "                t = r[\"text\"]\n",
    "                if total_chars + len(t) > int(max_context_chars):\n",
    "                    break\n",
    "                used.append(r)\n",
    "                total_chars += len(t)\n",
    "            sp.set_attribute(\"context_chars\", int(total_chars))\n",
    "            sp.set_attribute(\"context_chunks\", int(len(used)))\n",
    "            sp.add_event(\"context_ids\", {\"chunk_ids\": [u[\"chunk_id\"] for u in used[:8]]})\n",
    "            record_hw(\"pack_context.end\", sp)\n",
    "        timings[\"pack_context\"] = float(time.perf_counter() - t0)\n",
    "\n",
    "        # 5) Generate\n",
    "        t0 = time.perf_counter()\n",
    "        with tracer.start_as_current_span(\"rag.generate\") as sp:\n",
    "            sp.set_attribute(\"model\", nim.cfg.gen_model)\n",
    "            # Convert to rag_pipeline Chunk objects for prompt construction\n",
    "            from rag_pipeline import Chunk\n",
    "\n",
    "            context_chunks = [\n",
    "                Chunk(doc_id=u[\"doc_id\"], chunk_id=u[\"chunk_id\"], text=u[\"text\"], title=u.get(\"title\"))\n",
    "                for u in used\n",
    "            ]\n",
    "            prompt = make_prompt(query, context_chunks)\n",
    "            ans, _ = nim.chat(prompt, max_tokens=int(max_new_tokens), temperature=float(temperature))\n",
    "            sp.set_attribute(\"answer_chars\", int(len(ans or \"\")))\n",
    "            record_hw(\"generate.end\", sp)\n",
    "        timings[\"generate\"] = float(time.perf_counter() - t0)\n",
    "\n",
    "        timings[\"total\"] = float(time.perf_counter() - t_total0)\n",
    "        root.set_attribute(\"total_seconds\", float(timings[\"total\"]))\n",
    "\n",
    "        record_hw(\"end\", root)\n",
    "\n",
    "        ans_clean = clean_answer(ans)\n",
    "        return RAGRun(answer=ans, answer_clean=ans_clean, timings_s=timings, used_chunks=used, hw_samples=hw_samples)\n",
    "\n",
    "\n",
    "# Try a run\n",
    "q = os.environ.get(\"OBS_QUERY\", \"Kubernetes incident runbook\")\n",
    "run = run_rag_traced(q, top_k=10, rerank_k=5, max_new_tokens=160)\n",
    "print(\"TOTAL:\", f\"{run.timings_s['total']:.3f}s\")\n",
    "print(\"\\n--- Answer (clean) ---\\n\")\n",
    "print((run.answer_clean or \"\").strip()[:1200])\n",
    "\n",
    "# Hardware summary (stage boundary snapshots)\n",
    "if getattr(run, \"hw_samples\", None):\n",
    "    df_hw = pd.DataFrame(run.hw_samples)\n",
    "    display(df_hw[[c for c in [\"stage\", \"t_s\", \"gpu_vram_used_gib\", \"gpu_util_pct\", \"gpu_power_w\", \"gpu_temp_c\", \"mem_avail_gib\"] if c in df_hw.columns]])\n",
    "    # A simple time series, if GPU metrics exist\n",
    "    cols = [c for c in [\"gpu_vram_used_gib\", \"gpu_util_pct\", \"gpu_power_w\", \"gpu_temp_c\"] if c in df_hw.columns]\n",
    "    if cols:\n",
    "        px.line(df_hw, x=\"t_s\", y=cols, markers=True, title=\"Hardware snapshots across the RAG request\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-side metrics + pretty figures\n",
    "\n",
    "# Record a few runs with different knobs, then visualize.\n",
    "\n",
    "cases = [\n",
    "    {\"name\": \"fast\", \"top_k\": 6, \"rerank_k\": 0, \"max_new_tokens\": 96, \"max_context_chars\": 3000},\n",
    "    {\"name\": \"balanced\", \"top_k\": 10, \"rerank_k\": 5, \"max_new_tokens\": 160, \"max_context_chars\": 6000},\n",
    "    {\"name\": \"accurate-ish\", \"top_k\": 20, \"rerank_k\": 10, \"max_new_tokens\": 220, \"max_context_chars\": 9000},\n",
    "]\n",
    "\n",
    "rows_out = []\n",
    "for c in cases:\n",
    "    r = run_rag_traced(q, top_k=c[\"top_k\"], rerank_k=c[\"rerank_k\"], max_new_tokens=c[\"max_new_tokens\"], max_context_chars=c[\"max_context_chars\"])\n",
    "    rows_out.append(\n",
    "        {\n",
    "            \"case\": c[\"name\"],\n",
    "            **{k: float(v) for k, v in r.timings_s.items()},\n",
    "            \"answer_chars\": int(len(r.answer_clean or \"\")),\n",
    "            \"context_chunks\": int(len(r.used_chunks)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows_out)\n",
    "display(df[[\"case\", \"total\", \"embed_query\", \"retrieve\", \"rerank\", \"pack_context\", \"generate\", \"answer_chars\", \"context_chunks\"]])\n",
    "\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x=\"case\",\n",
    "    y=[\"embed_query\", \"retrieve\", \"rerank\", \"pack_context\", \"generate\"],\n",
    "    title=\"Stage latency breakdown (seconds)\",\n",
    "    labels={\"value\": \"seconds\", \"variable\": \"stage\"},\n",
    ")\n",
    "fig.update_layout(barmode=\"stack\")\n",
    "fig.show()\n",
    "\n",
    "fig2 = px.scatter(df, x=\"answer_chars\", y=\"total\", color=\"case\", title=\"Total latency vs answer size\")\n",
    "fig2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulting drills\n",
    "\n",
    "These are the kinds of “in the room with the customer” exercises that turn cloud engineers into strong AI consultants.\n",
    "\n",
    "### Drill A: Tail latency (p95 blows up)\n",
    "We’ll increase concurrency and watch:\n",
    "- error rate\n",
    "- p95\n",
    "- trace spans showing queueing/backpressure\n",
    "\n",
    "### Drill B: Quality regression (Top_K too small / too big)\n",
    "We’ll sweep `top_k` and track:\n",
    "- total latency\n",
    "- a simple proxy: did the *expected doc_id* appear in the chosen context? (hit-rate)\n",
    "\n",
    "### Drill C: Failure analysis (timeouts, 502)\n",
    "We’ll intentionally set aggressive timeouts and show how to:\n",
    "- classify errors\n",
    "- add safe retries\n",
    "- surface failures in traces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drill A: concurrency sweep (portable threadpool loadgen)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def pct(xs: list[float], p: float) -> float:\n",
    "    if not xs:\n",
    "        return float(\"nan\")\n",
    "    return float(np.percentile(np.array(xs, dtype=np.float64), p))\n",
    "\n",
    "\n",
    "def one_request(timeout_s: float) -> tuple[bool, float, str | None]:\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        # Keep it smaller so the sweep finishes.\n",
    "        _ = run_rag_traced(q, top_k=10, rerank_k=5, max_new_tokens=96)\n",
    "        return True, float(time.perf_counter() - t0), None\n",
    "    except Exception as e:\n",
    "        return False, float(time.perf_counter() - t0), f\"{type(e).__name__}: {str(e)[:120]}\"\n",
    "\n",
    "\n",
    "def run_load(total_requests: int, concurrency: int, timeout_s: float) -> dict[str, Any]:\n",
    "    # Start a lightweight hardware watcher so we can correlate p95 cliffs with saturation.\n",
    "    watcher = GPUWatcher(interval_s=0.25)\n",
    "    watcher.start()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    ok_lat: list[float] = []\n",
    "    errs: list[str] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=int(concurrency)) as ex:\n",
    "        futs = [ex.submit(one_request, float(timeout_s)) for _ in range(int(total_requests))]\n",
    "        for f in as_completed(futs):\n",
    "            ok, dt, err = f.result()\n",
    "            if ok:\n",
    "                ok_lat.append(float(dt))\n",
    "            else:\n",
    "                errs.append(str(err))\n",
    "\n",
    "    wall = float(time.perf_counter() - t0)\n",
    "\n",
    "    df_hw = watcher.stop()\n",
    "    # Aggregate a few signals\n",
    "    def _mean(col: str) -> float:\n",
    "        if col not in df_hw.columns or df_hw.empty:\n",
    "            return float(\"nan\")\n",
    "        return float(np.nanmean(df_hw[col].astype(float)))\n",
    "\n",
    "    def _max(col: str) -> float:\n",
    "        if col not in df_hw.columns or df_hw.empty:\n",
    "            return float(\"nan\")\n",
    "        return float(np.nanmax(df_hw[col].astype(float)))\n",
    "\n",
    "    return {\n",
    "        \"concurrency\": int(concurrency),\n",
    "        \"total_requests\": int(total_requests),\n",
    "        \"throughput_rps\": float(total_requests / wall) if wall > 0 else float(\"nan\"),\n",
    "        \"error_rate\": float(len(errs) / total_requests) if total_requests else float(\"nan\"),\n",
    "        \"p50_s\": pct(ok_lat, 50),\n",
    "        \"p95_s\": pct(ok_lat, 95),\n",
    "        \"p99_s\": pct(ok_lat, 99),\n",
    "        \"sample_error\": (errs[0] if errs else None),\n",
    "        \"gpu_util_mean\": _mean(\"gpu_util_pct\"),\n",
    "        \"gpu_util_max\": _max(\"gpu_util_pct\"),\n",
    "        \"gpu_vram_used_max_gib\": _max(\"gpu_vram_used_gib\"),\n",
    "        \"gpu_power_max_w\": _max(\"gpu_power_w\"),\n",
    "        \"gpu_temp_max_c\": _max(\"gpu_temp_c\"),\n",
    "    }\n",
    "\n",
    "\n",
    "levels = [1, 2, 4, 8, 16]\n",
    "rows = [run_load(total_requests=40, concurrency=c, timeout_s=60.0) for c in levels]\n",
    "df_load = pd.DataFrame(rows)\n",
    "display(df_load)\n",
    "\n",
    "fig = px.line(df_load, x=\"concurrency\", y=[\"p50_s\", \"p95_s\", \"p99_s\"], markers=True, title=\"RAG latency vs concurrency\")\n",
    "fig.show()\n",
    "\n",
    "fig2 = px.line(df_load, x=\"concurrency\", y=\"throughput_rps\", markers=True, title=\"Throughput vs concurrency\")\n",
    "fig2.show()\n",
    "\n",
    "fig3 = px.bar(df_load, x=\"concurrency\", y=\"error_rate\", title=\"Error rate vs concurrency\")\n",
    "fig3.show()\n",
    "\n",
    "# Hardware overlay: did we saturate?\n",
    "if any(c in df_load.columns for c in [\"gpu_util_mean\", \"gpu_vram_used_max_gib\"]):\n",
    "    cols = [c for c in [\"gpu_util_mean\", \"gpu_util_max\", \"gpu_vram_used_max_gib\", \"gpu_power_max_w\", \"gpu_temp_max_c\"] if c in df_load.columns]\n",
    "    fig4 = px.line(df_load, x=\"concurrency\", y=cols, markers=True, title=\"Hardware signals vs concurrency\")\n",
    "    fig4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drill B: sweep Top_K (latency vs a simple retrieval-quality proxy)\n",
    "\n",
    "# Proxy: because this workshop corpus is synthetic, we use a simple signal:\n",
    "# if any chunk from the same doc_id appears in the final context.\n",
    "\n",
    "TOPK = [3, 5, 8, 10, 15, 20, 30]\n",
    "\n",
    "def proxy_hit_rate(runs: list[RAGRun], gold_doc_id: str) -> float:\n",
    "    hits = 0\n",
    "    for r in runs:\n",
    "        if any(str(u.get(\"doc_id\")) == str(gold_doc_id) for u in r.used_chunks):\n",
    "            hits += 1\n",
    "    return float(hits / max(1, len(runs)))\n",
    "\n",
    "# Pick a doc as the \"gold\" target (first row) and build a query from its title\n",
    "row0 = df_docs.iloc[0]\n",
    "gold_doc_id = str(row0.get(\"doc_id\"))\n",
    "gold_title = str(row0.get(\"title\") or \"\")\n",
    "query2 = (gold_title[:90] or \"Kubernetes incident runbook\")\n",
    "\n",
    "records = []\n",
    "for k in TOPK:\n",
    "    r = run_rag_traced(query2, top_k=int(k), rerank_k=5, max_new_tokens=120)\n",
    "    records.append({\"top_k\": int(k), \"total_s\": float(r.timings_s[\"total\"]), \"context_chunks\": len(r.used_chunks)})\n",
    "\n",
    "_df = pd.DataFrame(records)\n",
    "display(_df)\n",
    "\n",
    "fig = px.line(_df, x=\"top_k\", y=\"total_s\", markers=True, title=\"Latency vs Top_K\")\n",
    "fig.show()\n",
    "\n",
    "fig2 = px.line(_df, x=\"top_k\", y=\"context_chunks\", markers=True, title=\"Context chunks used vs Top_K\")\n",
    "fig2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drill C: failure analysis (timeouts + retries)\n",
    "\n",
    "# This demonstrates how to surface failures in traces and still produce usable consulting artifacts.\n",
    "\n",
    "\n",
    "def embed_with_timeout(text: str, timeout_s: float) -> dict[str, Any]:\n",
    "    cfg = NIMConfig(base_url=NIM_BASE_URL, timeout_s=float(timeout_s))\n",
    "    c = NIMClient(cfg)\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        _emb, _ = c.embed([text], input_type=\"query\")\n",
    "        return {\"ok\": True, \"latency_s\": float(time.perf_counter() - t0), \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"latency_s\": float(time.perf_counter() - t0), \"error\": f\"{type(e).__name__}: {str(e)[:160]}\"}\n",
    "\n",
    "\n",
    "def embed_with_retry(text: str, timeout_s: float, retries: int = 3, base_sleep_s: float = 0.25) -> dict[str, Any]:\n",
    "    for attempt in range(1, int(retries) + 1):\n",
    "        out = embed_with_timeout(text, timeout_s=float(timeout_s))\n",
    "        out[\"attempt\"] = int(attempt)\n",
    "        if out[\"ok\"]:\n",
    "            return out\n",
    "        if attempt < retries:\n",
    "            time.sleep(float(base_sleep_s) * (2 ** (attempt - 1)))\n",
    "    return out\n",
    "\n",
    "\n",
    "# Intentionally aggressive timeout\n",
    "res = embed_with_retry(\"ping\", timeout_s=0.25, retries=3)\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulting checklist (what to say in front of a customer)\n",
    "\n",
    "When you have traces, your story becomes crisp.\n",
    "\n",
    "### 1) Frame the problem in SLO language\n",
    "- “Our p95 is Xs. We need it under Ys.”\n",
    "- “Error rate is Z% and rising with concurrency.”\n",
    "\n",
    "### 2) Point at the bottleneck stage\n",
    "- “Generation dominates because the token budget is large.”\n",
    "- “Rerank dominates because rerank_k is high.”\n",
    "- “Embed dominates because we’re embedding too often / not caching.”\n",
    "\n",
    "### 3) Offer safe, reversible knobs\n",
    "- Reduce `max_new_tokens` (often the biggest lever)\n",
    "- Reduce `top_k` (but explain recall risk)\n",
    "- Reduce `rerank_k` (often good tradeoff)\n",
    "- Cap concurrency; add backpressure\n",
    "- Add timeouts + retries with backoff (avoid retry storms)\n",
    "\n",
    "### 4) Call out capacity constraints explicitly\n",
    "- “These services are GPU-memory heavy; you may need to split embed/rerank vs chat onto different GPUs.”\n",
    "\n",
    "### 5) Leave them with artifacts\n",
    "- A trace screenshot showing stage breakdown\n",
    "- A small table of configs and p50/p95/error rate\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
