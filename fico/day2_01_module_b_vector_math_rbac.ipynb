{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module B: Vector Ops & RBAC Security Console\n",
    "\n",
    "**Goal:** Visualize \"Meaning\" as \"Distance\" and implement Security Filters.\n",
    "\n",
    "**Persona:** AI Architect\n",
    "\n",
    "**Hardware:** GPU (Optional but recommended for speed demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SETUP --\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=140)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ST_EMBED_MODEL_ID = os.environ.get(\"ST_EMBED_MODEL_ID\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "GPT2_MODEL_ID = os.environ.get(\"GPT2_MODEL_ID\", \"gpt2\")\n",
    "GLOVE_MODEL_ID = os.environ.get(\"GLOVE_MODEL_ID\", \"glove-wiki-gigaword-50\")\n",
    "\n",
    "print(\"device:\", DEVICE)\n",
    "print(\"ST_EMBED_MODEL_ID:\", ST_EMBED_MODEL_ID)\n",
    "print(\"GPT2_MODEL_ID:\", GPT2_MODEL_ID)\n",
    "print(\"GLOVE_MODEL_ID:\", GLOVE_MODEL_ID)\n",
    "\n",
    "\n",
    "def l2norm(v: np.ndarray) -> float:\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    return float(np.linalg.norm(v) + 1e-12)\n",
    "\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    b = np.asarray(b, dtype=np.float32)\n",
    "    return float((a @ b) / ((np.linalg.norm(a) + 1e-12) * (np.linalg.norm(b) + 1e-12)))\n",
    "\n",
    "\n",
    "def show_vector(v: np.ndarray, *, full: bool = True, max_items: int = 24) -> str:\n",
    "    \"\"\"Pretty-print a vector. Set full=True to show everything.\"\"\"\n",
    "    v = np.asarray(v, dtype=np.float32).reshape(-1)\n",
    "    if full or v.size <= max_items:\n",
    "        return np.array2string(v, precision=4, separator=\", \")\n",
    "    head = np.array2string(v[:max_items], precision=4, separator=\", \")\n",
    "    return head[:-1] + f\", ... ({v.size - max_items} more)]\"\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=2)\n",
    "def get_sentence_embedder() -> SentenceTransformer:\n",
    "    return SentenceTransformer(ST_EMBED_MODEL_ID, device=DEVICE)\n",
    "\n",
    "\n",
    "print(\"✅ Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Sparse representations: Bag‑of‑Words and TF‑IDF\n",
    "\n",
    "First, we’ll represent text as **sparse vectors**.\n",
    "\n",
    "- **Bag‑of‑Words (BoW)**: counts words.\n",
    "- **TF‑IDF**: counts words but downweights very common ones.\n",
    "\n",
    "These methods are simple and fast, but they struggle with synonyms and “meaning”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus (sparse vectors)\n",
    "\n",
    "docs = [\n",
    "    \"A cat sits on the mat.\",\n",
    "    \"A dog plays fetch in the park.\",\n",
    "    \"The loan APR depends on interest rate and fees.\",\n",
    "    \"Mortgage underwriting uses credit score and debt-to-income ratio.\",\n",
    "    \"GPU latency increases when memory is fragmented.\",\n",
    "    \"Kubernetes incident response runbooks reduce mean time to recovery.\",\n",
    "]\n",
    "query = \"APR interest rate for a loan\"\n",
    "\n",
    "print(\"query:\", query)\n",
    "\n",
    "# BoW\n",
    "bow = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "X_bow = bow.fit_transform(docs)\n",
    "q_bow = bow.transform([query])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "X_tfidf = tfidf.fit_transform(docs)\n",
    "q_tfidf = tfidf.transform([query])\n",
    "\n",
    "print(\"BoW vocab size:\", len(bow.vocabulary_))\n",
    "print(\"TF-IDF vocab size:\", len(tfidf.vocabulary_))\n",
    "\n",
    "# Similarity (dot-product in sparse space)\n",
    "# NOTE: TF-IDF vectors are not normalized by default; dot product is still a reasonable proxy here.\n",
    "sim_bow = (X_bow @ q_bow.T).toarray().reshape(-1)\n",
    "sim_tfidf = (X_tfidf @ q_tfidf.T).toarray().reshape(-1)\n",
    "\n",
    "out = pd.DataFrame(\n",
    "    {\n",
    "        \"doc\": docs,\n",
    "        \"bow_dot\": sim_bow,\n",
    "        \"tfidf_dot\": sim_tfidf,\n",
    "    }\n",
    ").sort_values(\"tfidf_dot\", ascending=False)\n",
    "\n",
    "display(out)\n",
    "\n",
    "# Inspect the sparse TF-IDF dimensions used by the query\n",
    "q_terms = np.array(tfidf.get_feature_names_out())\n",
    "q_nz = q_tfidf.nonzero()[1]\n",
    "print(\"\\nTF-IDF query nonzero terms:\")\n",
    "print(q_terms[q_nz])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the *sparse* TF-IDF vector for the query (interpretability)\n",
    "\n",
    "terms = np.array(tfidf.get_feature_names_out())\n",
    "q_vec = q_tfidf.toarray().reshape(-1)\n",
    "idx = np.where(q_vec != 0)[0]\n",
    "\n",
    "df_q = pd.DataFrame({\"term\": terms[idx], \"tfidf\": q_vec[idx]}).sort_values(\"tfidf\", ascending=False)\n",
    "display(df_q)\n",
    "\n",
    "print(\"Takeaway: sparse vectors are interpretable (you can see the exact words), but they don’t capture synonyms/meaning well.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Dense embeddings: vectors that carry meaning\n",
    "\n",
    "Dense embeddings map text to a fixed-size vector (e.g. 384 floats). You can think of it as a point in a high-dimensional space.\n",
    "\n",
    "Key idea:\n",
    "- Similar meaning → vectors close together.\n",
    "- Similarity is usually computed with **cosine similarity**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive embedding inspector: type a word/sentence and see the full vector\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder_small = SentenceTransformer(EMBED_MODEL_ID, device=DEVICE)\n",
    "\n",
    "w_text = widgets.Text(value=\"GPU latency\", description=\"Text:\", layout=widgets.Layout(width=\"700px\"))\n",
    "w_full = widgets.Checkbox(value=False, description=\"Show full vector\")\n",
    "run_btn = widgets.Button(description=\"Embed\", button_style=\"primary\")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def _embed_one(s: str) -> np.ndarray:\n",
    "    v = embedder_small.encode([s], normalize_embeddings=False, convert_to_numpy=True, show_progress_bar=False)[0]\n",
    "    return v.astype(np.float32)\n",
    "\n",
    "\n",
    "def _render(_=None):\n",
    "    s = (w_text.value or \"\").strip()\n",
    "    if not s:\n",
    "        return\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        v = _embed_one(s)\n",
    "        n = int(v.shape[0])\n",
    "        v_norm = v / (np.linalg.norm(v) + 1e-12)\n",
    "\n",
    "        print(\"model:\", EMBED_MODEL_ID)\n",
    "        print(\"text:\", repr(s))\n",
    "        print(\"dim:\", n)\n",
    "        print(\"L2 norm:\", f\"{float(np.linalg.norm(v)):.4f}\")\n",
    "\n",
    "        head_n = min(24, n)\n",
    "        df_head = pd.DataFrame({\"dim\": np.arange(head_n, dtype=int), \"value\": v[:head_n]})\n",
    "        display(df_head)\n",
    "        px.line(df_head, x=\"dim\", y=\"value\", title=\"Embedding preview (first dims)\").show()\n",
    "\n",
    "        if w_full.value:\n",
    "            df_full = pd.DataFrame({\"dim\": np.arange(n, dtype=int), \"value\": v})\n",
    "            display(df_full)\n",
    "\n",
    "        # Compare against a couple references\n",
    "        refs = [\"GPU performance\", \"interest rate APR\", \"Kubernetes crash loop\"]\n",
    "        E = embedder_small.encode(refs, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False).astype(np.float32)\n",
    "        sims = E @ v_norm.astype(np.float32)\n",
    "\n",
    "        print(\"\\ncosine similarities (normalized embeddings):\")\n",
    "        for r, s0 in sorted(zip(refs, sims.tolist()), key=lambda x: -x[1]):\n",
    "            print(f\"  {float(s0):0.3f} | {r}\")\n",
    "\n",
    "\n",
    "run_btn.on_click(_render)\n",
    "w_text.on_submit(_render)\n",
    "w_full.observe(_render, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([w_text, run_btn, w_full]), out]))\n",
    "_render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval: TF-IDF vs embeddings (toy corpus)\n",
    "\n",
    "def cosine_topk(vectors: np.ndarray, q: np.ndarray, k: int = 5):\n",
    "    vectors = vectors.astype(np.float32)\n",
    "    q = q.astype(np.float32)\n",
    "    vectors = vectors / (np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-12)\n",
    "    q = q / (np.linalg.norm(q) + 1e-12)\n",
    "    sims = vectors @ q\n",
    "    k = min(int(k), len(sims))\n",
    "    idx = np.argpartition(-sims, k - 1)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return idx.tolist(), sims[idx].tolist()\n",
    "\n",
    "E_docs = embedder_small.encode(docs, normalize_embeddings=False, convert_to_numpy=True, show_progress_bar=False).astype(np.float32)\n",
    "q_emb = embedder_small.encode([query], normalize_embeddings=False, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "\n",
    "idx, sims = cosine_topk(E_docs, q_emb, k=5)\n",
    "\n",
    "rows = []\n",
    "for i, s in zip(idx, sims):\n",
    "    rows.append({\"rank\": len(rows) + 1, \"cosine\": float(s), \"doc\": docs[int(i)]})\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nNote: embeddings can match conceptually even when exact words differ; TF-IDF mostly rewards literal overlap.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Vector space operations: add/subtract (king − man + woman)\n",
    "\n",
    "Now that we have vectors, we can do *math in meaning space*.\n",
    "\n",
    "The classic demo is:\n",
    "\n",
    "- `king - man + woman ≈ queen`\n",
    "\n",
    "This works best with **word embedding** models trained explicitly for word-level geometry (we’ll use a small GloVe model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors for analogies (GloVe)\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(f\"Loading gensim vectors: {GLOVE_MODEL_ID} (cached after first download)\")\n",
    "wv = api.load(GLOVE_MODEL_ID)\n",
    "\n",
    "# Analogy arithmetic\n",
    "#   king - man + woman ≈ queen\n",
    "v_king = wv[\"king\"]\n",
    "v_man = wv[\"man\"]\n",
    "v_woman = wv[\"woman\"]\n",
    "v_combo = v_king - v_man + v_woman\n",
    "\n",
    "print(\"Top results for king - man + woman:\")\n",
    "exclusions = {\"king\", \"man\", \"woman\"}\n",
    "rank = 0\n",
    "for word, score in wv.similar_by_vector(v_combo, topn=20):\n",
    "    if word in exclusions:\n",
    "        continue\n",
    "    rank += 1\n",
    "    print(f\"  {word:>12s}  sim={score:0.3f}\")\n",
    "    if rank >= 10:\n",
    "        break\n",
    "\n",
    "# Visualize in 2D (PCA) and draw the arithmetic as arrows\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\", \"royal\"]\n",
    "vecs = {w: wv[w] for w in words if w in wv}\n",
    "vecs[\"king - man + woman\"] = v_combo\n",
    "\n",
    "labels = list(vecs.keys())\n",
    "X = np.stack([vecs[k] for k in labels]).astype(np.float32)\n",
    "Z = PCA(n_components=2, random_state=7).fit_transform(X)\n",
    "\n",
    "coords = {lab: Z[i] for i, lab in enumerate(labels)}\n",
    "\n",
    "df_plot = pd.DataFrame({\"label\": labels, \"x\": Z[:, 0], \"y\": Z[:, 1]})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_plot[\"x\"], y=df_plot[\"y\"], mode=\"markers+text\", text=df_plot[\"label\"], textposition=\"top center\"))\n",
    "\n",
    "# Draw the path: king -> (king - man) -> (king - man + woman)\n",
    "# We approximate intermediate points in the projected space by projecting the intermediate vectors.\n",
    "X_steps = np.stack([v_king, v_king - v_man, v_combo]).astype(np.float32)\n",
    "Z_steps = PCA(n_components=2, random_state=7).fit(X).transform(X_steps)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=Z_steps[:, 0], y=Z_steps[:, 1], mode=\"lines+markers\", name=\"arithmetic\"))\n",
    "\n",
    "fig.update_layout(title=\"Vector arithmetic (king − man + woman) visualized (PCA)\", showlegend=False)\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Distance: Euclidean vs cosine (\"looking at stars\")\n",
    "\n",
    "Two common similarity choices:\n",
    "\n",
    "- **Euclidean distance**: cares about *direction* **and** *magnitude*\n",
    "- **Cosine similarity**: cares mostly about *direction* (angle)\n",
    "\n",
    "### The “stars” analogy\n",
    "Imagine each embedding as a direction in the sky.\n",
    "\n",
    "- With cosine, we mostly ask: **are these pointing in the same direction?**\n",
    "- Normalizing vectors puts everything on a unit sphere, so “closeness” becomes “angular closeness”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean vs cosine: a small 2D picture\n",
    "\n",
    "# Same direction, different magnitude\n",
    "u = np.array([1.0, 2.0], dtype=np.float32)\n",
    "u = u / (np.linalg.norm(u) + 1e-12)\n",
    "\n",
    "a = u * 1.0\n",
    "b = u * 5.0\n",
    "c = np.array([2.0, -1.0], dtype=np.float32)\n",
    "\n",
    "\n",
    "def euclid(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    return float(np.linalg.norm(x - y))\n",
    "\n",
    "\n",
    "print(\"Euclidean(a, b):\", euclid(a, b))\n",
    "print(\"Cosine(a, b):   \", cosine(a, b))\n",
    "print(\"Euclidean(a, c):\", euclid(a, c))\n",
    "print(\"Cosine(a, c):   \", cosine(a, c))\n",
    "\n",
    "# Plot vectors as arrows (lines from origin)\n",
    "pts = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"a (same dir)\", \"b (same dir)\", \"c (different dir)\"],\n",
    "        \"x\": [a[0], b[0], c[0]],\n",
    "        \"y\": [a[1], b[1], c[1]],\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "for _, r in pts.iterrows():\n",
    "    fig.add_trace(go.Scatter(x=[0, r.x], y=[0, r.y], mode=\"lines+markers+text\", text=[\"\", r[\"name\"]], textposition=\"top center\"))\n",
    "\n",
    "fig.update_layout(title=\"Euclidean vs cosine (2D vectors)\", xaxis_title=\"x\", yaxis_title=\"y\", showlegend=False)\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Sentence embeddings: average of word vectors + why attention mattered\n",
    "\n",
    "A simple way to embed a sentence is:\n",
    "\n",
    "- tokenize into words\n",
    "- look up each word vector\n",
    "- **average** them\n",
    "\n",
    "This works surprisingly well for a baseline.\n",
    "\n",
    "### Why attention was the breakthrough\n",
    "Averaging ignores word order and context. **Attention** made embeddings contextual (the vector for a word depends on surrounding words), which unlocked modern LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline sentence embedding via average of word vectors\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "def avg_glove(sentence: str) -> np.ndarray:\n",
    "    toks = TOKEN_RE.findall((sentence or \"\").lower())\n",
    "    vecs = [wv[t] for t in toks if t in wv]\n",
    "    if not vecs:\n",
    "        return np.zeros((wv.vector_size,), dtype=np.float32)\n",
    "    return np.mean(np.stack(vecs).astype(np.float32), axis=0)\n",
    "\n",
    "\n",
    "def cosine_np(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    x = x.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "    x = x / (np.linalg.norm(x) + 1e-12)\n",
    "    y = y / (np.linalg.norm(y) + 1e-12)\n",
    "    return float(x @ y)\n",
    "\n",
    "\n",
    "embedder_st = get_sentence_embedder()\n",
    "\n",
    "s1 = \"The cat sat on the mat.\"\n",
    "s2 = \"A feline rested on a rug.\"\n",
    "\n",
    "v1_avg = avg_glove(s1)\n",
    "v2_avg = avg_glove(s2)\n",
    "\n",
    "v1_st = embedder_st.encode([s1], normalize_embeddings=False, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "v2_st = embedder_st.encode([s2], normalize_embeddings=False, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "\n",
    "print(\"Sentence 1:\", s1)\n",
    "print(\"Sentence 2:\", s2)\n",
    "print()\n",
    "print(f\"cosine(avg-word-vectors): {cosine_np(v1_avg, v2_avg):0.3f}\")\n",
    "print(f\"cosine(sentence-transformers): {cosine_np(v1_st, v2_st):0.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) GPT‑2 mini demo: the model consumes embeddings\n",
    "\n",
    "LLMs don’t read raw text — they read **token IDs**, which are mapped through an **embedding table** into vectors.\n",
    "\n",
    "We’ll load GPT‑2 (an older LLM), inspect the embedding lookup shape, and generate a short continuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2: tokenize -> embedding lookup -> generate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "GPT2_ID = \"gpt2\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(GPT2_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(GPT2_ID)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Explain cosine similarity in one paragraph:\" \n",
    "inputs = tok(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"input_ids shape:\", tuple(input_ids.shape))\n",
    "\n",
    "# Embedding lookup (for GPT-2: transformer.wte)\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        emb = model.transformer.wte(input_ids)\n",
    "        print(\"token embeddings shape:\", tuple(emb.shape))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not access GPT-2 embedding table directly ({type(e).__name__}: {str(e)[:120]}).\")\n",
    "\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=80, do_sample=True, temperature=0.8, top_p=0.95)\n",
    "\n",
    "text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n=== GPT-2 output ===\\n\")\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G) Vector stores → similarity search → RAG\n",
    "\n",
    "A **vector store** is basically two things:\n",
    "\n",
    "- a place to store embeddings (vectors)\n",
    "- an index to retrieve the **top‑K most similar vectors** fast\n",
    "\n",
    "### Hash map vs vector store (analogy)\n",
    "- A hash map is for **exact lookup**: `value = map[key]` (fast when you already know the key).\n",
    "- A vector store is for **nearest-neighbor lookup**: `topK = nearest(query_vector)`.\n",
    "\n",
    "Different goal:\n",
    "- Hashing answers “**is this exact key present?**”\n",
    "- Vector search answers “**what is most similar to this meaning?**”\n",
    "\n",
    "Under the hood, most vector stores are optimizing one thing: return the best top‑K matches with minimal latency.\n",
    "\n",
    "Once you can retrieve the right chunks, **RAG** is the simple next step:\n",
    "\n",
    "1) embed the user question\n",
    "2) retrieve top‑K similar chunks\n",
    "3) give those chunks to a generator model to answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real corpus load (workshop): vectors + metadata\n",
    "# This powers the vector-store + radar + RBAC sections below.\n",
    "\n",
    "\n",
    "def _parse_json_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if not isinstance(val, str) or not val.strip():\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(val)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "csv_candidates = glob.glob(\"corpus_runs/**/fico_corpus_embedded.csv\", recursive=True)\n",
    "if not csv_candidates:\n",
    "    raise FileNotFoundError(\"No corpus found under corpus_runs/**/fico_corpus_embedded.csv\")\n",
    "\n",
    "csv_path = max(csv_candidates, key=lambda p: os.path.getmtime(p))\n",
    "run_dir = os.path.dirname(csv_path)\n",
    "\n",
    "emb_path = os.path.join(run_dir, \"fico_corpus_embeddings.npy\")\n",
    "pca_path = os.path.join(run_dir, \"fico_corpus_pca.json\")\n",
    "\n",
    "if not os.path.exists(emb_path) or not os.path.exists(pca_path):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing embeddings/PCA artifacts. Re-run generate_fico_corpus.py with --save-embeddings.\\n\"\n",
    "        f\"Expected: {emb_path} and {pca_path}\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Loading corpus: {csv_path}\")\n",
    "print(f\"✅ Loading embeddings: {emb_path}\")\n",
    "print(f\"✅ Loading PCA params: {pca_path}\")\n",
    "\n",
    "_df = pd.read_csv(csv_path)\n",
    "for col in [\"tags\", \"allowed_roles\", \"allowed_tenants\", \"restricted_tags\"]:\n",
    "    if col in _df.columns:\n",
    "        _df[col] = _df[col].apply(_parse_json_list)\n",
    "\n",
    "df = _df\n",
    "\n",
    "E = np.load(emb_path).astype(np.float32)  # (N, D)\n",
    "with open(pca_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    pca_payload = json.load(f)\n",
    "\n",
    "# Align embeddings row order to df by doc_id\n",
    "if \"doc_id\" not in df.columns:\n",
    "    raise ValueError(\"CSV is missing doc_id; rerun corpus generation.\")\n",
    "\n",
    "doc_ids_df = df[\"doc_id\"].astype(str).tolist()\n",
    "doc_ids_saved = [str(x) for x in pca_payload.get(\"doc_id_order\", [])]\n",
    "\n",
    "if len(doc_ids_saved) != len(doc_ids_df) or set(doc_ids_saved) != set(doc_ids_df):\n",
    "    raise ValueError(\"doc_id mismatch between CSV and PCA payload; rerun corpus generation.\")\n",
    "\n",
    "if doc_ids_saved != doc_ids_df:\n",
    "    idx = {doc_id: i for i, doc_id in enumerate(doc_ids_saved)}\n",
    "    order = [idx[x] for x in doc_ids_df]\n",
    "    E = E[order]\n",
    "\n",
    "# Pre-normalize embeddings so dot-product == cosine\n",
    "E_norm = E / (np.linalg.norm(E, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# PCA projection params used by the radar\n",
    "scaler_mean = np.array(pca_payload[\"scaler\"][\"mean\"], dtype=np.float32)\n",
    "scaler_scale = np.array(pca_payload[\"scaler\"][\"scale\"], dtype=np.float32)\n",
    "pca_mean = np.array(pca_payload[\"pca\"][\"mean\"], dtype=np.float32)\n",
    "pca_components = np.array(pca_payload[\"pca\"][\"components\"], dtype=np.float32)  # (3, D)\n",
    "\n",
    "# Query embedder (used for real semantic search)\n",
    "# Use the model recorded in the corpus artifacts; fall back to the workshop default.\n",
    "embed_model_id = pca_payload.get(\"embed_model\", ST_EMBED_MODEL_ID)\n",
    "embedder = SentenceTransformer(embed_model_id, device=DEVICE)\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} documents. embedding_dim={int(E_norm.shape[1])} embed_model={embed_model_id} device={DEVICE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store demo (real corpus): top‑K retrieval + RAG prompt\n",
    "\n",
    "QUERY = \"GPU latency impacts APR\"\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "def brute_topk(E_norm: np.ndarray, q_norm: np.ndarray, k: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sims = (E_norm @ q_norm).astype(np.float32)\n",
    "    k = min(int(k), sims.shape[0])\n",
    "    idx = np.argpartition(-sims, k - 1)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return idx, sims[idx]\n",
    "\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    v = embedder.encode([q], normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "    return v\n",
    "\n",
    "\n",
    "q_norm = embed_query(QUERY)\n",
    "\n",
    "# Brute-force top-K (O(N) scan). This is what vector indexes try to accelerate.\n",
    "t0 = time.perf_counter()\n",
    "idx, sims = brute_topk(E_norm, q_norm, TOP_K)\n",
    "dt_brute = time.perf_counter() - t0\n",
    "\n",
    "rows = []\n",
    "for i, s in zip(idx.tolist(), sims.tolist()):\n",
    "    r = df.iloc[int(i)]\n",
    "    rows.append(\n",
    "        {\n",
    "            \"cosine\": float(s),\n",
    "            \"doc_id\": str(r.get(\"doc_id\", \"\")),\n",
    "            \"title\": str(r.get(\"title\", \"\"))[:80],\n",
    "            \"doc_type\": str(r.get(\"doc_type\", \"\")),\n",
    "            \"tenant\": str(r.get(\"tenant_id\", \"\")),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"QUERY: {QUERY!r}\")\n",
    "print(f\"Brute-force topK time: {dt_brute*1000:.2f} ms over N={len(df)}\")\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "# Optional: FAISS top‑K (same math, faster at scale)\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "\n",
    "    if \"FAISS_INDEX\" not in globals():\n",
    "        FAISS_INDEX = faiss.IndexFlatIP(E_norm.shape[1])\n",
    "        FAISS_INDEX.add(E_norm.astype(np.float32))\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    D, I = FAISS_INDEX.search(q_norm.reshape(1, -1), TOP_K)\n",
    "    dt_faiss = time.perf_counter() - t0\n",
    "    print(f\"FAISS topK time: {dt_faiss*1000:.2f} ms\")\n",
    "except Exception as e:\n",
    "    print(f\"FAISS not available ({type(e).__name__}: {str(e)[:120]}).\")\n",
    "\n",
    "# RAG in one glance: retrieve top‑K, then build a prompt\n",
    "\n",
    "def _preview(row: pd.Series, max_chars: int = 350) -> str:\n",
    "    for k in (\"body_redacted\", \"body\", \"text\"):\n",
    "        if k in row.index and row.get(k) is not None:\n",
    "            s = str(row.get(k)).strip().replace(\"\\r\", \"\")\n",
    "            s = \"\\n\".join([ln for ln in s.splitlines() if ln.strip()])\n",
    "            return (s[:max_chars] + (\"…\" if len(s) > max_chars else \"\"))\n",
    "    return \"\"\n",
    "\n",
    "passages = []\n",
    "for i in idx.tolist()[:3]:\n",
    "    r = df.iloc[int(i)]\n",
    "    passages.append(_preview(r))\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join([p for p in passages if p])\n",
    "rag_prompt = (\n",
    "    \"You are a helpful assistant. Use ONLY the context.\\n\\n\"\n",
    "    f\"QUESTION:\\n{QUERY}\\n\\n\"\n",
    "    f\"CONTEXT:\\n{context}\\n\\n\"\n",
    "    \"ANSWER:\\n\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== RAG prompt (retrieve → generate) ===\\n\")\n",
    "print(rag_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) The Vector Radar: visualizing an embedding space\n",
    "\n",
    "This 3D plot is a **PCA projection** of the same embedding space used for similarity search.\n",
    "\n",
    "- Points close together are semantically related.\n",
    "- The exact same embeddings are what a vector store indexes for top‑K retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (No-op)\n",
    "# Corpus load happens above; leaving this cell so older links/cell numbers don’t break.\n",
    "\n",
    "print(\"✅ Corpus already loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Extra: more top‑K retrieval examples\n",
    "\n",
    "The real-corpus vector-store demo is already implemented above. This section is kept as optional extra space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional scratch space\n",
    "\n",
    "print(\"This cell is intentionally left as optional scratch space.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corpus Explorer (how the document corpus is constructed) ---\n",
    "# A compact dashboard to help learners understand what “a corpus” means in this workshop.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def _as_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def _safe_str(x):\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "\n",
    "def _preview_text(row: pd.Series, max_chars: int = 700) -> str:\n",
    "    # Prefer redacted body for workshop safety.\n",
    "    for k in [\"body_redacted\", \"body\", \"text\"]:\n",
    "        if k in row.index and row.get(k) is not None:\n",
    "            s = _safe_str(row.get(k)).strip()\n",
    "            if s:\n",
    "                s = s.replace(\"\\r\", \"\")\n",
    "                s = \"\\n\".join([ln for ln in s.splitlines() if ln.strip()])\n",
    "                return (s[:max_chars] + (\"…\" if len(s) > max_chars else \"\"))\n",
    "    return \"(no body/text field)\"\n",
    "\n",
    "\n",
    "def _doc_label(row: pd.Series) -> str:\n",
    "    doc_id = _safe_str(row.get(\"doc_id\", \"?\"))\n",
    "    cluster = _safe_str(row.get(\"cluster\", \"?\"))\n",
    "    doc_type = _safe_str(row.get(\"doc_type\", \"?\"))\n",
    "    tenant = _safe_str(row.get(\"tenant_id\", \"global\"))\n",
    "    title = _safe_str(row.get(\"title\", row.get(\"text\", \"(no title)\")))\n",
    "    title = \" \".join(title.split())\n",
    "    return f\"{doc_id} | {cluster} | {doc_type} | {tenant} | {title[:80]}\"\n",
    "\n",
    "\n",
    "def _explode_list_col(df0: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    if col not in df0.columns:\n",
    "        return pd.DataFrame({col: []})\n",
    "    s = df0[col].apply(_as_list)\n",
    "    return pd.DataFrame({col: [v for sub in s.tolist() for v in sub if _safe_str(v).strip()]})\n",
    "\n",
    "\n",
    "def _top_counts(df0: pd.DataFrame, col: str, top_n: int = 12) -> pd.DataFrame:\n",
    "    \"\"\"Return a 2-col df: [col, count] with stable column names across pandas versions.\"\"\"\n",
    "    if col not in df0.columns:\n",
    "        return pd.DataFrame({col: [], \"count\": []})\n",
    "\n",
    "    vc = df0[col].astype(str).fillna(\"(null)\").value_counts().head(int(top_n))\n",
    "\n",
    "    # pandas typically returns columns like [col, 'count'] already.\n",
    "    out = vc.rename(\"count\").reset_index()\n",
    "    if list(out.columns) == [\"index\", \"count\"]:\n",
    "        out = out.rename(columns={\"index\": col})\n",
    "    # If it's already [col, 'count'], leave it.\n",
    "    return out[[c for c in [col, \"count\"] if c in out.columns]]\n",
    "\n",
    "\n",
    "def _top_list_counts(df0: pd.DataFrame, col: str, top_n: int = 18) -> pd.DataFrame:\n",
    "    \"\"\"Same as _top_counts, but for list-valued columns stored as Python lists.\"\"\"\n",
    "    x = _explode_list_col(df0, col)\n",
    "    if x.empty:\n",
    "        return pd.DataFrame({col: [], \"count\": []})\n",
    "\n",
    "    vc = x[col].astype(str).value_counts().head(int(top_n))\n",
    "    out = vc.rename(\"count\").reset_index()\n",
    "    if list(out.columns) == [\"index\", \"count\"]:\n",
    "        out = out.rename(columns={\"index\": col})\n",
    "    return out[[c for c in [col, \"count\"] if c in out.columns]]\n",
    "\n",
    "\n",
    "def corpus_explorer(df: pd.DataFrame):\n",
    "    if df is None or len(df) == 0:\n",
    "        raise RuntimeError(\"df is missing/empty. Run the DATA LOADING cell first.\")\n",
    "\n",
    "    # Basic overview\n",
    "    n_docs = int(len(df))\n",
    "    n_clusters = int(df[\"cluster\"].nunique()) if \"cluster\" in df.columns else None\n",
    "    embed_dim = int(E_norm.shape[1]) if \"E_norm\" in globals() and hasattr(E_norm, \"shape\") else None\n",
    "\n",
    "    overview = {\n",
    "        \"documents\": n_docs,\n",
    "        \"clusters\": n_clusters,\n",
    "        \"embedding_dim\": embed_dim,\n",
    "        \"embed_model\": _safe_str(globals().get(\"embed_model_id\", \"\")) or _safe_str(getattr(globals().get(\"pca_payload\", {}), \"get\", lambda *_: \"\")(\"embed_model\", \"\")),\n",
    "        \"run_dir\": _safe_str(globals().get(\"run_dir\", \"\")),\n",
    "    }\n",
    "\n",
    "    df_schema = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": list(df.columns),\n",
    "            \"dtype\": [str(df[c].dtype) for c in df.columns],\n",
    "            \"example\": [\n",
    "                _safe_str(df[c].dropna().iloc[0])[:120] if df[c].notna().any() else \"\"\n",
    "                for c in df.columns\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Distributions\n",
    "    d_cluster = _top_counts(df, \"cluster\", top_n=20)\n",
    "    d_type = _top_counts(df, \"doc_type\", top_n=20)\n",
    "    d_tenant = _top_counts(df, \"tenant_id\", top_n=20)\n",
    "    d_tags = _top_list_counts(df, \"tags\", top_n=20)\n",
    "    d_roles = _top_list_counts(df, \"allowed_roles\", top_n=20)\n",
    "    d_restrict = _top_list_counts(df, \"restricted_tags\", top_n=20)\n",
    "\n",
    "    # Interactive doc viewer\n",
    "    sample_n = min(200, len(df))\n",
    "    df_sample = df.sample(n=sample_n, random_state=7).reset_index(drop=False).rename(columns={\"index\": \"row_index\"})\n",
    "    doc_opts = [( _doc_label(r), int(r[\"row_index\"]) ) for _, r in df_sample.iterrows()]\n",
    "\n",
    "    pick_doc = widgets.Dropdown(options=doc_opts, description=\"Document:\", layout=widgets.Layout(width=\"95%\"))\n",
    "    sim_k = widgets.IntSlider(value=5, min=3, max=12, step=1, description=\"Similar K:\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def render_doc(_=None):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            i = int(pick_doc.value)\n",
    "            row = df.iloc[i]\n",
    "\n",
    "            title = _safe_str(row.get(\"title\", row.get(\"text\", \"(no title)\")))\n",
    "            doc_id = _safe_str(row.get(\"doc_id\", i))\n",
    "            doc_type = _safe_str(row.get(\"doc_type\", \"kb\"))\n",
    "            tenant = _safe_str(row.get(\"tenant_id\", \"global\"))\n",
    "            cluster = _safe_str(row.get(\"cluster\", \"?\"))\n",
    "\n",
    "            tags = row.get(\"tags\", []) if \"tags\" in row.index else []\n",
    "            roles = row.get(\"allowed_roles\", []) if \"allowed_roles\" in row.index else []\n",
    "            tenants = row.get(\"allowed_tenants\", []) if \"allowed_tenants\" in row.index else []\n",
    "            restricted = row.get(\"restricted_tags\", []) if \"restricted_tags\" in row.index else []\n",
    "            redactions = int(row.get(\"redaction_count\", 0) or 0) if \"redaction_count\" in row.index else None\n",
    "\n",
    "            preview = _preview_text(row, max_chars=900)\n",
    "\n",
    "            card = widgets.HTML(\n",
    "                value=(\n",
    "                    f\"<div style='border:1px solid #e5e7eb; border-radius:12px; padding:14px; background:#fff'>\"\n",
    "                    f\"<div style='font-size:16px; font-weight:700; margin-bottom:6px'>{title}</div>\"\n",
    "                    f\"<div style='font-size:12px; color:#374151; margin-bottom:10px'>\"\n",
    "                    f\"<b>doc_id</b>: {doc_id} &nbsp; | &nbsp; <b>cluster</b>: {cluster} &nbsp; | &nbsp; <b>type</b>: {doc_type} &nbsp; | &nbsp; <b>tenant</b>: {tenant}\"\n",
    "                    f\"</div>\"\n",
    "                    f\"<div style='font-size:12px; color:#374151; margin-bottom:8px'>\"\n",
    "                    f\"<b>tags</b>: {tags} <br/>\"\n",
    "                    f\"<b>allowed_roles</b>: {roles} <br/>\"\n",
    "                    f\"<b>allowed_tenants</b>: {tenants} <br/>\"\n",
    "                    f\"<b>restricted_tags</b>: {restricted}\"\n",
    "                    + (f\"<br/><b>redaction_count</b>: {redactions}\" if redactions is not None else \"\")\n",
    "                    + \"</div>\"\n",
    "                    f\"<div style='font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace; font-size:12px; white-space:pre-wrap; background:#f9fafb; border-radius:10px; padding:10px'>\"\n",
    "                    f\"{preview}\"\n",
    "                    f\"</div>\"\n",
    "                    f\"</div>\"\n",
    "                )\n",
    "            )\n",
    "            display(card)\n",
    "\n",
    "            # Similar docs by embedding cosine\n",
    "            if \"E_norm\" in globals() and hasattr(E_norm, \"shape\"):\n",
    "                q = E_norm[int(i)]\n",
    "                sims = (E_norm @ q).astype(float)\n",
    "                sims[int(i)] = -1.0\n",
    "                k = int(sim_k.value)\n",
    "                idxs = np.argpartition(-sims, k - 1)[:k]\n",
    "                idxs = idxs[np.argsort(-sims[idxs])]\n",
    "                rows = []\n",
    "                for j in idxs:\n",
    "                    rj = df.iloc[int(j)]\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"cosine\": float(sims[int(j)]),\n",
    "                            \"doc_id\": _safe_str(rj.get(\"doc_id\", j)),\n",
    "                            \"cluster\": _safe_str(rj.get(\"cluster\", \"\")),\n",
    "                            \"doc_type\": _safe_str(rj.get(\"doc_type\", \"\")),\n",
    "                            \"tenant\": _safe_str(rj.get(\"tenant_id\", \"\")),\n",
    "                            \"title\": _safe_str(rj.get(\"title\", rj.get(\"text\", \"\")))[:90],\n",
    "                        }\n",
    "                    )\n",
    "                df_sim = pd.DataFrame(rows)\n",
    "                display(df_sim)\n",
    "\n",
    "    pick_doc.observe(render_doc, names=\"value\")\n",
    "    sim_k.observe(render_doc, names=\"value\")\n",
    "\n",
    "    # Tabs\n",
    "    tab = widgets.Tab()\n",
    "\n",
    "    # Overview tab\n",
    "    fig_cluster = px.bar(d_cluster, x=\"cluster\", y=\"count\", title=\"Docs by cluster\") if not d_cluster.empty else None\n",
    "    fig_type = px.bar(d_type, x=\"doc_type\", y=\"count\", title=\"Docs by doc_type\") if not d_type.empty else None\n",
    "    fig_tenant = px.bar(d_tenant, x=\"tenant_id\", y=\"count\", title=\"Docs by tenant\") if not d_tenant.empty else None\n",
    "\n",
    "    overview_box = widgets.VBox([\n",
    "        widgets.HTML(\n",
    "            \"<div style='padding:10px;border:1px solid #e5e7eb;border-radius:12px;background:#fff'>\"\n",
    "            \"<div style='font-size:16px;font-weight:700;margin-bottom:6px'>Corpus overview</div>\"\n",
    "            f\"<div style='font-size:12px;color:#374151'><pre style='margin:0'>{pd.Series(overview).to_string()}</pre></div>\"\n",
    "            \"</div>\"\n",
    "        ),\n",
    "        widgets.HTML(\"<div style='height:6px'></div>\"),\n",
    "        widgets.Output(),\n",
    "    ])\n",
    "\n",
    "    # Render plots in an Output so they look consistent in notebooks\n",
    "    out_overview = overview_box.children[-1]\n",
    "    with out_overview:\n",
    "        clear_output(wait=True)\n",
    "        if fig_cluster:\n",
    "            fig_cluster.show()\n",
    "        if fig_type:\n",
    "            fig_type.show()\n",
    "        if fig_tenant:\n",
    "            fig_tenant.show()\n",
    "\n",
    "    # Distributions tab\n",
    "    fig_tags = px.bar(d_tags, x=\"tags\", y=\"count\", title=\"Top tags\") if not d_tags.empty else None\n",
    "    fig_roles = px.bar(d_roles, x=\"allowed_roles\", y=\"count\", title=\"Top allowed_roles\") if not d_roles.empty else None\n",
    "    fig_restrict = px.bar(d_restrict, x=\"restricted_tags\", y=\"count\", title=\"Top restricted_tags\") if not d_restrict.empty else None\n",
    "\n",
    "    dist_out = widgets.Output()\n",
    "    with dist_out:\n",
    "        clear_output(wait=True)\n",
    "        if fig_tags:\n",
    "            fig_tags.show()\n",
    "        if fig_roles:\n",
    "            fig_roles.show()\n",
    "        if fig_restrict:\n",
    "            fig_restrict.show()\n",
    "\n",
    "    dist_box = widgets.VBox([\n",
    "        widgets.HTML(\n",
    "            \"<div style='padding:10px;border:1px solid #e5e7eb;border-radius:12px;background:#fff'>\"\n",
    "            \"<div style='font-size:16px;font-weight:700;margin-bottom:6px'>Distributions</div>\"\n",
    "            \"<div style='font-size:12px;color:#374151'>Tags/roles/restrictions are stored as JSON lists in the corpus CSV.</div>\"\n",
    "            \"</div>\"\n",
    "        ),\n",
    "        dist_out,\n",
    "    ])\n",
    "\n",
    "    # Schema tab\n",
    "    schema_out = widgets.Output()\n",
    "    with schema_out:\n",
    "        clear_output(wait=True)\n",
    "        display(df_schema.sort_values(\"column\"))\n",
    "\n",
    "    schema_box = widgets.VBox([\n",
    "        widgets.HTML(\n",
    "            \"<div style='padding:10px;border:1px solid #e5e7eb;border-radius:12px;background:#fff'>\"\n",
    "            \"<div style='font-size:16px;font-weight:700;margin-bottom:6px'>Schema</div>\"\n",
    "            \"<div style='font-size:12px;color:#374151'>This is the exact dataframe used by vector search + RBAC filtering.</div>\"\n",
    "            \"</div>\"\n",
    "        ),\n",
    "        schema_out,\n",
    "    ])\n",
    "\n",
    "    # Samples tab\n",
    "    samples_box = widgets.VBox([\n",
    "        widgets.HTML(\n",
    "            \"<div style='padding:10px;border:1px solid #e5e7eb;border-radius:12px;background:#fff'>\"\n",
    "            \"<div style='font-size:16px;font-weight:700;margin-bottom:6px'>Document viewer</div>\"\n",
    "            \"<div style='font-size:12px;color:#374151'>We prefer <b>body_redacted</b> previews to keep the workshop safe.</div>\"\n",
    "            \"</div>\"\n",
    "        ),\n",
    "        pick_doc,\n",
    "        widgets.HBox([sim_k]),\n",
    "        out,\n",
    "    ])\n",
    "\n",
    "    tab.children = [overview_box, schema_box, dist_box, samples_box]\n",
    "    tab.set_title(0, \"Overview\")\n",
    "    tab.set_title(1, \"Schema\")\n",
    "    tab.set_title(2, \"Distributions\")\n",
    "    tab.set_title(3, \"Samples\")\n",
    "\n",
    "    display(tab)\n",
    "    render_doc()\n",
    "\n",
    "\n",
    "corpus_explorer(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- INTERACTIVE RADAR (Real query projection) --\n",
    "\n",
    "\n",
    "def project_query_to_xyz(query: str):\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        return None\n",
    "\n",
    "    q_emb = embedder.encode([q], normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "\n",
    "    # Same transform used during corpus generation\n",
    "    x_scaled = (q_emb - scaler_mean) / (scaler_scale + 1e-12)\n",
    "    xyz = (x_scaled - pca_mean) @ pca_components.T\n",
    "    return xyz\n",
    "\n",
    "\n",
    "def plot_radar(inject_query=None):\n",
    "    fig = px.scatter_3d(\n",
    "        df,\n",
    "        x='x', y='y', z='z',\n",
    "        color='cluster',\n",
    "        symbol='cluster',\n",
    "        opacity=0.75,\n",
    "        title=\"FICO Vector Space Radar\",\n",
    "        hover_data={\n",
    "            \"title\": True,\n",
    "            \"doc_type\": True,\n",
    "            \"tenant_id\": True,\n",
    "            \"owner_team\": True,\n",
    "            \"redaction_count\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if inject_query:\n",
    "        xyz = project_query_to_xyz(inject_query)\n",
    "        if xyz is not None:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[float(xyz[0])],\n",
    "                    y=[float(xyz[1])],\n",
    "                    z=[float(xyz[2])],\n",
    "                    mode='markers+text',\n",
    "                    marker=dict(size=10, color='red', symbol='x'),\n",
    "                    text=[f\"QUERY: {inject_query}\"],\n",
    "                    textposition=\"top center\",\n",
    "                    name='Injected Query',\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=30))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Widget Setup\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a query (e.g., \"GPU latency impacts APR\")',\n",
    "    description='Query:',\n",
    ")\n",
    "button = widgets.Button(description=\"Inject Query\", button_style='danger')\n",
    "\n",
    "\n",
    "def on_click(b):\n",
    "    clear_output(wait=True)\n",
    "    display(ui)\n",
    "    plot_radar(text_input.value)\n",
    "\n",
    "\n",
    "button.on_click(on_click)\n",
    "ui = widgets.VBox([text_input, button])\n",
    "\n",
    "\n",
    "display(ui)\n",
    "plot_radar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: RBAC Security Filter (Extension)\n",
    "\n",
    "**Scenario:** A \"Junior Analyst\" tries to access sensitive data.\n",
    "\n",
    "**Task:** Filter search results based on `user_level`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- RBAC + REAL VECTOR SEARCH --\n",
    "# 1) Embed the query\n",
    "# 2) Retrieve top-K by cosine similarity over saved embeddings\n",
    "# 3) Apply RBAC filter (tenant/role/restricted tags)\n",
    "\n",
    "ROLE_LEVEL = {\n",
    "    \"public\": 1,\n",
    "    \"analyst\": 2,\n",
    "    \"risk_analyst\": 2,\n",
    "    \"sre\": 3,\n",
    "    \"security\": 3,\n",
    "    \"admin\": 3,\n",
    "}\n",
    "\n",
    "ALL_ROLES = list(ROLE_LEVEL.keys())\n",
    "\n",
    "\n",
    "def rbac_allow(row, user_role: str, user_tenant: str, clearance_tags: set[str]):\n",
    "    allowed_roles = row.get(\"allowed_roles\", []) or []\n",
    "    allowed_tenants = row.get(\"allowed_tenants\", []) or []\n",
    "    restricted_tags = row.get(\"restricted_tags\", []) or []\n",
    "\n",
    "    # Admin/security commonly have cross-tenant visibility.\n",
    "    if user_role in {\"admin\", \"security\"}:\n",
    "        tenant_ok = True\n",
    "    else:\n",
    "        tenant_ok = (\n",
    "            \"*\" in allowed_tenants\n",
    "            or (user_tenant in allowed_tenants)\n",
    "            or (str(row.get(\"tenant_id\", \"global\")) in [\"global\", user_tenant])\n",
    "        )\n",
    "\n",
    "    if \"public\" in allowed_roles:\n",
    "        role_ok = True\n",
    "    else:\n",
    "        role_ok = user_role in allowed_roles\n",
    "\n",
    "    if restricted_tags:\n",
    "        if user_role in {\"admin\", \"security\"}:\n",
    "            tags_ok = True\n",
    "        else:\n",
    "            tags_ok = set(restricted_tags).issubset(clearance_tags)\n",
    "    else:\n",
    "        tags_ok = True\n",
    "\n",
    "    ok = tenant_ok and role_ok and tags_ok\n",
    "\n",
    "    reason = []\n",
    "    if not tenant_ok:\n",
    "        reason.append(\"tenant\")\n",
    "    if not role_ok:\n",
    "        reason.append(\"role\")\n",
    "    if not tags_ok:\n",
    "        reason.append(\"restricted_tags\")\n",
    "\n",
    "    return ok, \"+\".join(reason) if reason else \"ok\"\n",
    "\n",
    "\n",
    "def vector_search(query: str, top_k: int = 30):\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        return [], None\n",
    "\n",
    "    q_emb = embedder.encode([q], normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)[0].astype(np.float32)\n",
    "    q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "    sims = E_norm @ q_norm\n",
    "\n",
    "    k = min(int(top_k), len(sims))\n",
    "    idx = np.argpartition(-sims, k - 1)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "\n",
    "    results = [(int(i), float(sims[int(i)])) for i in idx]\n",
    "    return results, q_emb\n",
    "\n",
    "\n",
    "def rbac_vector_search(query: str, user_role: str, user_tenant: str, clearance_tags: set[str], top_k: int = 30):\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        print(\"❌ Enter a query.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔎 Semantic search: '{q}' as role='{user_role}' tenant='{user_tenant}' clearance={sorted(list(clearance_tags))}\")\n",
    "\n",
    "    hits, _ = vector_search(q, top_k=top_k)\n",
    "    if not hits:\n",
    "        print(\"❌ No results.\")\n",
    "        return\n",
    "\n",
    "    visible = []\n",
    "    blocked_reasons = {}\n",
    "\n",
    "    for i, score in hits:\n",
    "        row = df.iloc[i]\n",
    "        ok, reason = rbac_allow(row, user_role, user_tenant, clearance_tags)\n",
    "        if ok:\n",
    "            visible.append((row, score))\n",
    "        else:\n",
    "            blocked_reasons[reason] = blocked_reasons.get(reason, 0) + 1\n",
    "\n",
    "    print(\"----- Results -----\")\n",
    "    if visible:\n",
    "        for row, score in visible[:20]:\n",
    "            title = row.get(\"title\", \"(no title)\")\n",
    "            doc_type = row.get(\"doc_type\", \"kb\")\n",
    "            tenant_id = row.get(\"tenant_id\", \"global\")\n",
    "            tags = row.get(\"tags\", []) or []\n",
    "            redactions = int(row.get(\"redaction_count\", 0) or 0)\n",
    "            redact_note = \" [has redactable lines]\" if redactions > 0 else \"\"\n",
    "            print(f\"✅ {score:0.3f} | {title} | type={doc_type} tenant={tenant_id} tags={tags}{redact_note}\")\n",
    "    else:\n",
    "        print(\"🚫 No visible documents in the top-K results.\")\n",
    "\n",
    "    blocked_count = len(hits) - len(visible)\n",
    "    if blocked_count > 0:\n",
    "        print(f\"\\n🔒 BLOCKED {blocked_count} of top-{len(hits)}\")\n",
    "        for k, v in blocked_reasons.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "\n",
    "        print(\"\\nBlocked examples (first 10):\")\n",
    "        shown = 0\n",
    "        for i, score in hits:\n",
    "            row = df.iloc[i]\n",
    "            ok, reason = rbac_allow(row, user_role, user_tenant, clearance_tags)\n",
    "            if ok:\n",
    "                continue\n",
    "            title = row.get(\"title\", \"(no title)\")\n",
    "            doc_type = row.get(\"doc_type\", \"kb\")\n",
    "            tenant_id = row.get(\"tenant_id\", \"global\")\n",
    "            allowed_roles = row.get(\"allowed_roles\", []) or []\n",
    "            restricted_tags = row.get(\"restricted_tags\", []) or []\n",
    "            print(f\"🚫 {score:0.3f} | {reason} | {title} | type={doc_type} tenant={tenant_id} roles={allowed_roles} restricted={restricted_tags}\")\n",
    "            shown += 1\n",
    "            if shown >= 10:\n",
    "                break\n",
    "\n",
    "\n",
    "# Widgets (optional)\n",
    "role_dropdown = widgets.Dropdown(options=ALL_ROLES, value=\"analyst\", description=\"Role:\")\n",
    "tenant_dropdown = widgets.Dropdown(options=[\"acme\", \"globex\", \"initech\"], value=\"acme\", description=\"Tenant:\")\n",
    "clearance_select = widgets.SelectMultiple(\n",
    "    options=[\"pii\", \"secrets\", \"customer-data\", \"prod\", \"credentials\"],\n",
    "    value=(),\n",
    "    description=\"Clearance:\",\n",
    ")\n",
    "topk_slider = widgets.IntSlider(value=30, min=5, max=100, step=5, description=\"Top-K:\")\n",
    "search_box = widgets.Text(placeholder='Try: \"GPU latency impacts APR\", \"Kubernetes incident runbook\"', description='Search:')\n",
    "btn_rbac = widgets.Button(description=\"Run Secure Semantic Search\")\n",
    "\n",
    "\n",
    "def on_rbac_click(b):\n",
    "    clear_output(wait=True)\n",
    "    display(ui_rbac)\n",
    "    rbac_vector_search(\n",
    "        search_box.value,\n",
    "        role_dropdown.value,\n",
    "        tenant_dropdown.value,\n",
    "        set(clearance_select.value),\n",
    "        top_k=int(topk_slider.value),\n",
    "    )\n",
    "\n",
    "\n",
    "btn_rbac.on_click(on_rbac_click)\n",
    "ui_rbac = widgets.VBox([role_dropdown, tenant_dropdown, clearance_select, topk_slider, search_box, btn_rbac])\n",
    "\n",
    "display(ui_rbac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RBAC Impact Metrics + Result Inspector ---\n",
    "# Works with the real semantic search state loaded earlier (df, E_norm, embedder, etc.).\n",
    "\n",
    "LAST_RUN = {}\n",
    "\n",
    "\n",
    "def rbac_breakdown(row, user_role: str, user_tenant: str, clearance_tags: set[str]):\n",
    "    allowed_roles = row.get(\"allowed_roles\", []) or []\n",
    "    allowed_tenants = row.get(\"allowed_tenants\", []) or []\n",
    "    restricted_tags = row.get(\"restricted_tags\", []) or []\n",
    "\n",
    "    if user_role in {\"admin\", \"security\"}:\n",
    "        tenant_ok = True\n",
    "    else:\n",
    "        tenant_ok = (\n",
    "            \"*\" in allowed_tenants\n",
    "            or (user_tenant in allowed_tenants)\n",
    "            or (str(row.get(\"tenant_id\", \"global\")) in [\"global\", user_tenant])\n",
    "        )\n",
    "\n",
    "    if \"public\" in allowed_roles:\n",
    "        role_ok = True\n",
    "    else:\n",
    "        role_ok = user_role in allowed_roles\n",
    "\n",
    "    if restricted_tags:\n",
    "        if user_role in {\"admin\", \"security\"}:\n",
    "            tags_ok = True\n",
    "        else:\n",
    "            tags_ok = set(restricted_tags).issubset(clearance_tags)\n",
    "    else:\n",
    "        tags_ok = True\n",
    "\n",
    "    return {\n",
    "        \"tenant_ok\": bool(tenant_ok),\n",
    "        \"role_ok\": bool(role_ok),\n",
    "        \"tags_ok\": bool(tags_ok),\n",
    "        \"allowed_roles\": allowed_roles,\n",
    "        \"allowed_tenants\": allowed_tenants,\n",
    "        \"restricted_tags\": restricted_tags,\n",
    "        \"user_role\": user_role,\n",
    "        \"user_tenant\": user_tenant,\n",
    "        \"user_clearance\": sorted(list(clearance_tags)),\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_rbac(hits, user_role: str, user_tenant: str, clearance_tags: set[str]):\n",
    "    visible = []\n",
    "    blocked = []\n",
    "    reason_counts = {}\n",
    "\n",
    "    for i, score in hits:\n",
    "        row = df.iloc[int(i)]\n",
    "        ok, reason = rbac_allow(row, user_role, user_tenant, clearance_tags)\n",
    "        if ok:\n",
    "            visible.append((int(i), float(score)))\n",
    "        else:\n",
    "            blocked.append((int(i), float(score), str(reason)))\n",
    "            reason_counts[str(reason)] = reason_counts.get(str(reason), 0) + 1\n",
    "\n",
    "    return visible, blocked, reason_counts\n",
    "\n",
    "\n",
    "def run_and_store(query: str, user_role: str, user_tenant: str, clearance_tags: set[str], top_k: int = 30):\n",
    "    hits, _ = vector_search(query, top_k=top_k)\n",
    "    visible, blocked, reasons = apply_rbac(hits, user_role, user_tenant, clearance_tags)\n",
    "\n",
    "    LAST_RUN.clear()\n",
    "    LAST_RUN.update(\n",
    "        {\n",
    "            \"query\": query,\n",
    "            \"top_k\": int(top_k),\n",
    "            \"user_role\": user_role,\n",
    "            \"user_tenant\": user_tenant,\n",
    "            \"clearance\": sorted(list(clearance_tags)),\n",
    "            \"hits\": hits,\n",
    "            \"visible\": visible,\n",
    "            \"blocked\": blocked,\n",
    "            \"blocked_reasons\": reasons,\n",
    "        }\n",
    "    )\n",
    "    return LAST_RUN\n",
    "\n",
    "\n",
    "def rbac_impact_metrics(query: str, tenant: str, clearance_tags: set[str], top_k: int = 30):\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        print(\"❌ Enter a query.\")\n",
    "        return\n",
    "\n",
    "    roles = [\"analyst\", \"security\"]\n",
    "    reports = {}\n",
    "\n",
    "    print(f\"\\n=== RBAC impact metrics for query: {q!r} (top_k={int(top_k)}) ===\")\n",
    "\n",
    "    for role in roles:\n",
    "        hits, _ = vector_search(q, top_k=top_k)\n",
    "        visible, blocked, reasons = apply_rbac(hits, role, tenant, clearance_tags)\n",
    "\n",
    "        reports[role] = {\n",
    "            \"hits\": hits,\n",
    "            \"visible\": visible,\n",
    "            \"blocked\": blocked,\n",
    "            \"reasons\": reasons,\n",
    "        }\n",
    "\n",
    "        print(f\"\\nRole={role} tenant={tenant} clearance={sorted(list(clearance_tags))}\")\n",
    "        print(f\"- topK before RBAC: {len(hits)}\")\n",
    "        print(f\"- topK after RBAC:  {len(visible)}\")\n",
    "        print(f\"- blocked:          {len(blocked)}\")\n",
    "        if reasons:\n",
    "            print(\"- blocked by reason:\")\n",
    "            for k, v in sorted(reasons.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "                print(f\"  - {k}: {v}\")\n",
    "        else:\n",
    "            print(\"- blocked by reason: (none)\")\n",
    "\n",
    "    # security vs analyst comparison\n",
    "    sec_ids = {i for i, _ in reports[\"security\"][\"visible\"]}\n",
    "    ana_ids = {i for i, _ in reports[\"analyst\"][\"visible\"]}\n",
    "    print(\"\\n=== security vs analyst comparison (visible set overlap) ===\")\n",
    "    print(f\"- security visible: {len(sec_ids)}\")\n",
    "    print(f\"- analyst visible:  {len(ana_ids)}\")\n",
    "    print(f\"- overlap:          {len(sec_ids & ana_ids)}\")\n",
    "    print(f\"- security-only:    {len(sec_ids - ana_ids)}\")\n",
    "\n",
    "\n",
    "def inspect_idx(i: int, user_role: str, user_tenant: str, clearance_tags: set[str], score: float | None = None):\n",
    "    row = df.iloc[int(i)]\n",
    "\n",
    "    ok, reason = rbac_allow(row, user_role, user_tenant, clearance_tags)\n",
    "    breakdown = rbac_breakdown(row, user_role, user_tenant, clearance_tags)\n",
    "\n",
    "    print(\"\\n=== Result inspector ===\")\n",
    "    if score is not None:\n",
    "        print(f\"score: {float(score):0.4f}\")\n",
    "    print(f\"doc_id: {row.get('doc_id')}\")\n",
    "    print(f\"pair_id: {row.get('pair_id')}\")\n",
    "    print(f\"cluster: {row.get('cluster')}\")\n",
    "    print(f\"tenant_id: {row.get('tenant_id')}\")\n",
    "    print(f\"doc_type: {row.get('doc_type')}\")\n",
    "    print(f\"owner_team: {row.get('owner_team')}\")\n",
    "    print(f\"tags: {row.get('tags')}\")\n",
    "    print(f\"restricted_tags: {row.get('restricted_tags')}\")\n",
    "    print(f\"allowed_roles: {row.get('allowed_roles')}\")\n",
    "    print(f\"allowed_tenants: {row.get('allowed_tenants')}\")\n",
    "    print(f\"redaction_count: {row.get('redaction_count')}\")\n",
    "\n",
    "    print(\"\\n--- RBAC decision ---\")\n",
    "    print(f\"visible: {ok} (reason={reason})\")\n",
    "    print(f\"tenant_ok={breakdown['tenant_ok']} role_ok={breakdown['role_ok']} tags_ok={breakdown['tags_ok']}\")\n",
    "\n",
    "    print(\"\\n--- TITLE ---\")\n",
    "    print(row.get(\"title\", \"\"))\n",
    "\n",
    "    print(\"\\n--- BODY (full) ---\")\n",
    "    print(row.get(\"body\", \"\"))\n",
    "\n",
    "    print(\"\\n--- BODY (redacted) ---\")\n",
    "    print(row.get(\"body_redacted\", \"\"))\n",
    "\n",
    "\n",
    "def inspect_last(rank: int = 0, which: str = \"visible\"):\n",
    "    if not LAST_RUN:\n",
    "        print(\"❌ No LAST_RUN yet. Run a search first (rbac_vector_search or run_and_store).\")\n",
    "        return\n",
    "\n",
    "    if which == \"visible\":\n",
    "        items = LAST_RUN.get(\"visible\", [])\n",
    "        if rank >= len(items):\n",
    "            print(f\"❌ rank {rank} out of range for visible (len={len(items)})\")\n",
    "            return\n",
    "        i, score = items[rank]\n",
    "    else:\n",
    "        items = LAST_RUN.get(\"hits\", [])\n",
    "        if rank >= len(items):\n",
    "            print(f\"❌ rank {rank} out of range for hits (len={len(items)})\")\n",
    "            return\n",
    "        i, score = items[rank]\n",
    "\n",
    "    inspect_idx(\n",
    "        i,\n",
    "        user_role=LAST_RUN.get(\"user_role\", \"analyst\"),\n",
    "        user_tenant=LAST_RUN.get(\"user_tenant\", \"acme\"),\n",
    "        clearance_tags=set(LAST_RUN.get(\"clearance\", [])),\n",
    "        score=score,\n",
    "    )\n",
    "\n",
    "\n",
    "# Optional widget UI for metrics + inspector\n",
    "metrics_btn = widgets.Button(description=\"RBAC impact metrics (security vs analyst)\")\n",
    "inspect_btn = widgets.Button(description=\"Inspect selected\")\n",
    "run_search_btn = widgets.Button(description=\"Run search (populate pick list)\")\n",
    "\n",
    "metrics_query = widgets.Text(value=\"Kubernetes incident runbook\", description=\"Query:\")\n",
    "metrics_role = widgets.Dropdown(options=[\"analyst\", \"sre\", \"security\", \"admin\"], value=\"security\", description=\"Role:\")\n",
    "metrics_tenant = widgets.Dropdown(options=[\"acme\", \"globex\", \"initech\"], value=\"acme\", description=\"Tenant:\")\n",
    "metrics_clearance = widgets.SelectMultiple(\n",
    "    options=[\"pii\", \"secrets\", \"customer-data\", \"prod\", \"credentials\"],\n",
    "    value=(),\n",
    "    description=\"Clearance:\",\n",
    ")\n",
    "metrics_topk = widgets.IntSlider(value=30, min=5, max=100, step=5, description=\"Top-K:\")\n",
    "\n",
    "# Pick list is populated by the button above (self-contained) or by other RBAC search cells.\n",
    "inspect_dropdown = widgets.Dropdown(options=[(\"(run a search first)\", -1)], description=\"Pick:\")\n",
    "\n",
    "\n",
    "def refresh_inspect_dropdown():\n",
    "    if not LAST_RUN:\n",
    "        inspect_dropdown.options = [(\"(run a search first)\", -1)]\n",
    "        return\n",
    "\n",
    "    opts = []\n",
    "    # show visible results first\n",
    "    for rank, (i, score) in enumerate(LAST_RUN.get(\"visible\", [])[:30]):\n",
    "        row = df.iloc[int(i)]\n",
    "        title = str(row.get(\"title\", \"(no title)\"))\n",
    "        opts.append((f\"v{rank} {score:0.3f} | {title[:80]}\", int(i)))\n",
    "\n",
    "    if not opts:\n",
    "        # fall back to raw hits if nothing visible\n",
    "        for rank, (i, score) in enumerate(LAST_RUN.get(\"hits\", [])[:30]):\n",
    "            row = df.iloc[int(i)]\n",
    "            title = str(row.get(\"title\", \"(no title)\"))\n",
    "            opts.append((f\"h{rank} {score:0.3f} | {title[:80]}\", int(i)))\n",
    "\n",
    "    inspect_dropdown.options = opts\n",
    "\n",
    "\n",
    "# Output panel so button actions always show results (and don't get wiped by clear_output)\n",
    "out_metrics = widgets.Output()\n",
    "\n",
    "\n",
    "def on_metrics_click(b):\n",
    "    with out_metrics:\n",
    "        clear_output(wait=True)\n",
    "        rbac_impact_metrics(metrics_query.value, metrics_tenant.value, set(metrics_clearance.value), top_k=int(metrics_topk.value))\n",
    "\n",
    "        # Keep the inspector pick list in sync with the same query/settings.\n",
    "        run_and_store(\n",
    "            metrics_query.value,\n",
    "            metrics_role.value,\n",
    "            metrics_tenant.value,\n",
    "            set(metrics_clearance.value),\n",
    "            top_k=int(metrics_topk.value),\n",
    "        )\n",
    "        refresh_inspect_dropdown()\n",
    "\n",
    "\n",
    "def on_run_search_click(b):\n",
    "    with out_metrics:\n",
    "        clear_output(wait=True)\n",
    "        run_and_store(\n",
    "            metrics_query.value,\n",
    "            metrics_role.value,\n",
    "            metrics_tenant.value,\n",
    "            set(metrics_clearance.value),\n",
    "            top_k=int(metrics_topk.value),\n",
    "        )\n",
    "        refresh_inspect_dropdown()\n",
    "\n",
    "        v = len(LAST_RUN.get(\"visible\", []) or [])\n",
    "        blk = len(LAST_RUN.get(\"blocked\", []) or [])\n",
    "        print(f\"Populated pick list (visible={v}, blocked={blk})\")\n",
    "\n",
    "\n",
    "def on_inspect_click(b):\n",
    "    with out_metrics:\n",
    "        clear_output(wait=True)\n",
    "        i = int(inspect_dropdown.value)\n",
    "        if i < 0:\n",
    "            print(\"❌ Run a search first to populate selectable results.\")\n",
    "            return\n",
    "        # Use current widget role/tenant if available from LAST_RUN, else defaults\n",
    "        role = LAST_RUN.get(\"user_role\", metrics_role.value)\n",
    "        tenant = LAST_RUN.get(\"user_tenant\", metrics_tenant.value)\n",
    "        clearance = set(LAST_RUN.get(\"clearance\", list(metrics_clearance.value)))\n",
    "        inspect_idx(i, role, tenant, clearance)\n",
    "\n",
    "\n",
    "metrics_btn.on_click(on_metrics_click)\n",
    "run_search_btn.on_click(on_run_search_click)\n",
    "inspect_btn.on_click(on_inspect_click)\n",
    "\n",
    "ui_metrics = widgets.VBox([\n",
    "    widgets.HTML(\"<b>RBAC impact metrics + result inspector</b>\"),\n",
    "    metrics_query,\n",
    "    widgets.HBox([metrics_role, metrics_tenant, metrics_topk]),\n",
    "    metrics_clearance,\n",
    "    widgets.HBox([metrics_btn, run_search_btn, inspect_btn]),\n",
    "    inspect_dropdown,\n",
    "    out_metrics,\n",
    "])\n",
    "\n",
    "# Call this manually after running a search, or just re-run this cell.\n",
    "refresh_inspect_dropdown()\n",
    "\n",
    "display(ui_metrics)\n",
    "\n",
    "# Direct-call examples (works even if widgets are broken):\n",
    "# rbac_impact_metrics(\"GPU latency impacts APR\", tenant=\"acme\", clearance_tags=set(), top_k=30)\n",
    "# run_and_store(\"Kubernetes incident runbook\", \"security\", \"acme\", set(), top_k=30); inspect_last(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NO-WIDGET FALLBACK ----\n",
    "# If ipywidgets don't render/click in your environment, run semantic search directly:\n",
    "\n",
    "print(\"\\n=== Non-widget demo (direct calls) ===\")\n",
    "\n",
    "# Security (cross-tenant) should see the most.\n",
    "rbac_vector_search(\"Kubernetes incident runbook\", \"security\", \"acme\", set(), top_k=30)\n",
    "\n",
    "print(\"\\n---\")\n",
    "# Analyst should see fewer (role-gated) and may hit restricted_tags blocks.\n",
    "rbac_vector_search(\"GPU latency impacts APR\", \"analyst\", \"acme\", set(), top_k=30)\n",
    "\n",
    "print(\"\\n---\")\n",
    "# Analyst with secrets clearance.\n",
    "rbac_vector_search(\"SECRET\", \"analyst\", \"acme\", {\"secrets\"}, top_k=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
