{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module C: RAG Architecture & Latency Profiler\n",
    "\n",
    "**Goal:** Meet the strict 1.5s SLA while maintaining accuracy.\n",
    "\n",
    "**Persona:** Solutions Architect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60-minute workshop flow (recommended)\n",
    "\n",
    "### Run of show\n",
    "- **0–10 min**: Environment + NIM preflight, then a fast **smoke test**.\n",
    "- **10–25 min**: Baseline latency waterfall + interpret where time goes.\n",
    "- **25–40 min**: **Top_K vs latency vs accuracy** (real eval + plots).\n",
    "- **40–55 min**: **Hit the 1.5s SLA** challenge (optimize config) + see accuracy tradeoffs.\n",
    "- **55–60 min**: Caching + robustness quick hits, then takeaways.\n",
    "\n",
    "### If you’re behind\n",
    "- Skip Phase 2 (RAGAS) and run only:\n",
    "  - Smoke test\n",
    "  - Top_K sweep\n",
    "  - SLA challenge\n",
    "\n",
    "### Checkpoints\n",
    "- **By minute 10**: you can run one query end-to-end and see an **answer + waterfall**.\n",
    "- **By minute 40**: you have **Top_K vs accuracy** and **Top_K vs latency** figures.\n",
    "- **By minute 55**: you found a config that **passes SLA** and you can explain the tradeoffs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Widgets are optional; notebook works without them.\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from rag_pipeline import (\n",
    "    RAGConfig,\n",
    "    Timer,\n",
    "    VectorIndex,\n",
    "    build_chunks_from_df,\n",
    "    chunk_text,\n",
    "    make_prompt,\n",
    "    normalize_rows,\n",
    "    clean_answer,\n",
    ")\n",
    "from nim_clients import NIMClient, NIMConfig\n",
    "\n",
    "print(\"✅ Module C initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment + kernel sanity checks (fail-fast) ---\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"=== Python kernel ===\")\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "\n",
    "print(\"\\n=== Core imports ===\")\n",
    "for name in [\"numpy\", \"pandas\", \"plotly\", \"torch\", \"transformers\", \"sentence_transformers\"]:\n",
    "    try:\n",
    "        mod = __import__(name)\n",
    "        ver = getattr(mod, \"__version__\", \"?\")\n",
    "        print(f\"✅ {name} {ver}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(\"\\n=== GPU / driver ===\")\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    print(\"torch.cuda.is_available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"torch.cuda.get_device_name:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"torch check failed:\", type(e).__name__, str(e)[:200])\n",
    "\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\", \"-L\"], stderr=subprocess.STDOUT, text=True)\n",
    "    print(out.strip())\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi unavailable:\", type(e).__name__, str(e)[:200])\n",
    "\n",
    "print(\"\\nIf you saw missing imports above, run:\\n  cd fico && ./scripts/setup_workshop.sh\\nThen restart the kernel and select: Python (fico)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NIM preflight + auto-select mode ---\n",
    "# This runs before we build models/indexes so you know early whether NIM is available.\n",
    "\n",
    "from nim_clients import NIMClient, NIMConfig\n",
    "\n",
    "nim_probe = NIMClient(NIMConfig())\n",
    "\n",
    "NIM_OK = False\n",
    "try:\n",
    "    _embs, _dt = nim_probe.embed([\"ping\"])\n",
    "    NIM_OK = True\n",
    "    print(f\"✅ NIM reachable at {nim_probe.cfg.base_url} (embed latency {float(_dt):.3f}s)\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ NIM not reachable at {nim_probe.cfg.base_url}: {type(e).__name__}: {str(e)[:180]}\")\n",
    "    print(\"To start local NIMs:\")\n",
    "    print(\"  cd fico && export NGC_API_KEY=... && ./scripts/start_nims.sh\")\n",
    "\n",
    "# If widgets exist later, we’ll set the default value when w_mode is created.\n",
    "DEFAULT_MODE = \"nim\" if NIM_OK else \"local\"\n",
    "print(\"Default mode:\", DEFAULT_MODE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Setup Notes\n",
    "\n",
    "### Models\n",
    "- **Local generation/judge**: `LOCAL_GEN_MODEL` (default: `meta-llama/Llama-3.1-8B-Instruct`)\n",
    "- **Local embedding**: `LOCAL_EMBED_MODEL` (default: `sentence-transformers/all-MiniLM-L6-v2`)\n",
    "- **Local reranker**: `LOCAL_RERANK_MODEL` (default: `cross-encoder/ms-marco-MiniLM-L-6-v2`)\n",
    "\n",
    "### NIM endpoints (localhost)\n",
    "This lab expects NIMs to be running locally and reachable via HTTP.\n",
    "\n",
    "You can override endpoints/models using env vars:\n",
    "- `NIM_API_KEY` (optional)\n",
    "- `NIM_EMBED_MODEL`, `NIM_RERANK_MODEL`, `NIM_GEN_MODEL`\n",
    "- `NIM_EMBED_PATH` (default `/v1/embeddings`)\n",
    "- `NIM_RERANK_PATH` (default `/v1/rerank`)\n",
    "- `NIM_CHAT_PATH` (default `/v1/chat/completions`)\n",
    "- `NIM_BASE_URL` (default `https://inference-fi8kkq0j0.brevlab.com`)\n",
    "\n",
    "### What is measured in the SLA waterfall\n",
    "- The waterfall measures **per-request latency** for query embed/retrieve/rerank/generate.\n",
    "- **Index building** (chunk embedding for the corpus) is cached and is not part of the SLA.\n",
    "\n",
    "### If widgets don’t work\n",
    "Scroll to the bottom for the **NO-WIDGET FALLBACK** cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Real Latency Waterfall (Local vs NIM)\n",
    "\n",
    "This phase runs a **real** RAG request and measures wall-clock time for:\n",
    "\n",
    "- Embed query\n",
    "- Retrieve (vector index)\n",
    "- Rerank\n",
    "- Generate\n",
    "\n",
    "Then it plots a measured waterfall and checks the 1.5s SLA.\n",
    "\n",
    "We run it in two modes:\n",
    "- **Local**: embedding + rerank + generation in-process\n",
    "- **NIM**: embedding + rerank + generation via localhost NIM endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load corpus from Module B run (single remaining run) ---\n",
    "RUN_DIR = Path(\"corpus_runs/llm_richer_n20_20251211_193028\")\n",
    "CSV_PATH = RUN_DIR / \"fico_corpus_embedded.csv\"\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing corpus CSV: {CSV_PATH}\")\n",
    "\n",
    "df_docs = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Parse list-like columns stored as JSON strings\n",
    "for col in [\"tags\", \"allowed_roles\", \"allowed_tenants\", \"restricted_tags\"]:\n",
    "    if col in df_docs.columns:\n",
    "        df_docs[col] = df_docs[col].apply(lambda v: json.loads(v) if isinstance(v, str) and v.strip().startswith(\"[\") else [])\n",
    "\n",
    "print(f\"✅ Loaded {len(df_docs)} docs from {CSV_PATH}\")\n",
    "\n",
    "\n",
    "# --- Build chunk dataset ---\n",
    "# Use redacted body to avoid leaking sensitive lines during workshops.\n",
    "chunks = build_chunks_from_df(df_docs)\n",
    "\n",
    "# Respect chunk settings via widgets; default chunking above is coarse, so we'll rebuild chunks when chunk_size changes.\n",
    "\n",
    "def build_chunks(chunk_size: int, overlap: int):\n",
    "    out = []\n",
    "    for _, row in df_docs.iterrows():\n",
    "        doc_id = str(row.get(\"doc_id\"))\n",
    "        title = row.get(\"title\")\n",
    "        tenant_id = row.get(\"tenant_id\")\n",
    "        doc_type = row.get(\"doc_type\")\n",
    "        tags = row.get(\"tags\")\n",
    "        body = row.get(\"body_redacted\") or row.get(\"body\") or \"\"\n",
    "        parts = chunk_text(str(body), chunk_size=chunk_size, overlap=overlap)\n",
    "        for j, part in enumerate(parts):\n",
    "            out.append(\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}::c{j:03d}\",\n",
    "                    \"title\": str(title) if title is not None else None,\n",
    "                    \"tenant_id\": str(tenant_id) if tenant_id is not None else None,\n",
    "                    \"doc_type\": str(doc_type) if doc_type is not None else None,\n",
    "                    \"tags\": list(tags) if isinstance(tags, list) else [],\n",
    "                    \"text\": part,\n",
    "                }\n",
    "            )\n",
    "    return out\n",
    "\n",
    "\n",
    "# --- Models ---\n",
    "# Local models\n",
    "LOCAL_EMBED_MODEL = os.environ.get(\"LOCAL_EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "LOCAL_RERANK_MODEL = os.environ.get(\"LOCAL_RERANK_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "# Text-only default (works with AutoModelForCausalLM). Override via env var.\n",
    "LOCAL_GEN_MODEL = os.environ.get(\"LOCAL_GEN_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "local_embedder = SentenceTransformer(LOCAL_EMBED_MODEL, device=\"cuda\")\n",
    "local_reranker = CrossEncoder(LOCAL_RERANK_MODEL, device=\"cuda\")\n",
    "\n",
    "# NIM client\n",
    "nim = NIMClient(NIMConfig())\n",
    "\n",
    "\n",
    "def embed_local(texts: list[str]):\n",
    "    emb = local_embedder.encode(texts, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "\n",
    "def rerank_local(query: str, docs: list[str]):\n",
    "    pairs = [(query, d) for d in docs]\n",
    "    scores = local_reranker.predict(pairs)\n",
    "    order = np.argsort(-np.array(scores))\n",
    "    return order.tolist(), scores\n",
    "\n",
    "\n",
    "_LOCAL_GEN = {\"tok\": None, \"model\": None}\n",
    "\n",
    "\n",
    "def ensure_local_generator():\n",
    "    if _LOCAL_GEN[\"tok\"] is not None and _LOCAL_GEN[\"model\"] is not None:\n",
    "        return _LOCAL_GEN[\"tok\"], _LOCAL_GEN[\"model\"]\n",
    "\n",
    "    import torch\n",
    "    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    trust_remote_code = str(os.environ.get(\"HF_TRUST_REMOTE_CODE\", \"0\")).lower() in {\"1\", \"true\", \"yes\"}\n",
    "\n",
    "    # Fail fast on known vision configs (e.g. GLM-4V) which cannot be loaded as a text CausalLM.\n",
    "    try:\n",
    "        cfg = AutoConfig.from_pretrained(LOCAL_GEN_MODEL, trust_remote_code=trust_remote_code)\n",
    "    except KeyError as e:\n",
    "        # Usually indicates your transformers version doesn't recognize a new `model_type`.\n",
    "        raise ValueError(\n",
    "            f\"Transformers in this environment doesn't recognize model_type={e!s} while loading LOCAL_GEN_MODEL={LOCAL_GEN_MODEL!r}. \"\n",
    "            \"Fix: (1) pick a simpler text model like TinyLlama/TinyLlama-1.1B-Chat-v1.0, or (2) upgrade transformers (pip install -U transformers), \"\n",
    "            \"or (3) set HF_TRUST_REMOTE_CODE=1 if the model requires custom loading code.\"\n",
    "        ) from e\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to load config for LOCAL_GEN_MODEL={LOCAL_GEN_MODEL!r}: {type(e).__name__}: {e}\"\n",
    "        ) from e\n",
    "\n",
    "    if cfg.__class__.__name__.lower().startswith(\"glm4v\") or getattr(cfg, \"model_type\", \"\") in {\"glm4v\", \"glm-4v\"}:\n",
    "        raise ValueError(\n",
    "            \"LOCAL_GEN_MODEL appears to be a vision (VLM) model config, which can't be loaded via AutoModelForCausalLM. \"\n",
    "            \"Set LOCAL_GEN_MODEL to a text-only causal LM (e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0) or switch Mode to 'nim'.\"\n",
    "        )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(LOCAL_GEN_MODEL, use_fast=True, trust_remote_code=trust_remote_code)\n",
    "    tok.padding_side = \"left\"\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LOCAL_GEN_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=dtype,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    _LOCAL_GEN[\"tok\"] = tok\n",
    "    _LOCAL_GEN[\"model\"] = model\n",
    "    return tok, model\n",
    "\n",
    "\n",
    "def gen_local(prompt: str, max_new_tokens: int, temperature: float):\n",
    "    import torch\n",
    "\n",
    "    tok, model = ensure_local_generator()\n",
    "\n",
    "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=float(temperature),\n",
    "            top_p=0.95,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
    "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def build_index_for_mode(mode: str, chunk_rows: list[dict], use_faiss: bool = True):\n",
    "    texts = [r[\"text\"] for r in chunk_rows]\n",
    "\n",
    "    if mode == \"local\":\n",
    "        E = embed_local(texts)\n",
    "        E_norm = normalize_rows(E)\n",
    "        return VectorIndex(E_norm, use_faiss=use_faiss), E_norm\n",
    "\n",
    "    if mode == \"nim\":\n",
    "        # Batch to avoid request-size limits\n",
    "        embs, _ = nim.embed_many(texts, batch_size=64)\n",
    "        E = np.array(embs, dtype=np.float32)\n",
    "        E_norm = normalize_rows(E)\n",
    "        return VectorIndex(E_norm, use_faiss=use_faiss), E_norm\n",
    "\n",
    "    raise ValueError(f\"unknown mode: {mode}\")\n",
    "\n",
    "\n",
    "def run_rag(query: str, *, mode: str, chunk_rows: list[dict], index: VectorIndex, cfg: RAGConfig):\n",
    "    timer = Timer()\n",
    "\n",
    "    # Embed query\n",
    "    if mode == \"local\":\n",
    "        q_emb = timer.time(\"embed\", lambda: embed_local([query])[0])\n",
    "        net_t = 0.0\n",
    "    else:\n",
    "        embs, net_t = timer.time(\"network+embed\", lambda: nim.embed([query]))\n",
    "        q_emb = np.array(embs[0], dtype=np.float32)\n",
    "\n",
    "    q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "    # Retrieve\n",
    "    idxs, sims = timer.time(\"retrieve\", lambda: index.search(q_norm, cfg.top_k))\n",
    "    idxs = [int(i) for i in idxs]\n",
    "\n",
    "    candidates = [chunk_rows[i] for i in idxs]\n",
    "\n",
    "    # Rerank\n",
    "    if cfg.rerank_top_k > 0:\n",
    "        docs = [c[\"text\"] for c in candidates]\n",
    "        if mode == \"local\":\n",
    "            order, _ = timer.time(\"rerank\", lambda: rerank_local(query, docs))\n",
    "        else:\n",
    "            order, _ = timer.time(\"network+rerank\", lambda: nim.rerank(query, docs, top_n=min(cfg.rerank_top_k, len(docs))))\n",
    "\n",
    "        # NIM rerank returns indices into docs; local returns order\n",
    "        if isinstance(order, list) and order and isinstance(order[0], int):\n",
    "            reranked = [candidates[i] for i in order[: cfg.rerank_top_k]]\n",
    "        else:\n",
    "            reranked = candidates[: cfg.rerank_top_k]\n",
    "    else:\n",
    "        reranked = candidates\n",
    "\n",
    "    # Context packing\n",
    "    context = []\n",
    "    used = 0\n",
    "    for r in reranked:\n",
    "        t = r[\"text\"]\n",
    "        if used + len(t) > cfg.max_context_chars:\n",
    "            break\n",
    "        context.append(r)\n",
    "        used += len(t)\n",
    "\n",
    "    from rag_pipeline import Chunk\n",
    "\n",
    "    context_chunks = [\n",
    "        Chunk(\n",
    "            doc_id=str(r.get(\"doc_id\")),\n",
    "            chunk_id=str(r.get(\"chunk_id\")),\n",
    "            text=str(r.get(\"text\")),\n",
    "            title=r.get(\"title\"),\n",
    "            tenant_id=r.get(\"tenant_id\"),\n",
    "            doc_type=r.get(\"doc_type\"),\n",
    "            tags=r.get(\"tags\") if isinstance(r.get(\"tags\"), list) else [],\n",
    "        )\n",
    "        for r in context\n",
    "    ]\n",
    "\n",
    "    prompt = make_prompt(query, context_chunks)\n",
    "\n",
    "    # Generate\n",
    "    if mode == \"local\":\n",
    "        answer = timer.time(\"generate\", lambda: gen_local(prompt, cfg.max_new_tokens, cfg.temperature))\n",
    "    else:\n",
    "        answer, _ = timer.time(\"network+generate\", lambda: nim.chat(prompt, max_tokens=cfg.max_new_tokens, temperature=cfg.temperature))\n",
    "\n",
    "    # Timings\n",
    "    timings = dict(timer.timings)\n",
    "    if net_t and \"network+embed\" not in timings:\n",
    "        timings[\"network+embed\"] = float(net_t)\n",
    "\n",
    "    return answer, context, timings\n",
    "\n",
    "\n",
    "def plot_measured_waterfall(timings_s: dict, title: str):\n",
    "    # Normalize keys into ordered components\n",
    "    keys = [\n",
    "        \"network+embed\",\n",
    "        \"embed\",\n",
    "        \"retrieve\",\n",
    "        \"network+rerank\",\n",
    "        \"rerank\",\n",
    "        \"network+generate\",\n",
    "        \"generate\",\n",
    "    ]\n",
    "    labels = {\n",
    "        \"network+embed\": \"NIM:Embed (HTTP)\",\n",
    "        \"embed\": \"Embed\",\n",
    "        \"retrieve\": \"Retrieve\",\n",
    "        \"network+rerank\": \"NIM:Rerank (HTTP)\",\n",
    "        \"rerank\": \"Rerank\",\n",
    "        \"network+generate\": \"NIM:Gen (HTTP)\",\n",
    "        \"generate\": \"Generate\",\n",
    "    }\n",
    "\n",
    "    parts = [(k, float(timings_s.get(k, 0.0))) for k in keys if float(timings_s.get(k, 0.0)) > 0]\n",
    "    total = sum(v for _, v in parts)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Waterfall(\n",
    "            name=\"Latency\",\n",
    "            orientation=\"v\",\n",
    "            measure=[\"relative\"] * len(parts) + [\"total\"],\n",
    "            x=[labels[k] for k, _ in parts] + [\"TOTAL\"],\n",
    "            textposition=\"outside\",\n",
    "            text=[f\"{v:.3f}s\" for _, v in parts] + [f\"{total:.3f}s\"],\n",
    "            y=[v for _, v in parts] + [0],\n",
    "            connector={\"line\": {\"color\": \"rgb(63, 63, 63)\"}},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_hline(\n",
    "        y=1.5,\n",
    "        line_dash=\"dot\",\n",
    "        annotation_text=\"SLA Limit (1.5s)\",\n",
    "        line_color=\"red\" if total > 1.5 else \"green\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=title, showlegend=False, yaxis=dict(title=\"Time (seconds)\", range=[0, max(2.5, total * 1.2)]))\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"⏱️ TOTAL LATENCY: {total:.3f}s\")\n",
    "    if total > 1.5:\n",
    "        print(\"❌ FAILED SLA. Optimize configuration!\")\n",
    "    else:\n",
    "        print(\"✅ PASSED SLA.\")\n",
    "\n",
    "\n",
    "# -- CONTROLS (optional) --\n",
    "style = {\"description_width\": \"initial\"}\n",
    "\n",
    "w_chunk = widgets.IntSlider(value=900, min=200, max=2000, description=\"Chunk Size (chars):\", style=style)\n",
    "w_overlap = widgets.IntSlider(value=150, min=0, max=500, description=\"Overlap (chars):\", style=style)\n",
    "w_k = widgets.IntSlider(value=10, min=1, max=50, description=\"Top_K Chunks:\", style=style)\n",
    "w_rerank_k = widgets.IntSlider(value=5, min=0, max=20, description=\"Rerank Top_K:\", style=style)\n",
    "\n",
    "w_query = widgets.Text(value=\"Kubernetes incident runbook\", description=\"Query:\")\n",
    "\n",
    "# Auto-default to NIM if reachable, otherwise local.\n",
    "_default_mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "w_mode = widgets.Dropdown(options=[\"local\", \"nim\"], value=_default_mode, description=\"Mode:\")\n",
    "btn_run = widgets.Button(description=\"Run RAG + plot latency\")\n",
    "btn_compare = widgets.Button(description=\"Compare Local vs NIM (SLA)\")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "# Cache indexes per mode + chunk settings so repeated runs are fast\n",
    "_INDEX_CACHE = {}\n",
    "\n",
    "\n",
    "def get_index(mode: str, chunk_size: int, overlap: int):\n",
    "    key = (mode, int(chunk_size), int(overlap))\n",
    "    if key in _INDEX_CACHE:\n",
    "        return _INDEX_CACHE[key]\n",
    "\n",
    "    rows = build_chunks(int(chunk_size), int(overlap))\n",
    "    index, _ = build_index_for_mode(mode, rows, use_faiss=True)\n",
    "    _INDEX_CACHE[key] = (rows, index)\n",
    "    return _INDEX_CACHE[key]\n",
    "\n",
    "\n",
    "def on_run_click(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        rows, index = get_index(w_mode.value, w_chunk.value, w_overlap.value)\n",
    "        cfg = RAGConfig(mode=w_mode.value, top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
    "        answer, context, timings = run_rag(w_query.value, mode=w_mode.value, chunk_rows=rows, index=index, cfg=cfg)\n",
    "        plot_measured_waterfall(timings, title=f\"RAG Latency ({w_mode.value})\")\n",
    "\n",
    "        cleaned = clean_answer(answer or \"\")\n",
    "        print(\"\\n--- Answer ---\")\n",
    "        if not cleaned:\n",
    "            print(\"(empty)\")\n",
    "        elif len(cleaned) > 4000:\n",
    "            print(cleaned[:4000] + \"\\n... [truncated] ...\")\n",
    "        else:\n",
    "            print(cleaned)\n",
    "\n",
    "\n",
    "def on_compare_click(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "\n",
    "        rows_l, idx_l = get_index(\"local\", w_chunk.value, w_overlap.value)\n",
    "        rows_n, idx_n = get_index(\"nim\", w_chunk.value, w_overlap.value)\n",
    "\n",
    "        cfg_l = RAGConfig(mode=\"local\", top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
    "        cfg_n = RAGConfig(mode=\"nim\", top_k=int(w_k.value), rerank_top_k=int(w_rerank_k.value))\n",
    "\n",
    "        q = w_query.value\n",
    "\n",
    "        ans_l, ctx_l, t_l = run_rag(q, mode=\"local\", chunk_rows=rows_l, index=idx_l, cfg=cfg_l)\n",
    "        ans_n, ctx_n, t_n = run_rag(q, mode=\"nim\", chunk_rows=rows_n, index=idx_n, cfg=cfg_n)\n",
    "\n",
    "        print(\"=== LOCAL ===\")\n",
    "        plot_measured_waterfall(t_l, title=\"RAG Latency (local)\")\n",
    "        print(\"\\n=== NIM ===\")\n",
    "        plot_measured_waterfall(t_n, title=\"RAG Latency (nim)\")\n",
    "\n",
    "        total_l = float(sum(t_l.values()))\n",
    "        total_n = float(sum(t_n.values()))\n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(f\"Local total: {total_l:.3f}s\")\n",
    "        print(f\"NIM total:   {total_n:.3f}s\")\n",
    "        print(f\"Delta (NIM-Local): {(total_n-total_l):+.3f}s\")\n",
    "\n",
    "\n",
    "btn_run.on_click(on_run_click)\n",
    "btn_compare.on_click(on_compare_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([w_mode, btn_run, btn_compare]),\n",
    "    w_query,\n",
    "    widgets.HBox([w_chunk, w_overlap]),\n",
    "    widgets.HBox([w_k, w_rerank_k]),\n",
    "    out,\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: 5-minute smoke test\n",
    "\n",
    "Run this once after **Phase 1 setup** (the big cell that defines functions and widgets). It should:\n",
    "- Build/reuse an index\n",
    "- Run one RAG query\n",
    "- Show an answer + latency waterfall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast smoke test (non-widget)\n",
    "\n",
    "SMOKE_QUERY = \"Kubernetes incident runbook\"\n",
    "SMOKE_TOP_K = 5\n",
    "SMOKE_RERANK_K = 3\n",
    "SMOKE_MAX_NEW_TOKENS = 80\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "print(\"Smoke test mode:\", mode)\n",
    "\n",
    "missing = [name for name in [\"get_index\", \"run_rag\", \"plot_measured_waterfall\"] if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Missing functions: \" + \", \".join(missing) + \"\\nRun the Phase 1 setup cell above first (the big cell that defines functions and widgets).\"\n",
    "    )\n",
    "\n",
    "rows, index = get_index(mode, chunk_size=900, overlap=150)\n",
    "cfg = RAGConfig(mode=mode, top_k=SMOKE_TOP_K, rerank_top_k=SMOKE_RERANK_K, max_new_tokens=SMOKE_MAX_NEW_TOKENS)\n",
    "ans, ctx, timings = run_rag(SMOKE_QUERY, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "\n",
    "plot_measured_waterfall(timings, title=f\"Smoke test latency ({mode})\")\n",
    "\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(clean_answer(ans)[:1200])\n",
    "print(\"\\n(Context chunks used:\", len(ctx), \")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presets: Fast vs Balanced vs Accurate\n",
    "\n",
    "These presets are meant to make the tradeoffs visible:\n",
    "- **Fast**: usually passes SLA, lower answer quality\n",
    "- **Balanced**: a reasonable default\n",
    "- **Accurate**: higher recall/precision, typically slower\n",
    "\n",
    "We’ll compare them with a **stacked latency breakdown** figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preset comparison (non-widget): stacked latency breakdown\n",
    "\n",
    "PRESETS = {\n",
    "    \"fast\": {\"chunk_size\": 900, \"overlap\": 150, \"top_k\": 6, \"rerank_top_k\": 0, \"max_new_tokens\": 80, \"temperature\": 0.2},\n",
    "    \"balanced\": {\"chunk_size\": 900, \"overlap\": 150, \"top_k\": 10, \"rerank_top_k\": 5, \"max_new_tokens\": 180, \"temperature\": 0.2},\n",
    "    \"accurate\": {\"chunk_size\": 900, \"overlap\": 150, \"top_k\": 20, \"rerank_top_k\": 10, \"max_new_tokens\": 220, \"temperature\": 0.2},\n",
    "}\n",
    "\n",
    "PRESET_QUERY = \"Kubernetes incident runbook\"\n",
    "\n",
    "missing = [name for name in [\"get_index\", \"run_rag\"] if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing functions: \" + \", \".join(missing) + \"\\nRun the Phase 1 setup cell above first.\")\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "rows, index = get_index(mode, chunk_size=900, overlap=150)\n",
    "\n",
    "records = []\n",
    "for name, p in PRESETS.items():\n",
    "    cfg = RAGConfig(\n",
    "        mode=mode,\n",
    "        top_k=int(p[\"top_k\"]),\n",
    "        rerank_top_k=int(p[\"rerank_top_k\"]),\n",
    "        max_new_tokens=int(p[\"max_new_tokens\"]),\n",
    "        temperature=float(p[\"temperature\"]),\n",
    "    )\n",
    "    ans, ctx, timings = run_rag(PRESET_QUERY, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "\n",
    "    # Normalize stage names for plotting\n",
    "    stage_map = {\n",
    "        \"network+embed\": \"embed\",\n",
    "        \"embed\": \"embed\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"network+rerank\": \"rerank\",\n",
    "        \"rerank\": \"rerank\",\n",
    "        \"network+generate\": \"generate\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    "    agg = {}\n",
    "    for k, v in timings.items():\n",
    "        kk = stage_map.get(k)\n",
    "        if kk:\n",
    "            agg[kk] = agg.get(kk, 0.0) + float(v)\n",
    "\n",
    "    total = float(sum(timings.values()))\n",
    "    records.append(\n",
    "        {\n",
    "            \"preset\": name,\n",
    "            \"total_s\": total,\n",
    "            \"embed_s\": agg.get(\"embed\", 0.0),\n",
    "            \"retrieve_s\": agg.get(\"retrieve\", 0.0),\n",
    "            \"rerank_s\": agg.get(\"rerank\", 0.0),\n",
    "            \"generate_s\": agg.get(\"generate\", 0.0),\n",
    "            \"answer_preview\": clean_answer(ans)[:180],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_p = pd.DataFrame(records).sort_values(\"total_s\")\n",
    "display(df_p[[\"preset\", \"total_s\", \"embed_s\", \"retrieve_s\", \"rerank_s\", \"generate_s\", \"answer_preview\"]])\n",
    "\n",
    "fig = px.bar(\n",
    "    df_p,\n",
    "    x=\"preset\",\n",
    "    y=[\"embed_s\", \"retrieve_s\", \"rerank_s\", \"generate_s\"],\n",
    "    title=f\"Preset latency breakdown (mode={mode})\",\n",
    "    labels={\"value\": \"seconds\", \"variable\": \"stage\"},\n",
    ")\n",
    "fig.update_layout(barmode=\"stack\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: RAG is just “context shopping” (and chunking is the aisle layout)\n",
    "\n",
    "RAG isn’t magic memory. It’s **prompt enrichment**:\n",
    "\n",
    "1) embed the query\n",
    "2) cosine-similarity search over chunk embeddings\n",
    "3) paste the top‑K chunks into the prompt\n",
    "4) let the LLM answer using that context\n",
    "\n",
    "If the answer isn’t in the retrieved context, the LLM can’t “remember” it.\n",
    "\n",
    "Small joke to keep us honest: if you hide the answer *between couch cushions* (a chunk boundary), retrieval can’t find it. The LLM will then do what all of us do under pressure: confidently improvise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking failure playground: the answer can literally fall between chunks.\n",
    "# (Uses the same cosine + VectorIndex path as the real pipeline.)\n",
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def _build_synth_doc(*, key_at: int = 790):\n",
    "    key = \"RECOVERY_KEY=BANANA-42-ALPHA-777\"\n",
    "\n",
    "    # Intentionally include lots of \"recovery\"-ish language as distractors.\n",
    "    filler = (\n",
    "        \"Incident runbook note: recovery steps, rollback guidance, postmortem reminders, \"\n",
    "        \"and lots of words about recovery that are not actually the key. \"\n",
    "    )\n",
    "\n",
    "    # Make a prefix of exactly key_at characters so we can force a boundary split for certain chunk sizes.\n",
    "    prefix = (filler * (key_at // len(filler) + 3))[:key_at]\n",
    "\n",
    "    suffix = (\n",
    "        \"\\n\\nMore notes: rotate credentials quarterly. Keep secrets out of Slack. \"\n",
    "        \"If you see the word banana in an incident, someone is probably having a bad day.\"\n",
    "    )\n",
    "\n",
    "    doc = prefix + key + suffix\n",
    "    return doc, key\n",
    "\n",
    "\n",
    "SYNTH_DOC, SYNTH_KEY = _build_synth_doc(key_at=790)\n",
    "\n",
    "\n",
    "def _chunk_spans(text: str, chunk_size: int, overlap: int):\n",
    "    \"\"\"Return [(chunk_id, start, end, chunk_text)].\"\"\"\n",
    "    chunk_size = max(50, int(chunk_size))\n",
    "    overlap = max(0, min(int(overlap), chunk_size - 1))\n",
    "\n",
    "    spans = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    j = 0\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        spans.append((j, start, end, text[start:end]))\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "        j += 1\n",
    "    return spans\n",
    "\n",
    "\n",
    "def _plot_chunk_strip(spans, *, key: str):\n",
    "    rows = []\n",
    "    for j, s, e, t in spans:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"chunk\": f\"c{j:02d}\",\n",
    "                \"start\": int(s),\n",
    "                \"end\": int(e),\n",
    "                \"len\": int(e - s),\n",
    "                \"has_key\": bool(key in t),\n",
    "                \"preview\": (t.replace(\"\\n\", \" \")[:120] + (\"…\" if len(t) > 120 else \"\")),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for r in rows:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[r[\"len\"]],\n",
    "                y=[\"doc\"],\n",
    "                base=[r[\"start\"]],\n",
    "                orientation=\"h\",\n",
    "                marker_color=(\"#16a34a\" if r[\"has_key\"] else \"#94a3b8\"),\n",
    "                hovertemplate=(\n",
    "                    \"<b>%{customdata[0]}</b><br>chars: %{base}–%{customdata[1]}<br>len=%{x}<br>has_key=%{customdata[2]}<br><br>%{customdata[3]}<extra></extra>\"\n",
    "                ),\n",
    "                customdata=[[r[\"chunk\"], r[\"end\"], r[\"has_key\"], r[\"preview\"]]],\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Chunk boundary strip (green = contains full key)\",\n",
    "        xaxis_title=\"character offset\",\n",
    "        yaxis=dict(visible=False),\n",
    "        barmode=\"stack\",\n",
    "        height=220,\n",
    "    )\n",
    "    return df, fig\n",
    "\n",
    "\n",
    "def _plot_similarities(df_chunks: pd.DataFrame, retrieved_ids: set[str]):\n",
    "    dfp = df_chunks.copy()\n",
    "    dfp[\"retrieved\"] = dfp[\"chunk\"].isin(retrieved_ids)\n",
    "\n",
    "    fig = px.bar(\n",
    "        dfp,\n",
    "        x=\"chunk\",\n",
    "        y=\"cosine\",\n",
    "        color=\"retrieved\",\n",
    "        title=\"Cosine similarity per chunk (highlighting Top‑K retrieved)\",\n",
    "        hover_data=[\"has_key\"],\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"chunk\", yaxis_title=\"cosine similarity\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _pack_context_text(texts: list[str], max_chars: int) -> list[str]:\n",
    "    out = []\n",
    "    used = 0\n",
    "    for t in texts:\n",
    "        if used + len(t) > int(max_chars):\n",
    "            break\n",
    "        out.append(t)\n",
    "        used += len(t)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --- Controls ---\n",
    "style = {\"description_width\": \"initial\"}\n",
    "\n",
    "w_demo_query = widgets.Text(\n",
    "    value=\"What is the RECOVERY_KEY?\", description=\"Query:\", layout=widgets.Layout(width=\"900px\"), style=style\n",
    ")\n",
    "\n",
    "w_demo_chunk = widgets.IntSlider(value=400, min=120, max=1400, step=20, description=\"Chunk size (chars)\", style=style)\n",
    "w_demo_overlap = widgets.IntSlider(value=0, min=0, max=400, step=20, description=\"Overlap (chars)\", style=style)\n",
    "w_demo_topk = widgets.IntSlider(value=3, min=1, max=12, step=1, description=\"Top‑K\", style=style)\n",
    "w_demo_ctx = widgets.IntSlider(value=900, min=200, max=4000, step=100, description=\"Max context chars\", style=style)\n",
    "\n",
    "out_demo = widgets.Output()\n",
    "\n",
    "\n",
    "def _render_demo(*_):\n",
    "    with out_demo:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Chunk the synthetic doc\n",
    "        spans = _chunk_spans(SYNTH_DOC, chunk_size=int(w_demo_chunk.value), overlap=int(w_demo_overlap.value))\n",
    "        df_strip, fig_strip = _plot_chunk_strip(spans, key=SYNTH_KEY)\n",
    "\n",
    "        # Build embeddings + VectorIndex over chunks (same math as the real pipeline)\n",
    "        chunk_texts = [t for _, _, _, t in spans]\n",
    "        E = embed_local(chunk_texts)\n",
    "        E_norm = normalize_rows(E)\n",
    "        idx = VectorIndex(E_norm, use_faiss=True)\n",
    "\n",
    "        q = str(w_demo_query.value)\n",
    "        q_emb = embed_local([q])[0]\n",
    "        q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "        topk = int(w_demo_topk.value)\n",
    "        I, sims = idx.search(q_norm, k=topk)\n",
    "\n",
    "        # Similarities for all chunks (for plotting)\n",
    "        all_sims = (E_norm @ q_norm.reshape(-1, 1)).reshape(-1)\n",
    "        df_chunks = df_strip[[\"chunk\", \"has_key\"]].copy()\n",
    "        df_chunks[\"cosine\"] = all_sims.astype(float)\n",
    "\n",
    "        retrieved = [int(i) for i in I]\n",
    "        retrieved_ids = {f\"c{i:02d}\" for i in retrieved}\n",
    "\n",
    "        # Pack context the same way run_rag does (truncate by max_context_chars)\n",
    "        retrieved_texts = [chunk_texts[i] for i in retrieved]\n",
    "        packed = _pack_context_text(retrieved_texts, max_chars=int(w_demo_ctx.value))\n",
    "\n",
    "        has_key_anywhere = any(SYNTH_KEY in t for t in packed)\n",
    "\n",
    "        print(\"=== Retrieval result ===\")\n",
    "        print(f\"chunk_size={int(w_demo_chunk.value)} overlap={int(w_demo_overlap.value)} top_k={topk} max_context_chars={int(w_demo_ctx.value)}\")\n",
    "        print(\"Key present in packed context:\", has_key_anywhere)\n",
    "        if not has_key_anywhere:\n",
    "            print(\"\\nIf the key isn’t in context, the LLM can’t cite it. It will either say ‘I don’t know’ or confidently hallucinate.\")\n",
    "\n",
    "        fig_strip.show()\n",
    "        _plot_similarities(df_chunks, retrieved_ids=retrieved_ids).show()\n",
    "\n",
    "        print(\"\\n=== Packed context preview (what the model actually sees) ===\")\n",
    "        for i, t in enumerate(packed, start=1):\n",
    "            snippet = t.replace(\"\\n\", \" \")\n",
    "            print(f\"\\n[chunk {i}/{len(packed)} | {len(t)} chars]\")\n",
    "            print(textwrap.shorten(snippet, width=450, placeholder=\" …\"))\n",
    "\n",
    "        # Quick \"extraction\" check (acts like a sanity oracle)\n",
    "        extracted = SYNTH_KEY if has_key_anywhere else None\n",
    "        print(\"\\n=== Quick extraction check ===\")\n",
    "        print(\"extracted:\", extracted)\n",
    "\n",
    "\n",
    "for w in [w_demo_query, w_demo_chunk, w_demo_overlap, w_demo_topk, w_demo_ctx]:\n",
    "    w.observe(_render_demo, names=\"value\")\n",
    "\n",
    "print(\"Tip: start with chunk_size=400 overlap=0 (usually broken), then try chunk_size=900 or overlap=200 (usually fixed).\")\n",
    "display(widgets.VBox([w_demo_query, widgets.HBox([w_demo_chunk, w_demo_overlap]), widgets.HBox([w_demo_topk, w_demo_ctx]), out_demo]))\n",
    "\n",
    "_render_demo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt builder: show the *exact* prompt that would be sent to the generator.\n",
    "# This uses the same context-packing rule as run_rag (max_context_chars).\n",
    "\n",
    "from rag_pipeline import Chunk\n",
    "\n",
    "\n",
    "out_prompt = widgets.Output()\n",
    "\n",
    "w_p_show = widgets.IntSlider(value=2400, min=400, max=12000, step=200, description=\"Show first N chars\", style=style)\n",
    "\n",
    "\n",
    "def _render_prompt(*_):\n",
    "    with out_prompt:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        spans = _chunk_spans(SYNTH_DOC, chunk_size=int(w_demo_chunk.value), overlap=int(w_demo_overlap.value))\n",
    "        chunk_texts = [t for _, _, _, t in spans]\n",
    "\n",
    "        # Build embeddings + retrieve (same as the demo above)\n",
    "        E = embed_local(chunk_texts)\n",
    "        E_norm = normalize_rows(E)\n",
    "        idx = VectorIndex(E_norm, use_faiss=True)\n",
    "\n",
    "        q = str(w_demo_query.value)\n",
    "        q_emb = embed_local([q])[0]\n",
    "        q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "        I, _ = idx.search(q_norm, k=int(w_demo_topk.value))\n",
    "        retrieved_texts = [chunk_texts[int(i)] for i in I]\n",
    "\n",
    "        packed = _pack_context_text(retrieved_texts, max_chars=int(w_demo_ctx.value))\n",
    "\n",
    "        ctx_chunks = [Chunk(doc_id=\"synth\", chunk_id=f\"synth::c{i:03d}\", text=t, title=\"SyntheticDoc\") for i, t in enumerate(packed)]\n",
    "        prompt = make_prompt(q, ctx_chunks)\n",
    "\n",
    "        print(\"=== Prompt stats ===\")\n",
    "        print(\"prompt_chars:\", len(prompt))\n",
    "        print(\"context_chunks_used:\", len(packed))\n",
    "        print(\"max_context_chars setting:\", int(w_demo_ctx.value))\n",
    "\n",
    "        show_n = int(w_p_show.value)\n",
    "        print(\"\\n=== Prompt preview ===\")\n",
    "        if len(prompt) <= show_n:\n",
    "            print(prompt)\n",
    "        else:\n",
    "            print(prompt[:show_n] + \"\\n... [truncated for display] ...\")\n",
    "\n",
    "\n",
    "for w in [w_demo_query, w_demo_chunk, w_demo_overlap, w_demo_topk, w_demo_ctx, w_p_show]:\n",
    "    w.observe(_render_prompt, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([w_p_show, out_prompt]))\n",
    "\n",
    "_render_prompt()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real accuracy evaluation: Gold QA set + local judge\n",
    "\n",
    "This section creates a small **gold evaluation set** from the corpus and uses a **local LLM judge** to score correctness.\n",
    "\n",
    "- The gold set is cached to disk so you don’t regenerate it every run.\n",
    "- Scores are **0/1/2** (incorrect/partial/correct).\n",
    "\n",
    "This is the backbone for the **Top_K vs accuracy** and **SLA vs accuracy** exercises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold set + local judge helpers\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "CACHE_DIR = Path(\".module_c_cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GOLD_PATH = CACHE_DIR / \"gold_eval.jsonl\"\n",
    "\n",
    "\n",
    "def _extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Best-effort JSON extraction from a model response.\"\"\"\n",
    "    s = (text or \"\").strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"empty model output\")\n",
    "\n",
    "    # Try direct parse first\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Extract first {...} block\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if not m:\n",
    "        raise ValueError(\"no JSON object found\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "\n",
    "def _ensure_local_llm():\n",
    "    \"\"\"Return (tokenizer, model) using ensure_local_generator if present.\"\"\"\n",
    "    if \"ensure_local_generator\" in globals():\n",
    "        return ensure_local_generator()\n",
    "\n",
    "    # Fallback: define a minimal loader using LOCAL_GEN_MODEL\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_id = os.environ.get(\"LOCAL_GEN_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    tok.padding_side = \"left\"\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=dtype)\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "\n",
    "def llm_generate_deterministic(prompt: str, *, max_new_tokens: int = 220) -> str:\n",
    "    \"\"\"Deterministic generation (good for eval/judging).\"\"\"\n",
    "    import torch\n",
    "\n",
    "    tok, model = _ensure_local_llm()\n",
    "\n",
    "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
    "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def build_gold_eval(*, n: int = 6, seed: int = 42, force: bool = False):\n",
    "    \"\"\"Build (or load) a small gold set from the corpus: question + reference answer.\"\"\"\n",
    "    if GOLD_PATH.exists() and not force:\n",
    "        rows = [json.loads(line) for line in GOLD_PATH.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "        return rows\n",
    "\n",
    "    if \"df_docs\" not in globals():\n",
    "        raise RuntimeError(\"df_docs is not defined. Run the Phase 1 setup cell that loads the corpus first.\")\n",
    "\n",
    "    sample = df_docs.sample(n=min(int(n), len(df_docs)), random_state=int(seed))\n",
    "    out = []\n",
    "\n",
    "    for _, r in sample.iterrows():\n",
    "        doc_id = str(r.get(\"doc_id\"))\n",
    "        ctx = str(r.get(\"body_redacted\") or r.get(\"body\") or \"\")\n",
    "        ctx = ctx[:1800]\n",
    "\n",
    "        prompt = (\n",
    "            \"You are creating a grounded evaluation example for a retrieval-augmented QA system.\\n\"\n",
    "            \"Return ONLY valid JSON with keys: question, reference_answer, expected_keywords.\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- The question MUST be answerable ONLY from the context.\\n\"\n",
    "            \"- The reference_answer MUST be a short factual answer grounded in the context.\\n\"\n",
    "            \"- expected_keywords is a list of 3-6 short strings that should appear in a correct answer.\\n\\n\"\n",
    "            f\"CONTEXT:\\n{ctx}\\n\\nJSON:\" \n",
    "        )\n",
    "\n",
    "        raw = llm_generate_deterministic(prompt, max_new_tokens=240)\n",
    "        try:\n",
    "            j = _extract_json_object(raw)\n",
    "            q = str(j.get(\"question\") or \"\").strip()\n",
    "            a = str(j.get(\"reference_answer\") or \"\").strip()\n",
    "            kws = j.get(\"expected_keywords\")\n",
    "            if not isinstance(kws, list):\n",
    "                kws = []\n",
    "            kws = [str(x).strip() for x in kws if str(x).strip()]\n",
    "\n",
    "            # Minimal sanity\n",
    "            if len(q) < 8 or len(a) < 8:\n",
    "                raise ValueError(\"question/answer too short\")\n",
    "\n",
    "        except Exception:\n",
    "            # Fallback: still real (derived from corpus), avoids workshop hard-fail.\n",
    "            q = f\"What is a key point from document {doc_id}?\"\n",
    "            a = (ctx.splitlines()[0] if ctx else \"I don't know\").strip()[:240]\n",
    "            kws = []\n",
    "\n",
    "        out.append({\"doc_id\": doc_id, \"question\": q, \"reference_answer\": a, \"expected_keywords\": kws})\n",
    "\n",
    "    # Persist\n",
    "    with open(GOLD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in out:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def judge_answer(question: str, reference_answer: str, candidate_answer: str):\n",
    "    \"\"\"Return {score:0|1|2, rationale:str} using a local LLM judge.\"\"\"\n",
    "    prompt = (\n",
    "        \"You are grading a system answer against a reference answer.\\n\"\n",
    "        \"Score rubric:\\n\"\n",
    "        \"- 2 = correct (matches reference in meaning; may be phrased differently)\\n\"\n",
    "        \"- 1 = partially correct (some correct info but missing/incorrect key parts)\\n\"\n",
    "        \"- 0 = incorrect or not supported\\n\\n\"\n",
    "        \"Return ONLY valid JSON with keys: score, rationale.\\n\"\n",
    "        \"rationale should be one short sentence.\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\nREFERENCE_ANSWER:\\n{reference_answer}\\n\\nCANDIDATE_ANSWER:\\n{candidate_answer}\\n\\nJSON:\"\n",
    "    )\n",
    "\n",
    "    raw = llm_generate_deterministic(prompt, max_new_tokens=180)\n",
    "    try:\n",
    "        j = _extract_json_object(raw)\n",
    "        score = int(j.get(\"score\"))\n",
    "        score = 0 if score < 0 else 2 if score > 2 else score\n",
    "        rationale = str(j.get(\"rationale\") or \"\").strip()\n",
    "        return {\"score\": score, \"rationale\": rationale}\n",
    "    except Exception:\n",
    "        # Non-mocked fallback: simple keyword overlap against reference.\n",
    "        ref = (reference_answer or \"\").lower()\n",
    "        cand = (candidate_answer or \"\").lower()\n",
    "        hits = sum(1 for w in set(ref.split()) if len(w) > 4 and w in cand)\n",
    "        score = 2 if hits >= 4 else 1 if hits >= 2 else 0\n",
    "        return {\"score\": score, \"rationale\": \"fallback keyword-overlap judge\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build/load the gold evaluation set\n",
    "\n",
    "GOLD_N = int(os.environ.get(\"GOLD_N\", \"6\"))\n",
    "GOLD_SEED = int(os.environ.get(\"GOLD_SEED\", \"42\"))\n",
    "\n",
    "print(f\"Loading gold set from {GOLD_PATH} (n={GOLD_N}, seed={GOLD_SEED})\")\n",
    "\n",
    "gold_eval = build_gold_eval(n=GOLD_N, seed=GOLD_SEED, force=False)\n",
    "\n",
    "df_gold = pd.DataFrame(gold_eval)\n",
    "display(df_gold[[\"doc_id\", \"question\", \"reference_answer\"]])\n",
    "\n",
    "# Pretty summary figure\n",
    "fig = px.histogram(\n",
    "    df_gold,\n",
    "    x=df_gold[\"reference_answer\"].astype(str).str.len(),\n",
    "    nbins=12,\n",
    "    title=\"Gold set: reference answer length distribution (chars)\",\n",
    "    labels={\"x\": \"reference_answer length (chars)\", \"y\": \"count\"},\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: judge scores a real RAG run (not mocked)\n",
    "\n",
    "if not gold_eval:\n",
    "    raise RuntimeError(\"gold_eval is empty\")\n",
    "\n",
    "ex = gold_eval[0]\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "missing = [name for name in [\"get_index\", \"run_rag\"] if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing functions: \" + \", \".join(missing) + \"\\nRun the Phase 1 setup cell above first.\")\n",
    "\n",
    "rows, index = get_index(mode, chunk_size=900, overlap=150)\n",
    "cfg = RAGConfig(mode=mode, top_k=10, rerank_top_k=5, max_new_tokens=160, temperature=0.2)\n",
    "\n",
    "ans, ctx, timings = run_rag(ex[\"question\"], mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "ans_clean = clean_answer(ans)\n",
    "\n",
    "retrieval_hit = any(str(c.get(\"doc_id\")) == str(ex[\"doc_id\"]) for c in ctx)\n",
    "judge = judge_answer(ex[\"question\"], ex[\"reference_answer\"], ans_clean)\n",
    "\n",
    "print(\"Question:\", ex[\"question\"])\n",
    "print(\"Reference:\", ex[\"reference_answer\"])\n",
    "print(\"\\nCandidate:\", ans_clean[:600])\n",
    "print(\"\\nRetrieval hit (source doc in context):\", retrieval_hit)\n",
    "print(\"Judge score:\", judge[\"score\"], \"|\", judge.get(\"rationale\", \"\"))\n",
    "\n",
    "fig = px.bar(\n",
    "    pd.DataFrame([{\"score\": int(judge[\"score\"])}]),\n",
    "    x=[\"score\"],\n",
    "    y=[int(judge[\"score\"])],\n",
    "    range_y=[0, 2],\n",
    "    title=\"Judge score (0=incorrect, 2=correct)\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top‑K: coverage vs accuracy vs latency (best practices)\n",
    "\n",
    "When you change **Top‑K**, you’re mostly changing two things:\n",
    "\n",
    "- **Coverage (recall)**: did we *retrieve* the right evidence at all?\n",
    "  - In this notebook we measure it as **retrieval hit‑rate**: “did the gold doc show up in the retrieved context?”\n",
    "- **Accuracy**: did the model answer correctly once it had context?\n",
    "  - Here it’s the **judge score (0/1/2)**.\n",
    "- **Latency**: bigger Top‑K usually costs more:\n",
    "  - retrieval can get slower as you ask for more results\n",
    "  - reranking gets more expensive (more pairs)\n",
    "  - generation can get slower if the context grows\n",
    "\n",
    "### Practical defaults (a good starting point)\n",
    "- **Retriever `top_k`**: start around **10–30**.\n",
    "- **Rerank** (if you have it): retrieve bigger (e.g. **20–50**) then **rerank down to 5–10**.\n",
    "- **Context budget**: always enforce a cap (this notebook uses `max_context_chars`). Otherwise your prompt becomes a “junk drawer with punctuation.”\n",
    "\n",
    "### Common failure modes\n",
    "- **Top‑K too small**: great latency, terrible recall (you never fetch the right chunk).\n",
    "- **Top‑K too large**: high recall, but noisy context can *reduce* answer quality (“where did this random paragraph come from?”).\n",
    "\n",
    "Next: we’ll run a real sweep and plot **accuracy vs Top‑K**, **latency vs Top‑K**, and **retrieval hit‑rate vs Top‑K**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top_K vs Accuracy vs Latency (real evaluation)\n",
    "\n",
    "This sweep varies **Top_K** and measures:\n",
    "- **Accuracy** (local judge score 0/1/2 against reference answers)\n",
    "- **Latency** (total + per-stage breakdown)\n",
    "- **Retrieval hit-rate** (did we retrieve the gold doc into context?)\n",
    "\n",
    "This is the key demonstration of how shrinking or growing Top_K affects quality and performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top_K sweep (real): runs RAG + local judge on the gold set\n",
    "\n",
    "TOPK_VALUES = [3, 5, 8, 10, 15, 20, 30]\n",
    "FIXED_RERANK_K = 5\n",
    "FIXED_MAX_NEW_TOKENS = 140\n",
    "FIXED_TEMP = 0.2\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "missing = [name for name in [\"get_index\", \"run_rag\"] if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing functions: \" + \", \".join(missing) + \"\\nRun the Phase 1 setup cell above first.\")\n",
    "\n",
    "if \"gold_eval\" not in globals() or not gold_eval:\n",
    "    raise RuntimeError(\"gold_eval is missing/empty. Run the gold eval cells above first.\")\n",
    "\n",
    "rows, index = get_index(mode, chunk_size=900, overlap=150)\n",
    "\n",
    "rows_out = []\n",
    "for top_k in TOPK_VALUES:\n",
    "    for i, ex in enumerate(gold_eval):\n",
    "        q = ex[\"question\"]\n",
    "        ref = ex[\"reference_answer\"]\n",
    "        gold_doc = str(ex[\"doc_id\"])\n",
    "\n",
    "        cfg = RAGConfig(\n",
    "            mode=mode,\n",
    "            top_k=int(top_k),\n",
    "            rerank_top_k=int(FIXED_RERANK_K),\n",
    "            max_new_tokens=int(FIXED_MAX_NEW_TOKENS),\n",
    "            temperature=float(FIXED_TEMP),\n",
    "        )\n",
    "\n",
    "        ans, ctx, timings = run_rag(q, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "        ans_clean = clean_answer(ans)\n",
    "\n",
    "        retrieval_hit = any(str(c.get(\"doc_id\")) == gold_doc for c in ctx)\n",
    "        judge = judge_answer(q, ref, ans_clean)\n",
    "\n",
    "        # Aggregate stage timings into canonical stages\n",
    "        stage_map = {\n",
    "            \"network+embed\": \"embed\",\n",
    "            \"embed\": \"embed\",\n",
    "            \"retrieve\": \"retrieve\",\n",
    "            \"network+rerank\": \"rerank\",\n",
    "            \"rerank\": \"rerank\",\n",
    "            \"network+generate\": \"generate\",\n",
    "            \"generate\": \"generate\",\n",
    "        }\n",
    "        stage = {\"embed\": 0.0, \"retrieve\": 0.0, \"rerank\": 0.0, \"generate\": 0.0}\n",
    "        for k, v in timings.items():\n",
    "            kk = stage_map.get(k)\n",
    "            if kk:\n",
    "                stage[kk] += float(v)\n",
    "\n",
    "        rows_out.append(\n",
    "            {\n",
    "                \"top_k\": int(top_k),\n",
    "                \"ex_id\": int(i),\n",
    "                \"gold_doc_id\": gold_doc,\n",
    "                \"retrieval_hit\": bool(retrieval_hit),\n",
    "                \"score\": int(judge[\"score\"]),\n",
    "                \"total_s\": float(sum(timings.values())),\n",
    "                \"embed_s\": stage[\"embed\"],\n",
    "                \"retrieve_s\": stage[\"retrieve\"],\n",
    "                \"rerank_s\": stage[\"rerank\"],\n",
    "                \"generate_s\": stage[\"generate\"],\n",
    "                \"answer_len\": int(len(ans_clean)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_runs = pd.DataFrame(rows_out)\n",
    "display(df_runs.head(10))\n",
    "\n",
    "# Aggregate\n",
    "agg = (\n",
    "    df_runs.groupby(\"top_k\")\n",
    "    .agg(\n",
    "        mean_score=(\"score\", \"mean\"),\n",
    "        std_score=(\"score\", \"std\"),\n",
    "        mean_total_s=(\"total_s\", \"mean\"),\n",
    "        std_total_s=(\"total_s\", \"std\"),\n",
    "        hit_rate=(\"retrieval_hit\", \"mean\"),\n",
    "        mean_embed_s=(\"embed_s\", \"mean\"),\n",
    "        mean_retrieve_s=(\"retrieve_s\", \"mean\"),\n",
    "        mean_rerank_s=(\"rerank_s\", \"mean\"),\n",
    "        mean_generate_s=(\"generate_s\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .fillna(0.0)\n",
    ")\n",
    "\n",
    "display(agg)\n",
    "\n",
    "# Figures\n",
    "fig_acc = px.line(\n",
    "    agg,\n",
    "    x=\"top_k\",\n",
    "    y=\"mean_score\",\n",
    "    error_y=\"std_score\",\n",
    "    markers=True,\n",
    "    title=f\"Accuracy vs Top_K (mode={mode}, judge score 0–2)\",\n",
    "    labels={\"mean_score\": \"mean judge score\"},\n",
    ")\n",
    "fig_acc.update_yaxes(range=[-0.05, 2.05])\n",
    "fig_acc.show()\n",
    "\n",
    "fig_lat = px.line(\n",
    "    agg,\n",
    "    x=\"top_k\",\n",
    "    y=\"mean_total_s\",\n",
    "    error_y=\"std_total_s\",\n",
    "    markers=True,\n",
    "    title=f\"Latency vs Top_K (mode={mode})\",\n",
    "    labels={\"mean_total_s\": \"mean total seconds\"},\n",
    ")\n",
    "fig_lat.show()\n",
    "\n",
    "fig_hit = px.line(\n",
    "    agg,\n",
    "    x=\"top_k\",\n",
    "    y=\"hit_rate\",\n",
    "    markers=True,\n",
    "    title=\"Retrieval hit-rate vs Top_K (did we retrieve the gold doc?)\",\n",
    "    labels={\"hit_rate\": \"hit rate\"},\n",
    ")\n",
    "fig_hit.update_yaxes(range=[-0.05, 1.05])\n",
    "fig_hit.show()\n",
    "\n",
    "# Stacked stage breakdown vs Top_K\n",
    "stage_long = agg.melt(\n",
    "    id_vars=[\"top_k\"],\n",
    "    value_vars=[\"mean_embed_s\", \"mean_retrieve_s\", \"mean_rerank_s\", \"mean_generate_s\"],\n",
    "    var_name=\"stage\",\n",
    "    value_name=\"seconds\",\n",
    ")\n",
    "stage_long[\"stage\"] = stage_long[\"stage\"].str.replace(\"mean_\", \"\").str.replace(\"_s\", \"\")\n",
    "\n",
    "fig_stage = px.bar(\n",
    "    stage_long,\n",
    "    x=\"top_k\",\n",
    "    y=\"seconds\",\n",
    "    color=\"stage\",\n",
    "    title=\"Mean stage latency breakdown vs Top_K\",\n",
    ")\n",
    "fig_stage.update_layout(barmode=\"stack\")\n",
    "fig_stage.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why rerankers exist (and why they cost latency)\n",
    "\n",
    "Think of retrieval as a fast **semantic filter**. A **reranker** is the slower second pass that answers:\n",
    "\n",
    "> “Given this query, which of these candidates is *most relevant*?”\n",
    "\n",
    "- **Retriever**: fast, approximate, great at *recall* (bringing plausible candidates)\n",
    "- **Reranker**: slower, more precise, improves *precision* (promotes the best evidence)\n",
    "\n",
    "Latency tradeoff (rule of thumb): reranking cost grows with the number of candidates you ask it to score (roughly **O(retriever_top_k)** pair evaluations).\n",
    "\n",
    "You’ll also be able to quantify reranker impact on quality using **RAGAS** later in [`day2_04_evals_grading_ragas.ipynb`](/home/shadeform/workshop-v1/fico/day2_04_evals_grading_ragas.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker before/after: watch candidates move.\n",
    "\n",
    "out_rr = widgets.Output()\n",
    "\n",
    "w_rr_query = widgets.Text(value=\"Kubernetes incident runbook\", description=\"Query:\", layout=widgets.Layout(width=\"900px\"), style=style)\n",
    "w_rr_chunk = widgets.IntSlider(value=900, min=200, max=2000, step=100, description=\"Chunk size (chars)\", style=style)\n",
    "w_rr_overlap = widgets.IntSlider(value=150, min=0, max=500, step=50, description=\"Overlap (chars)\", style=style)\n",
    "\n",
    "w_rr_topk = widgets.IntSlider(value=20, min=5, max=60, step=1, description=\"Retriever Top‑K\", style=style)\n",
    "w_rr_rerank = widgets.IntSlider(value=10, min=0, max=30, step=1, description=\"Rerank Top‑K\", style=style)\n",
    "\n",
    "\n",
    "def _render_rerank(*_):\n",
    "    with out_rr:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        mode = \"local\"  # keep this deterministic + show real rerank scores\n",
    "\n",
    "        rows, index = get_index(mode, chunk_size=int(w_rr_chunk.value), overlap=int(w_rr_overlap.value))\n",
    "\n",
    "        q = str(w_rr_query.value)\n",
    "        q_emb = embed_local([q])[0]\n",
    "        q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-12)\n",
    "\n",
    "        top_k = int(w_rr_topk.value)\n",
    "        rerank_k = int(w_rr_rerank.value)\n",
    "\n",
    "        I, D = index.search(q_norm, k=top_k)\n",
    "        I = [int(i) for i in I]\n",
    "        sims = [float(s) for s in D]\n",
    "\n",
    "        candidates = [rows[i] for i in I]\n",
    "        docs = [c.get(\"text\", \"\") for c in candidates]\n",
    "\n",
    "        rr_scores = [None] * len(candidates)\n",
    "        rr_rank: dict[int, int] = {}\n",
    "\n",
    "        if rerank_k > 0:\n",
    "            rr_order, scores = rerank_local(q, docs)\n",
    "            rr_scores = [float(s) for s in scores]\n",
    "            # Map candidate idx -> rerank rank\n",
    "            rr_rank = {int(i): (r + 1) for r, i in enumerate(rr_order)}\n",
    "\n",
    "        rerank_ranks = [rr_rank.get(i) for i in range(len(candidates))] if rr_rank else [None] * len(candidates)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"retrieve_rank\": list(range(1, len(candidates) + 1)),\n",
    "                \"cosine\": sims,\n",
    "                \"rerank_rank\": rerank_ranks,\n",
    "                \"rerank_score\": rr_scores,\n",
    "                \"doc_id\": [c.get(\"doc_id\") for c in candidates],\n",
    "                \"chunk_id\": [c.get(\"chunk_id\") for c in candidates],\n",
    "                \"title\": [c.get(\"title\") for c in candidates],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Show movement only for the top rerank_k (since that's what would be kept)\n",
    "        df[\"kept_after_rerank\"] = df[\"rerank_rank\"].notna() & (df[\"rerank_rank\"] <= rerank_k)\n",
    "\n",
    "        display(df.head(25))\n",
    "\n",
    "        # Rank-movement slope chart (only meaningful when rerank_k > 0)\n",
    "        if rerank_k > 0:\n",
    "            df_plot = df[df[\"rerank_rank\"].notna()].copy()\n",
    "            if not df_plot.empty:\n",
    "                fig = go.Figure()\n",
    "                for _, r in df_plot.iterrows():\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=[\"retriever\", \"reranker\"],\n",
    "                            y=[float(r[\"retrieve_rank\"]), float(r[\"rerank_rank\"])],\n",
    "                            mode=\"lines+markers\",\n",
    "                            line=dict(width=2),\n",
    "                            marker=dict(size=7),\n",
    "                            opacity=0.55,\n",
    "                            hovertemplate=(\n",
    "                                \"doc_id=%{customdata[0]}<br>chunk_id=%{customdata[1]}<br>retrieve_rank=%{customdata[2]}<br>rerank_rank=%{customdata[3]}<br>cosine=%{customdata[4]:.3f}<br>rerank_score=%{customdata[5]}<extra></extra>\"\n",
    "                            ),\n",
    "                            customdata=[[r[\"doc_id\"], r[\"chunk_id\"], int(r[\"retrieve_rank\"]), int(r[\"rerank_rank\"]), float(r[\"cosine\"]), r[\"rerank_score\"]]],\n",
    "                            showlegend=False,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                fig.update_yaxes(autorange=\"reversed\", title=\"rank (1 is best)\")\n",
    "                fig.update_layout(\n",
    "                    title=\"Rank movement: retriever vs reranker (lines show candidates that got rescored)\",\n",
    "                    height=420,\n",
    "                )\n",
    "                fig.show()\n",
    "\n",
    "        kept = int(df[\"kept_after_rerank\"].sum()) if rerank_k > 0 else len(df)\n",
    "        print(\"\\nTakeaway:\")\n",
    "        if rerank_k == 0:\n",
    "            print(\"- rerank_k=0 → no reranker used (fastest, but relies purely on embedding similarity).\")\n",
    "        else:\n",
    "            print(f\"- retriever_top_k={top_k} candidates scored → rerank keeps top {rerank_k} (kept={kept}).\")\n",
    "            print(\"- rerank often improves precision, but adds latency (more pair scoring).\")\n",
    "            print(\"- You can measure quality impact with RAGAS in day2_04.\")\n",
    "\n",
    "\n",
    "for w in [w_rr_query, w_rr_chunk, w_rr_overlap, w_rr_topk, w_rr_rerank]:\n",
    "    w.observe(_render_rerank, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([w_rr_query, widgets.HBox([w_rr_chunk, w_rr_overlap]), widgets.HBox([w_rr_topk, w_rr_rerank]), out_rr]))\n",
    "\n",
    "_render_rerank()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knob Lab: Chunking × Top_K × Rerank_K tradeoffs\n",
    "\n",
    "This sweep explores the key knobs that affect both latency and quality.\n",
    "\n",
    "We’ll plot:\n",
    "- Scatter: latency vs a fast quality proxy (keyword hits)\n",
    "- Heatmap: latency by (Top_K, Rerank_K)\n",
    "- Heatmap: quality proxy by (Top_K, Rerank_K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid sweep across chunk_size/top_k/rerank_k with fast proxy metrics\n",
    "\n",
    "CHUNK_SIZES = [400, 900, 1400]\n",
    "TOPK_GRID = [5, 10, 20]\n",
    "RERANK_GRID = [0, 3, 5]\n",
    "SWEEP_MAX_NEW_TOKENS = 120\n",
    "\n",
    "if \"gold_eval\" not in globals() or not gold_eval:\n",
    "    raise RuntimeError(\"gold_eval missing/empty\")\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "missing = [name for name in [\"get_index\", \"run_rag\"] if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing functions: \" + \", \".join(missing) + \"\\nRun the Phase 1 setup cell above first.\")\n",
    "\n",
    "# Use the first gold item as a representative query (fast), with its expected keywords.\n",
    "ex = gold_eval[0]\n",
    "SWEEP_QUERY = ex[\"question\"]\n",
    "EXPECTED = [k.lower() for k in (ex.get(\"expected_keywords\") or [])]\n",
    "\n",
    "records = []\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    rows, index = get_index(mode, chunk_size=chunk_size, overlap=150)\n",
    "    for top_k in TOPK_GRID:\n",
    "        for rerank_k in RERANK_GRID:\n",
    "            cfg = RAGConfig(\n",
    "                mode=mode,\n",
    "                top_k=int(top_k),\n",
    "                rerank_top_k=int(rerank_k),\n",
    "                max_new_tokens=int(SWEEP_MAX_NEW_TOKENS),\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            ans, ctx, timings = run_rag(SWEEP_QUERY, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "            ans_clean = clean_answer(ans)\n",
    "\n",
    "            total = float(sum(timings.values()))\n",
    "\n",
    "            # Proxy: keyword hit rate against expected keywords (if any)\n",
    "            a_low = ans_clean.lower()\n",
    "            hits = sum(1 for k in EXPECTED if k and k in a_low)\n",
    "            hit_rate = (hits / max(1, len(EXPECTED))) if EXPECTED else 0.0\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"chunk_size\": int(chunk_size),\n",
    "                    \"top_k\": int(top_k),\n",
    "                    \"rerank_k\": int(rerank_k),\n",
    "                    \"total_s\": total,\n",
    "                    \"answer_len\": int(len(ans_clean)),\n",
    "                    \"keyword_hit_rate\": float(hit_rate),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_grid = pd.DataFrame(records)\n",
    "display(df_grid.sort_values(\"total_s\").head(12))\n",
    "\n",
    "# Scatter: latency vs proxy quality\n",
    "fig_scatter = px.scatter(\n",
    "    df_grid,\n",
    "    x=\"total_s\",\n",
    "    y=\"keyword_hit_rate\",\n",
    "    color=\"chunk_size\",\n",
    "    symbol=\"rerank_k\",\n",
    "    hover_data=[\"top_k\"],\n",
    "    title=f\"Knob sweep: latency vs keyword-hit proxy (mode={mode})\",\n",
    "    labels={\"total_s\": \"total seconds\", \"keyword_hit_rate\": \"keyword hit rate\"},\n",
    ")\n",
    "fig_scatter.show()\n",
    "\n",
    "# Heatmaps at chunk_size=900 for readability\n",
    "fixed_chunk = 900\n",
    "sub = df_grid[df_grid[\"chunk_size\"] == fixed_chunk]\n",
    "\n",
    "lat_pivot = sub.pivot_table(index=\"rerank_k\", columns=\"top_k\", values=\"total_s\", aggfunc=\"mean\")\n",
    "qual_pivot = sub.pivot_table(index=\"rerank_k\", columns=\"top_k\", values=\"keyword_hit_rate\", aggfunc=\"mean\")\n",
    "\n",
    "fig_lat_hm = px.imshow(\n",
    "    lat_pivot,\n",
    "    title=f\"Heatmap: mean total latency (s) at chunk_size={fixed_chunk}\",\n",
    "    labels={\"x\": \"top_k\", \"y\": \"rerank_k\", \"color\": \"seconds\"},\n",
    ")\n",
    "fig_lat_hm.show()\n",
    "\n",
    "fig_qual_hm = px.imshow(\n",
    "    qual_pivot,\n",
    "    title=f\"Heatmap: keyword-hit proxy at chunk_size={fixed_chunk}\",\n",
    "    labels={\"x\": \"top_k\", \"y\": \"rerank_k\", \"color\": \"hit rate\"},\n",
    ")\n",
    "fig_qual_hm.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: meet the 1.5s SLA without killing accuracy\n",
    "\n",
    "Goal: find a configuration that:\n",
    "- **Passes the 1.5s SLA** (latency) across a small query set\n",
    "- Keeps **accuracy** high (local judge score vs gold references)\n",
    "\n",
    "Rules (recommended knobs):\n",
    "- `chunk_size`, `top_k`, `rerank_top_k`, `max_new_tokens`, `temperature`\n",
    "\n",
    "Tip: optimize the **largest bar** in the waterfall first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA + accuracy autograder (interactive)\n",
    "\n",
    "SLA_S = 1.5\n",
    "\n",
    "if \"gold_eval\" not in globals() or len(gold_eval) < 3:\n",
    "    raise RuntimeError(\"Need gold_eval with at least 3 items. Run the gold eval cells above first.\")\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "# Use 3 gold items as the challenge set (keeps runtime reasonable).\n",
    "challenge_set = gold_eval[:3]\n",
    "\n",
    "# Leaderboard\n",
    "LEADERBOARD = []\n",
    "\n",
    "# Controls\n",
    "style = {\"description_width\": \"initial\"}\n",
    "\n",
    "w_c_chunk = widgets.IntSlider(value=900, min=300, max=1800, step=100, description=\"Chunk size (chars)\", style=style)\n",
    "w_c_topk = widgets.IntSlider(value=20, min=1, max=40, step=1, description=\"Top_K\", style=style)\n",
    "w_c_rerank = widgets.IntSlider(value=10, min=0, max=20, step=1, description=\"Rerank Top_K\", style=style)\n",
    "w_c_tokens = widgets.IntSlider(value=220, min=32, max=512, step=16, description=\"Max new tokens\", style=style)\n",
    "w_c_temp = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.05, description=\"Temperature\", style=style)\n",
    "\n",
    "btn_grade = widgets.Button(description=\"Grade config (SLA + accuracy)\")\n",
    "out_grade = widgets.Output()\n",
    "\n",
    "\n",
    "def grade_config(*, chunk_size: int, top_k: int, rerank_k: int, max_new_tokens: int, temperature: float):\n",
    "    rows, index = get_index(mode, chunk_size=chunk_size, overlap=150)\n",
    "\n",
    "    per_q = []\n",
    "    for ex in challenge_set:\n",
    "        cfg = RAGConfig(\n",
    "            mode=mode,\n",
    "            top_k=int(top_k),\n",
    "            rerank_top_k=int(rerank_k),\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            temperature=float(temperature),\n",
    "        )\n",
    "        ans, ctx, timings = run_rag(ex[\"question\"], mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "        ans_clean = clean_answer(ans)\n",
    "        judge = judge_answer(ex[\"question\"], ex[\"reference_answer\"], ans_clean)\n",
    "        total = float(sum(timings.values()))\n",
    "\n",
    "        per_q.append(\n",
    "            {\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"total_s\": total,\n",
    "                \"sla_pass\": total <= SLA_S,\n",
    "                \"score\": int(judge[\"score\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(per_q)\n",
    "    summary = {\n",
    "        \"mode\": mode,\n",
    "        \"chunk_size\": int(chunk_size),\n",
    "        \"top_k\": int(top_k),\n",
    "        \"rerank_k\": int(rerank_k),\n",
    "        \"max_new_tokens\": int(max_new_tokens),\n",
    "        \"temperature\": float(temperature),\n",
    "        \"sla_pass_all\": bool(df[\"sla_pass\"].all()),\n",
    "        \"mean_total_s\": float(df[\"total_s\"].mean()),\n",
    "        \"mean_score\": float(df[\"score\"].mean()),\n",
    "    }\n",
    "\n",
    "    return df, summary\n",
    "\n",
    "\n",
    "def on_grade_click(_):\n",
    "    with out_grade:\n",
    "        clear_output()\n",
    "        df, summary = grade_config(\n",
    "            chunk_size=int(w_c_chunk.value),\n",
    "            top_k=int(w_c_topk.value),\n",
    "            rerank_k=int(w_c_rerank.value),\n",
    "            max_new_tokens=int(w_c_tokens.value),\n",
    "            temperature=float(w_c_temp.value),\n",
    "        )\n",
    "\n",
    "        LEADERBOARD.append(summary)\n",
    "\n",
    "        print(\"=== Summary ===\")\n",
    "        print(summary)\n",
    "        display(df[[\"question\", \"total_s\", \"sla_pass\", \"score\"]])\n",
    "\n",
    "        # Figure: per-query latency with SLA line\n",
    "        fig = px.bar(\n",
    "            df.assign(q_id=[f\"q{i+1}\" for i in range(len(df))]),\n",
    "            x=\"q_id\",\n",
    "            y=\"total_s\",\n",
    "            color=\"sla_pass\",\n",
    "            title=f\"Challenge: per-query latency (SLA={SLA_S}s)\",\n",
    "            labels={\"total_s\": \"seconds\"},\n",
    "        )\n",
    "        fig.add_hline(y=SLA_S, line_dash=\"dot\", annotation_text=\"SLA\")\n",
    "        fig.show()\n",
    "\n",
    "        # Leaderboard (SLA-first, accuracy-second)\n",
    "        lb = pd.DataFrame(LEADERBOARD)\n",
    "        lb = lb.sort_values([\"sla_pass_all\", \"mean_score\", \"mean_total_s\"], ascending=[False, False, True])\n",
    "        display(lb[[\"sla_pass_all\", \"mean_score\", \"mean_total_s\", \"chunk_size\", \"top_k\", \"rerank_k\", \"max_new_tokens\", \"temperature\"]].head(10))\n",
    "\n",
    "\n",
    "btn_grade.on_click(on_grade_click)\n",
    "\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([w_c_chunk, w_c_topk]),\n",
    "            widgets.HBox([w_c_rerank, w_c_tokens, w_c_temp]),\n",
    "            btn_grade,\n",
    "            out_grade,\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache demo: cold start vs warm start (what’s in/out of SLA)\n",
    "\n",
    "The SLA measures **per-request** time (embed/retrieve/rerank/generate).\n",
    "\n",
    "Index building (chunk embedding for the corpus) is **cached** and usually **excluded** from the SLA.\n",
    "\n",
    "This demo shows how much caching matters for workshop smoothness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold vs warm index build timing\n",
    "\n",
    "import time as _time\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "\n",
    "if \"_INDEX_CACHE\" not in globals():\n",
    "    raise RuntimeError(\"_INDEX_CACHE not found. Run the Phase 1 setup cell first.\")\n",
    "\n",
    "# Cold\n",
    "_INDEX_CACHE.clear()\n",
    "\n",
    "t0 = _time.perf_counter()\n",
    "rows_cold, idx_cold = get_index(mode, chunk_size=900, overlap=150)\n",
    "cold_s = _time.perf_counter() - t0\n",
    "\n",
    "# Warm\n",
    "\n",
    "t1 = _time.perf_counter()\n",
    "rows_warm, idx_warm = get_index(mode, chunk_size=900, overlap=150)\n",
    "warm_s = _time.perf_counter() - t1\n",
    "\n",
    "print(f\"Index build cold: {cold_s:.3f}s\")\n",
    "print(f\"Index build warm: {warm_s:.3f}s\")\n",
    "\n",
    "fig = px.bar(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\"state\": \"cold\", \"seconds\": cold_s},\n",
    "            {\"state\": \"warm\", \"seconds\": warm_s},\n",
    "        ]\n",
    "    ),\n",
    "    x=\"state\",\n",
    "    y=\"seconds\",\n",
    "    title=f\"Index build time (mode={mode})\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness Lab: NIM down, timeouts, and context overflow\n",
    "\n",
    "Real systems fail. This section shows how to:\n",
    "- Detect **NIM down** and fall back\n",
    "- Handle **timeouts** with simple retries\n",
    "- See how **max_context_chars** impacts latency and accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) NIM down detection + fallback guidance\n",
    "\n",
    "print(\"NIM_OK:\", bool(globals().get(\"NIM_OK\", False)))\n",
    "print(\"DEFAULT_MODE:\", globals().get(\"DEFAULT_MODE\", \"local\"))\n",
    "\n",
    "if not bool(globals().get(\"NIM_OK\", False)):\n",
    "    print(\"\\nNIM is not reachable. For workshops you can:\")\n",
    "    print(\"- run local mode (DEFAULT_MODE=local), or\")\n",
    "    print(\"- start NIMs: cd fico && export NGC_API_KEY=... && ./scripts/start_nims.sh\")\n",
    "\n",
    "\n",
    "# 2) Timeout + retry demo (only meaningful when NIM is up)\n",
    "\n",
    "def nim_embed_with_retry(text: str, *, retries: int = 3, base_sleep_s: float = 0.3):\n",
    "    cfg = NIMConfig(timeout_s=0.5)  # intentionally low to demonstrate timeout behavior\n",
    "    client = NIMClient(cfg)\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            embs, dt = client.embed([text])\n",
    "            return {\"ok\": True, \"attempt\": attempt, \"latency_s\": float(dt)}\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                return {\"ok\": False, \"attempt\": attempt, \"error\": f\"{type(e).__name__}: {str(e)[:180]}\"}\n",
    "            _sleep = base_sleep_s * (2 ** (attempt - 1))\n",
    "            time.sleep(_sleep)\n",
    "\n",
    "\n",
    "timeout_demo = nim_embed_with_retry(\"ping\", retries=3)\n",
    "print(\"\\nTimeout/retry demo:\", timeout_demo)\n",
    "\n",
    "\n",
    "# 3) Context overflow: max_context_chars vs latency + judge score\n",
    "\n",
    "if \"gold_eval\" not in globals() or not gold_eval:\n",
    "    raise RuntimeError(\"gold_eval missing/empty\")\n",
    "\n",
    "mode = DEFAULT_MODE if \"DEFAULT_MODE\" in globals() else \"local\"\n",
    "rows, index = get_index(mode, chunk_size=900, overlap=150)\n",
    "\n",
    "ex = gold_eval[0]\n",
    "ctx_limits = [800, 1500, 3000, 6000, 10000]\n",
    "\n",
    "rows_out = []\n",
    "for limit in ctx_limits:\n",
    "    cfg = RAGConfig(\n",
    "        mode=mode,\n",
    "        top_k=20,\n",
    "        rerank_top_k=5,\n",
    "        max_context_chars=int(limit),\n",
    "        max_new_tokens=140,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    ans, ctx, timings = run_rag(ex[\"question\"], mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "    ans_clean = clean_answer(ans)\n",
    "    judge = judge_answer(ex[\"question\"], ex[\"reference_answer\"], ans_clean)\n",
    "\n",
    "    rows_out.append(\n",
    "        {\n",
    "            \"max_context_chars\": int(limit),\n",
    "            \"total_s\": float(sum(timings.values())),\n",
    "            \"score\": int(judge[\"score\"]),\n",
    "            \"context_chunks\": int(len(ctx)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_ctx = pd.DataFrame(rows_out)\n",
    "display(df_ctx)\n",
    "\n",
    "fig_ctx = px.line(\n",
    "    df_ctx,\n",
    "    x=\"max_context_chars\",\n",
    "    y=[\"total_s\", \"score\"],\n",
    "    markers=True,\n",
    "    title=f\"Context budget tradeoff (mode={mode})\",\n",
    ")\n",
    "fig_ctx.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling intuition: more ingested chunks → more retrieval work\n",
    "\n",
    "Retrieval latency depends on how many vectors you’ve ingested (number of chunks).\n",
    "\n",
    "In this workshop we use an **exact** cosine search (`VectorIndex` with FAISS `IndexFlatIP` if available; otherwise numpy). Exact search does *more work* as the corpus grows.\n",
    "\n",
    "(If you’re thinking “can we go faster?” — yes: ANN indexes and/or GPU acceleration. We’ll mention both after the plot.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure retrieval latency vs number of indexed vectors.\n",
    "# We simulate larger corpora by tiling the existing embedding matrix.\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def _measure_retrieve_ms(E_norm: np.ndarray, q_norms: np.ndarray, *, k: int, trials: int, use_faiss: bool = True) -> float:\n",
    "    idx = VectorIndex(E_norm, use_faiss=use_faiss)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(int(trials)):\n",
    "        q = q_norms[random.randrange(q_norms.shape[0])]\n",
    "        _ = idx.search(q, k=int(k))\n",
    "    dt = time.perf_counter() - t0\n",
    "    return (dt / float(trials)) * 1000.0\n",
    "\n",
    "\n",
    "# Base vectors from the current workshop index\n",
    "rows_base, index_base = get_index(\"local\", chunk_size=900, overlap=150)\n",
    "E_base = index_base.E_norm.astype(np.float32)\n",
    "\n",
    "# A few query vectors\n",
    "q_texts = [\n",
    "    \"Kubernetes incident runbook\",\n",
    "    \"APR calculation and interest rate adjustments\",\n",
    "    \"GPU memory failures and inference performance\",\n",
    "]\n",
    "qE = embed_local(q_texts)\n",
    "qE = normalize_rows(qE)\n",
    "\n",
    "# Scale up N by tiling (keeps semantics realistic-ish without needing a bigger corpus file)\n",
    "N_base = int(E_base.shape[0])\n",
    "D = int(E_base.shape[1])\n",
    "print(f\"Base index vectors: N={N_base} dim={D}\")\n",
    "\n",
    "if N_base <= 0:\n",
    "    raise RuntimeError(\"Index has zero vectors; cannot run scaling demo.\")\n",
    "\n",
    "sizes = []\n",
    "if N_base < 3000:\n",
    "    sizes = [N_base, 2 * N_base, 4 * N_base, 8 * N_base]\n",
    "else:\n",
    "    sizes = [int(x) for x in [1000, 3000, 8000, 20000, 50000] if x <= N_base]\n",
    "\n",
    "# Cap for notebook friendliness\n",
    "sizes = [int(s) for s in sizes if int(s) >= 50]\n",
    "if not sizes:\n",
    "    sizes = [int(N_base)]\n",
    "\n",
    "max_n = min(max(sizes), 120000)\n",
    "\n",
    "records = []\n",
    "for n in sizes:\n",
    "    n = int(min(n, max_n))\n",
    "    reps = (n + N_base - 1) // N_base\n",
    "    E_big = np.vstack([E_base] * reps)[:n]\n",
    "\n",
    "    ms = _measure_retrieve_ms(E_big, qE, k=10, trials=80, use_faiss=True)\n",
    "    records.append({\"n_vectors\": int(n), \"ms_per_query\": float(ms)})\n",
    "\n",
    "\n",
    "df_scale = pd.DataFrame(records).sort_values(\"n_vectors\")\n",
    "display(df_scale)\n",
    "\n",
    "fig = px.line(df_scale, x=\"n_vectors\", y=\"ms_per_query\", markers=True, title=\"Retrieve latency vs number of indexed vectors (exact search)\")\n",
    "fig.update_layout(xaxis_title=\"# vectors (chunks)\", yaxis_title=\"ms / query\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- This is exact cosine search: more vectors → more work.\")\n",
    "print(\"- For large corpora, teams often use ANN indexes (FAISS IVF/HNSW) to trade a little recall for big speed.\")\n",
    "print(\"- FAISS also has GPU indexes (search can be faster if vectors + index fit on GPU memory).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much space does “1MB of text” take when embedded?\n",
    "\n",
    "Back-of-the-envelope for embedding storage (vectors only):\n",
    "\n",
    "- Each chunk → one embedding vector\n",
    "- Bytes ≈ `num_chunks * embedding_dim * bytes_per_number`\n",
    "  - float32 → 4 bytes\n",
    "  - float16 → 2 bytes\n",
    "\n",
    "Important: real vector stores also have **metadata** + **index overhead** (and if you store raw chunk text too, that can dominate). But this gets you into the right order of magnitude.\n",
    "\n",
    "Small joke: storage math is the only place where “it’s just vectors” stops being a comforting sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive vector-memory estimator\n",
    "\n",
    "out_mem = widgets.Output()\n",
    "\n",
    "w_mem_mb = widgets.IntSlider(value=1, min=1, max=200, step=1, description=\"Text size (MB)\", style=style)\n",
    "w_mem_chunk = widgets.IntSlider(value=900, min=100, max=4000, step=100, description=\"Chunk size (chars)\", style=style)\n",
    "w_mem_overlap = widgets.IntSlider(value=150, min=0, max=1000, step=50, description=\"Overlap (chars)\", style=style)\n",
    "\n",
    "w_mem_dim = widgets.IntSlider(value=384, min=64, max=4096, step=64, description=\"Embedding dim\", style=style)\n",
    "w_mem_dtype = widgets.Dropdown(options=[(\"float32 (4 bytes)\", 4), (\"float16 (2 bytes)\", 2)], value=4, description=\"dtype\", style=style)\n",
    "\n",
    "\n",
    "def _estimate_chunks(n_chars: int, chunk_size: int, overlap: int) -> int:\n",
    "    chunk_size = max(1, int(chunk_size))\n",
    "    overlap = max(0, min(int(overlap), chunk_size - 1))\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    if n_chars <= chunk_size:\n",
    "        return 1\n",
    "    # ceil((n_chars - chunk_size)/step) + 1\n",
    "    return int(((n_chars - chunk_size + step - 1) // step) + 1)\n",
    "\n",
    "\n",
    "def _render_mem(*_):\n",
    "    with out_mem:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        n_chars = int(w_mem_mb.value) * 1_000_000  # rough: 1MB ≈ 1,000,000 chars\n",
    "        chunk_size = int(w_mem_chunk.value)\n",
    "        overlap = int(w_mem_overlap.value)\n",
    "        dim = int(w_mem_dim.value)\n",
    "        bpn = int(w_mem_dtype.value)\n",
    "\n",
    "        n_chunks = _estimate_chunks(n_chars, chunk_size=chunk_size, overlap=overlap)\n",
    "        bytes_vectors = n_chunks * dim * bpn\n",
    "        mb_vectors = bytes_vectors / (1024 * 1024)\n",
    "\n",
    "        per_vec_bytes = dim * bpn\n",
    "\n",
    "        print(\"=== Estimate (vectors only) ===\")\n",
    "        print(f\"text_chars≈{n_chars:,}\")\n",
    "        print(f\"chunk_size={chunk_size} overlap={overlap} → chunks≈{n_chunks:,}\")\n",
    "        print(f\"embedding_dim={dim} dtype_bytes={bpn} → per_vector≈{per_vec_bytes:,} bytes\")\n",
    "        print(f\"vector_bytes≈{bytes_vectors:,} bytes ≈ {mb_vectors:,.2f} MiB\")\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                {\"item\": \"text size (MB)\", \"value\": float(w_mem_mb.value)},\n",
    "                {\"item\": \"#chunks (≈vectors)\", \"value\": float(n_chunks)},\n",
    "                {\"item\": \"vector storage (MiB)\", \"value\": float(mb_vectors)},\n",
    "            ]\n",
    "        )\n",
    "        fig = px.bar(df, x=\"item\", y=\"value\", title=\"Back-of-the-envelope (not counting metadata/index overhead)\")\n",
    "        fig.show()\n",
    "\n",
    "        print(\"\\nNotes:\")\n",
    "        print(\"- This counts ONLY embedding vectors (no metadata, no index graph/inverted lists, no raw text).\")\n",
    "        print(\"- Real systems often store metadata + raw chunk text; those can dominate overall storage.\")\n",
    "        print(\"- If you quantize embeddings (e.g., int8) you can shrink memory further, with some accuracy tradeoff.\")\n",
    "\n",
    "\n",
    "for w in [w_mem_mb, w_mem_chunk, w_mem_overlap, w_mem_dim, w_mem_dtype]:\n",
    "    w.observe(_render_mem, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([w_mem_mb, w_mem_dtype]), widgets.HBox([w_mem_chunk, w_mem_overlap]), w_mem_dim, out_mem]))\n",
    "\n",
    "_render_mem()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways + cheat sheet\n",
    "\n",
    "### How to hit the SLA (usually)\n",
    "- **If `generate` dominates**: reduce `max_new_tokens`, shorten context (`max_context_chars`), switch to faster model.\n",
    "- **If `rerank` dominates**: reduce `rerank_top_k` or use a faster reranker.\n",
    "- **If `embed` dominates**: batch where possible; prefer NIM embeddings if available.\n",
    "- **If retrieval is slow**: lower `top_k`, shrink chunks, enable FAISS.\n",
    "\n",
    "### Accuracy vs latency heuristics\n",
    "- Smaller `top_k` is faster but risks **missing the right doc** (hit-rate drops).\n",
    "- Bigger `top_k` increases recall but can:\n",
    "  - increase rerank cost\n",
    "  - worsen generation quality via noisy context\n",
    "\n",
    "### Operational checklist (Brev)\n",
    "- If imports fail: `cd fico && ./scripts/setup_workshop.sh` then restart kernel and pick **Python (fico)**.\n",
    "- If NIM mode fails: `cd fico && export NGC_API_KEY=... && ./scripts/start_nims.sh`\n",
    "- Expect first index build to be slow; repeats should be fast (cache).\n",
    "\n",
    "### DOCX ingestion (quick)\n",
    "If you have a `.docx`, the simplest path is:\n",
    "\n",
    "`docx -> text -> chunk_text() -> embed -> VectorIndex`\n",
    "\n",
    "```python\n",
    "# pip install python-docx\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "def docx_to_text(path: str) -> str:\n",
    "    doc = Document(path)\n",
    "    parts = [(p.text or \"\").strip() for p in doc.paragraphs]\n",
    "    parts = [p for p in parts if p]\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "text = docx_to_text(\"/path/to/file.docx\")\n",
    "parts = chunk_text(text, chunk_size=900, overlap=150)\n",
    "chunk_rows = [\n",
    "    {\"doc_id\": \"docx-001\", \"chunk_id\": f\"docx-001::c{i:03d}\", \"title\": \"My DOCX\", \"text\": t}\n",
    "    for i, t in enumerate(parts)\n",
    "]\n",
    "index, _ = build_index_for_mode(\"local\", chunk_rows, use_faiss=True)\n",
    "```\n",
    "\n",
    "### What to run if you have 5 minutes left\n",
    "- Smoke test\n",
    "- Top_K sweep (accuracy + latency plots)\n",
    "- One SLA challenge attempt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Real Automated Eval (RAGAS)\n",
    "\n",
    "This phase builds a tiny evaluation set from the corpus and runs **real RAGAS** metrics.\n",
    "\n",
    "- It runs the same queries through **Local** and **NIM** modes.\n",
    "- It uses a **local Llama-3.1-8B-Instruct** judge (as requested) to compute metrics.\n",
    "\n",
    "Note: RAGAS will be slower than Phase 1 because it calls an LLM as a judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local generator/judge (cached) ---\n",
    "# Reuse the cached local generator from Phase 1 if it exists; otherwise define it.\n",
    "\n",
    "if \"ensure_local_generator\" not in globals():\n",
    "    _LOCAL_GEN = {\"tok\": None, \"model\": None}\n",
    "\n",
    "    def ensure_local_generator():\n",
    "        if _LOCAL_GEN[\"tok\"] is not None and _LOCAL_GEN[\"model\"] is not None:\n",
    "            return _LOCAL_GEN[\"tok\"], _LOCAL_GEN[\"model\"]\n",
    "\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "        tok = AutoTokenizer.from_pretrained(LOCAL_GEN_MODEL, use_fast=True)\n",
    "        tok.padding_side = \"left\"\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LOCAL_GEN_MODEL,\n",
    "            device_map=\"auto\",\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        _LOCAL_GEN[\"tok\"] = tok\n",
    "        _LOCAL_GEN[\"model\"] = model\n",
    "        return tok, model\n",
    "\n",
    "\n",
    "def gen_local_cached(prompt: str, max_new_tokens: int = 180, temperature: float = 0.2):\n",
    "    import torch\n",
    "\n",
    "    tok, model = ensure_local_generator()\n",
    "\n",
    "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=float(temperature),\n",
    "            top_p=0.95,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
    "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "# --- RAGAS eval ---\n",
    "# We create a small synthetic QA set grounded in context, then evaluate.\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def make_eval_questions(n: int = 6):\n",
    "    # Sample docs and generate grounded questions using the local model.\n",
    "    # We generate questions from redacted bodies to keep the lab safe.\n",
    "    sample = df_docs.sample(n=min(n, len(df_docs)), random_state=42)\n",
    "    rows = []\n",
    "    for _, r in sample.iterrows():\n",
    "        ctx = str(r.get(\"body_redacted\") or r.get(\"body\") or \"\")\n",
    "        ctx = ctx[:2000]\n",
    "        prompt = (\n",
    "            \"Create one question that can be answered ONLY from the provided context.\\n\"\n",
    "            \"Return ONLY the question.\\n\\n\"\n",
    "            f\"CONTEXT:\\n{ctx}\\n\\nQUESTION:\"\n",
    "        )\n",
    "        q = gen_local_cached(prompt, max_new_tokens=64, temperature=0.2)\n",
    "        q = (q.splitlines()[0] if q else \"\").strip()\n",
    "        if not q.endswith(\"?\"):\n",
    "            q = q + \"?\"\n",
    "        rows.append({\"question\": q, \"doc_id\": str(r.get(\"doc_id\"))})\n",
    "    return rows\n",
    "\n",
    "\n",
    "def answer_with_mode(question: str, mode: str, chunk_size: int, overlap: int, top_k: int, rerank_k: int):\n",
    "    rows, index = get_index(mode, chunk_size, overlap)\n",
    "    cfg = RAGConfig(mode=mode, top_k=int(top_k), rerank_top_k=int(rerank_k))\n",
    "    answer, ctx_rows, timings = run_rag(question, mode=mode, chunk_rows=rows, index=index, cfg=cfg)\n",
    "\n",
    "    contexts = [c[\"text\"] for c in ctx_rows]\n",
    "    return answer, contexts, timings\n",
    "\n",
    "\n",
    "def run_ragas_eval(n_questions: int = 6, chunk_size: int = 900, overlap: int = 150, top_k: int = 10, rerank_k: int = 5):\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "    eval_qs = make_eval_questions(n_questions)\n",
    "\n",
    "    # Build dataset entries for both modes\n",
    "    # ragas>=0.4 expects: user_input, response, retrieved_contexts\n",
    "    records = []\n",
    "    lat_records = []\n",
    "    for q in eval_qs:\n",
    "        for mode in [\"local\", \"nim\"]:\n",
    "            ans, ctxs, timings = answer_with_mode(q[\"question\"], mode, chunk_size, overlap, top_k, rerank_k)\n",
    "            records.append(\n",
    "                {\n",
    "                    \"user_input\": q[\"question\"],\n",
    "                    \"response\": ans,\n",
    "                    \"retrieved_contexts\": ctxs,\n",
    "                    \"mode\": mode,\n",
    "                }\n",
    "            )\n",
    "            lat_records.append({\"mode\": mode, **timings})\n",
    "\n",
    "    ds = Dataset.from_list(records)\n",
    "\n",
    "    # RAGAS needs an LLM judge. Use local Llama via a simple langchain wrapper.\n",
    "    from transformers import pipeline\n",
    "    from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "    tok, model = ensure_local_generator()\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    # Evaluate\n",
    "    report = evaluate(ds, metrics=[faithfulness, answer_relevancy], llm=llm)\n",
    "    df_report = report.to_pandas()\n",
    "\n",
    "    # Aggregate by mode\n",
    "    print(\"\\n=== RAGAS report (by mode) ===\")\n",
    "    print(df_report.groupby(\"mode\")[[\"faithfulness\", \"answer_relevancy\"]].mean())\n",
    "\n",
    "    # Latency summary\n",
    "    df_lat = pd.DataFrame(lat_records).fillna(0.0)\n",
    "    print(\"\\n=== Latency summary (per-stage means) ===\")\n",
    "    cols = [c for c in df_lat.columns if c != \"mode\"]\n",
    "    print(df_lat.groupby(\"mode\")[cols].mean())\n",
    "\n",
    "    return df_report, df_lat\n",
    "\n",
    "\n",
    "btn_eval = widgets.Button(description=\"Run RAGAS Eval (Local vs NIM)\")\n",
    "out_eval = widgets.Output()\n",
    "\n",
    "\n",
    "def on_eval_click(b):\n",
    "    with out_eval:\n",
    "        clear_output()\n",
    "        # Keep small by default for workshops\n",
    "        run_ragas_eval(n_questions=6, chunk_size=int(w_chunk.value), overlap=int(w_overlap.value), top_k=int(w_k.value), rerank_k=int(w_rerank_k.value))\n",
    "\n",
    "\n",
    "btn_eval.on_click(on_eval_click)\n",
    "display(btn_eval, out_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NO-WIDGET FALLBACK ----\n",
    "# If ipywidgets don't render/click in your environment, run these direct calls.\n",
    "\n",
    "print(\"\\n=== No-widget fallback: Phase 1 ===\")\n",
    "\n",
    "# Build indexes once\n",
    "rows_local, index_local = get_index(\"local\", chunk_size=900, overlap=150)\n",
    "rows_nim, index_nim = get_index(\"nim\", chunk_size=900, overlap=150)\n",
    "\n",
    "cfg = RAGConfig(mode=\"local\", top_k=10, rerank_top_k=5)\n",
    "\n",
    "q = \"GPU latency impacts APR\"\n",
    "\n",
    "ans_l, ctx_l, t_l = run_rag(q, mode=\"local\", chunk_rows=rows_local, index=index_local, cfg=cfg)\n",
    "plot_measured_waterfall(t_l, title=\"RAG Latency (local)\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "ans_n, ctx_n, t_n = run_rag(q, mode=\"nim\", chunk_rows=rows_nim, index=index_nim, cfg=RAGConfig(mode=\"nim\", top_k=10, rerank_top_k=5))\n",
    "plot_measured_waterfall(t_n, title=\"RAG Latency (nim)\")\n",
    "\n",
    "print(\"\\n=== No-widget fallback: Phase 2 (RAGAS) ===\")\n",
    "# Small eval by default\n",
    "run_ragas_eval(n_questions=4, chunk_size=900, overlap=150, top_k=10, rerank_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
