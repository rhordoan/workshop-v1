{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module E: Quantization Lab - Memory, Speed, and Quality Tradeoffs\n",
        "\n",
        "**Goal:** Understand how quantization affects model size, inference speed, and output quality.\n",
        "\n",
        "**Persona:** ML Engineer optimizing for production deployment\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "In this hands-on lab, you will:\n",
        "\n",
        "1. **Learn the fundamentals** of model quantization\n",
        "2. **Load the same model** at different precision levels (FP16, INT8, INT4)\n",
        "3. **Benchmark performance** - memory, latency, and perplexity\n",
        "4. **Compare quality** in an interactive side-by-side arena\n",
        "\n",
        "## Why Quantization Matters\n",
        "\n",
        "| Scenario | Challenge | Quantization Helps By |\n",
        "|----------|-----------|----------------------|\n",
        "| Edge deployment | Limited GPU memory | 4x smaller model fits |\n",
        "| Cost optimization | Expensive GPU hours | Faster inference = lower cost |\n",
        "| Latency requirements | Real-time responses | Smaller = faster |\n",
        "| Multi-model serving | Many models, one GPU | Each model uses less VRAM |\n",
        "\n",
        "## Model: Qwen2.5-1.5B-Instruct\n",
        "\n",
        "We'll use **Qwen2.5-1.5B-Instruct** - a 1.5 billion parameter instruction-tuned model that:\n",
        "- Is small enough to experiment with quickly\n",
        "- Is large enough to show meaningful differences\n",
        "- Has strong performance for its size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check bitsandbytes installation\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check if bitsandbytes is installed\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(f\"\u2705 bitsandbytes {bnb.__version__} is installed\")\n",
        "    print(f\"   Location: {bnb.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u274c bitsandbytes NOT found: {e}\")\n",
        "    print(\"\\n\ud83d\udd27 Installing bitsandbytes...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n",
        "    print(\"\u2705 Installation complete. Please restart the kernel!\")\n",
        "\n",
        "# Check CUDA availability\n",
        "import torch\n",
        "print(f\"\\n\ud83d\udda5\ufe0f CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 FIXED: bitsandbytes Installation Complete!\n",
        "\n",
        "**Good news**: All quantization libraries are now installed in your `.venv`:\n",
        "- \u2705 bitsandbytes 0.49.0\n",
        "- \u2705 auto-gptq 0.7.1  \n",
        "- \u2705 optimum 2.0.0\n",
        "- \u2705 PyTorch 2.9.1+cu128 with CUDA support\n",
        "\n",
        "### To use them:\n",
        "\n",
        "**1. Restart your Jupyter kernel**: \n",
        "   - Go to **Kernel \u2192 Restart Kernel** (or press `00` in Jupyter)\n",
        "\n",
        "**2. Run the debug cell above** to verify the installation\n",
        "\n",
        "**3. Continue with the lab!**\n",
        "\n",
        "---\n",
        "\n",
        "### Still seeing import errors after restart?\n",
        "\n",
        "**Check kernel matches venv**: In Jupyter, go to **Kernel \u2192 Change Kernel** and ensure it says `Python (fico)` or uses `.venv/bin/python`\n",
        "\n",
        "**Manual install if needed**:\n",
        "```bash\n",
        "cd /home/shadeform/workshop-v1/fico\n",
        "./scripts/install_bitsandbytes.sh\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Quantization Fundamentals\n",
        "\n",
        "### What is Quantization?\n",
        "\n",
        "Quantization reduces the **precision** of model weights from higher bit-widths to lower ones.\n",
        "\n",
        "```\n",
        "Original Weight (FP32):  0.123456789012345678901234567890\n",
        "FP16 Weight:            0.1234567890\n",
        "INT8 Weight:            0.12345\n",
        "INT4 Weight:            0.12\n",
        "```\n",
        "\n",
        "### The Precision Ladder\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Precision    \u2502 Bits \u2502 Memory (1.5B) \u2502 Use Case                         \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 FP32         \u2502  32  \u2502    ~6 GB      \u2502 Training, highest accuracy       \u2502\n",
        "\u2502 FP16/BF16    \u2502  16  \u2502    ~3 GB      \u2502 Standard inference               \u2502\n",
        "\u2502 INT8         \u2502   8  \u2502   ~1.5 GB     \u2502 Good quality, 2x compression     \u2502\n",
        "\u2502 INT4 (NF4)   \u2502   4  \u2502   ~0.8 GB     \u2502 Acceptable quality, 4x compress  \u2502\n",
        "\u2502 INT2/1.58b   \u2502  1-2 \u2502   ~0.4 GB     \u2502 Experimental, research           \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### Memory Formula\n",
        "\n",
        "```\n",
        "Model Size (GB) \u2248 (Parameters \u00d7 Bits per Weight) / (8 \u00d7 10^9)\n",
        "\n",
        "For 1.5B parameters:\n",
        "  FP16: 1.5B \u00d7 16 / 8B = 3.0 GB\n",
        "  INT8: 1.5B \u00d7  8 / 8B = 1.5 GB\n",
        "  INT4: 1.5B \u00d7  4 / 8B = 0.75 GB\n",
        "```\n",
        "\n",
        "### Quantization Methods\n",
        "\n",
        "| Method | Description | Pros | Cons |\n",
        "|--------|-------------|------|------|\n",
        "| **Post-Training (PTQ)** | Quantize after training | Fast, no retraining | Some accuracy loss |\n",
        "| **Quantization-Aware (QAT)** | Train with quantization | Better accuracy | Requires training |\n",
        "| **GPTQ** | 4-bit, layer-by-layer | Good quality at 4-bit | Slower to quantize |\n",
        "| **bitsandbytes** | Dynamic 8/4-bit | Easy to use, on-the-fly | Slightly slower inference |\n",
        "| **AWQ** | Activation-aware | Best 4-bit quality | Slower quantization |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Check\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Python:\", sys.executable)\n",
        "print(\"\\n=== Package Versions ===\")\n",
        "\n",
        "packages = [\n",
        "    (\"torch\", \"PyTorch\"),\n",
        "    (\"transformers\", \"Transformers\"),\n",
        "    (\"accelerate\", \"Accelerate\"),\n",
        "    (\"bitsandbytes\", \"BitsAndBytes\"),\n",
        "    (\"plotly\", \"Plotly\"),\n",
        "    (\"ipywidgets\", \"Widgets\"),\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for pkg, name in packages:\n",
        "    try:\n",
        "        mod = __import__(pkg)\n",
        "        ver = getattr(mod, \"__version__\", \"?\")\n",
        "        print(f\"\u2705 {name}: {ver}\")\n",
        "    except ImportError:\n",
        "        print(f\"\u274c {name}: NOT INSTALLED\")\n",
        "        missing.append(pkg)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n\u26a0\ufe0f Install missing: pip install {' '.join(missing)}\")\n",
        "\n",
        "print(\"\\n=== GPU Check ===\")\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\u2705 CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f CUDA not available - quantization benchmarks require GPU!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core Imports\n",
        "import time\n",
        "import gc\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Our utility functions\n",
        "from quant_utils import (\n",
        "    clear_gpu_memory,\n",
        "    get_gpu_memory_mb,\n",
        "    get_model_size_mb,\n",
        "    count_parameters,\n",
        "    calculate_perplexity_simple,\n",
        "    benchmark_generation,\n",
        "    load_model_fp16,\n",
        "    load_model_int8,\n",
        "    load_model_int4,\n",
        "    compare_outputs,\n",
        "    QuantizationBenchmark,\n",
        "    create_memory_chart,\n",
        "    create_speed_chart,\n",
        "    create_perplexity_chart,\n",
        "    create_tradeoff_chart,\n",
        "    create_summary_dashboard,\n",
        ")\n",
        "\n",
        "print(\"\u2705 All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "CALIBRATION_FILE = Path(\"calibration_texts/fico_calibration.txt\")\n",
        "\n",
        "# Load calibration text\n",
        "if CALIBRATION_FILE.exists():\n",
        "    CALIBRATION_TEXT = CALIBRATION_FILE.read_text()\n",
        "    print(f\"\u2705 Loaded calibration text: {len(CALIBRATION_TEXT)} chars\")\n",
        "else:\n",
        "    CALIBRATION_TEXT = \"\"\"\n",
        "    Credit scoring is a statistical analysis performed by lenders to determine creditworthiness.\n",
        "    The FICO Score ranges from 300 to 850 and is based on payment history, amounts owed,\n",
        "    length of credit history, credit mix, and new credit inquiries.\n",
        "    \"\"\"\n",
        "    print(\"\u26a0\ufe0f Using default calibration text\")\n",
        "\n",
        "# HARDER benchmark prompts that reveal quality differences\n",
        "BENCHMARK_PROMPTS = [\n",
        "    # Math reasoning (requires precision)\n",
        "    \"If a credit card has a $5,000 balance at 18.5% APR and you pay $200/month, how much interest will you pay in the first month? Show your calculation.\",\n",
        "    \n",
        "    # Multi-step reasoning\n",
        "    \"Alice has a credit score of 680. She pays off a $3,000 credit card balance and her utilization drops from 80% to 15%. Her payment history is perfect. Will her score likely go up or down, and by approximately how much?\",\n",
        "    \n",
        "    # Factual precision\n",
        "    \"List the exact 5 factors that make up a FICO Score and their precise percentage weights. Format: Factor (XX%).\",\n",
        "    \n",
        "    # Complex financial calculation\n",
        "    \"Calculate the effective annual rate for a loan with 12% nominal rate compounded monthly. Show the formula and final percentage.\",\n",
        "    \n",
        "    # Code generation (requires syntax precision)\n",
        "    \"Write a Python function called calculate_fico_factors that takes payment_history, credit_utilization, credit_age, credit_mix, and new_credit as inputs (all 0-100) and returns a weighted score using the standard FICO percentages (35%, 30%, 15%, 10%, 10%). Include input validation.\",\n",
        "    \n",
        "    # Long context reasoning\n",
        "    \"A borrower has: 1) 10 years of credit history, 2) never missed a payment, 3) 5 credit cards with total limit $50k and balance $5k, 4) 1 mortgage, 5) no new inquiries in 2 years. Analyze each FICO factor and predict if this profile would likely score above 750, between 670-750, or below 670. Explain your reasoning for each factor.\",\n",
        "]\n",
        "\n",
        "# Quality arena prompts with difficulty levels\n",
        "ARENA_PROMPTS = {\n",
        "    \"Math_Easy\": \"What is 25% of 800?\",\n",
        "    \n",
        "    \"Math_Medium\": \"A loan of $10,000 at 5% annual interest compounded monthly for 3 years. What's the total amount paid?\",\n",
        "    \n",
        "    \"Math_Hard\": \"Calculate the internal rate of return (IRR) for an investment: pay $1000 today, receive $300/year for 4 years. Show calculation steps.\",\n",
        "    \n",
        "    \"Reasoning_Easy\": \"Which is better for your credit: paying off a credit card or opening a new one?\",\n",
        "    \n",
        "    \"Reasoning_Medium\": \"You have two credit cards: Card A ($5k limit, $4k balance) and Card B ($10k limit, $1k balance). You have $3k to pay down debt. Should you pay Card A or split between both to minimize utilization impact? Explain numerically.\",\n",
        "    \n",
        "    \"Reasoning_Hard\": \"Three scenarios: A) Pay minimum on all cards, B) Pay off smallest balance first (snowball), C) Pay highest interest first (avalanche). For someone with $20k debt across 4 cards (rates: 24%, 18%, 15%, 12%), calculate total interest paid over 2 years for each strategy. Which is optimal?\",\n",
        "    \n",
        "    \"Code_Easy\": \"Write a function to check if a credit score is 'excellent' (740+), 'good' (670-739), or 'poor' (<670).\",\n",
        "    \n",
        "    \"Code_Medium\": \"Write a Python function that calculates credit utilization ratio given a list of tuples (balance, limit) for multiple cards. Handle edge cases like zero limits.\",\n",
        "    \n",
        "    \"Code_Hard\": \"Write a complete Python class CreditScoreSimulator with methods to: 1) add accounts, 2) record payments, 3) calculate utilization, 4) estimate score impact. Include proper error handling and docstrings.\",\n",
        "    \n",
        "    \"Factual_Precision\": \"What are the exact numeric ranges for FICO score categories: Exceptional, Very Good, Good, Fair, and Poor?\",\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Configuration:\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Benchmark prompts: {len(BENCHMARK_PROMPTS)} (HARDER TESTS)\")\n",
        "print(f\"   Arena prompts: {len(ARENA_PROMPTS)} (DIFFICULTY LEVELS)\")\n",
        "print(f\"\\n\ud83d\udca1 These prompts test: math precision, multi-step reasoning, code generation, factual accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Loading Models at Different Precisions\n",
        "\n",
        "We'll load the same model three times:\n",
        "1. **FP16** (16-bit floating point) - Baseline\n",
        "2. **INT8** (8-bit integer) - 2x smaller\n",
        "3. **INT4** (4-bit integer) - 4x smaller\n",
        "\n",
        "For each, we'll measure:\n",
        "- Load time\n",
        "- GPU memory usage\n",
        "- Peak memory during loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Storage for models and results\n",
        "models = {}  # {precision: (model, tokenizer)}\n",
        "load_results = {}  # {precision: ModelLoadResult}\n",
        "benchmarks = []  # List of QuantizationBenchmark\n",
        "\n",
        "print(\"\ud83d\udd04 Will load models one at a time to accurately measure memory.\")\n",
        "print(\"   This takes a few minutes per model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FP16 Model (Baseline)\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading FP16 Model (Baseline)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "clear_gpu_memory()\n",
        "print(f\"GPU Memory before: {get_gpu_memory_mb():.1f} MB\")\n",
        "\n",
        "result_fp16 = load_model_fp16(MODEL_NAME)\n",
        "\n",
        "print(f\"\\n\u2705 FP16 Model Loaded!\")\n",
        "print(f\"   Load time: {result_fp16.load_time_seconds:.2f}s\")\n",
        "print(f\"   Memory used: {result_fp16.memory_mb:.1f} MB\")\n",
        "print(f\"   Peak memory: {result_fp16.peak_memory_mb:.1f} MB\")\n",
        "\n",
        "# Store\n",
        "models[\"FP16\"] = (result_fp16.model, result_fp16.tokenizer)\n",
        "load_results[\"FP16\"] = result_fp16\n",
        "\n",
        "# Quick test\n",
        "test_input = result_fp16.tokenizer(\"Hello\", return_tensors=\"pt\").to(result_fp16.model.device)\n",
        "with torch.no_grad():\n",
        "    test_output = result_fp16.model.generate(**test_input, max_new_tokens=5)\n",
        "print(f\"   Test output: {result_fp16.tokenizer.decode(test_output[0], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free FP16 model before loading INT8 (for accurate memory measurement)\n",
        "print(\"\\n\ud83e\uddf9 Clearing FP16 model to load INT8...\")\n",
        "del models[\"FP16\"]  # Delete the entire entry\n",
        "clear_gpu_memory()\n",
        "print(f\"GPU Memory after clearing: {get_gpu_memory_mb():.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load INT8 Model\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading INT8 Model (8-bit quantized)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "clear_gpu_memory()\n",
        "print(f\"GPU Memory before: {get_gpu_memory_mb():.1f} MB\")\n",
        "\n",
        "result_int8 = load_model_int8(MODEL_NAME)\n",
        "\n",
        "print(f\"\\n\u2705 INT8 Model Loaded!\")\n",
        "print(f\"   Load time: {result_int8.load_time_seconds:.2f}s\")\n",
        "print(f\"   Memory used: {result_int8.memory_mb:.1f} MB\")\n",
        "print(f\"   Peak memory: {result_int8.peak_memory_mb:.1f} MB\")\n",
        "print(f\"   Compression: {load_results['FP16'].memory_mb / result_int8.memory_mb:.1f}x smaller\")\n",
        "\n",
        "# Store\n",
        "models[\"INT8\"] = (result_int8.model, result_int8.tokenizer)\n",
        "load_results[\"INT8\"] = result_int8\n",
        "\n",
        "# Quick test\n",
        "test_input = result_int8.tokenizer(\"Hello\", return_tensors=\"pt\").to(result_int8.model.device)\n",
        "with torch.no_grad():\n",
        "    test_output = result_int8.model.generate(**test_input, max_new_tokens=5)\n",
        "print(f\"   Test output: {result_int8.tokenizer.decode(test_output[0], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free INT8 model before loading INT4\n",
        "print(\"\\n\ud83e\uddf9 Clearing INT8 model to load INT4...\")\n",
        "del models[\"INT8\"]  # Delete the entire entry\n",
        "clear_gpu_memory()\n",
        "print(f\"GPU Memory after clearing: {get_gpu_memory_mb():.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load INT4 Model (NF4 quantization)\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading INT4-NF4 Model (4-bit quantized)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "clear_gpu_memory()\n",
        "print(f\"GPU Memory before: {get_gpu_memory_mb():.1f} MB\")\n",
        "\n",
        "result_int4 = load_model_int4(MODEL_NAME, quant_type=\"nf4\")\n",
        "\n",
        "print(f\"\\n\u2705 INT4-NF4 Model Loaded!\")\n",
        "print(f\"   Load time: {result_int4.load_time_seconds:.2f}s\")\n",
        "print(f\"   Memory used: {result_int4.memory_mb:.1f} MB\")\n",
        "print(f\"   Peak memory: {result_int4.peak_memory_mb:.1f} MB\")\n",
        "print(f\"   Compression: {load_results['FP16'].memory_mb / result_int4.memory_mb:.1f}x smaller\")\n",
        "\n",
        "# Store\n",
        "models[\"INT4-NF4\"] = (result_int4.model, result_int4.tokenizer)\n",
        "load_results[\"INT4-NF4\"] = result_int4\n",
        "\n",
        "# Quick test\n",
        "test_input = result_int4.tokenizer(\"Hello\", return_tensors=\"pt\").to(result_int4.model.device)\n",
        "with torch.no_grad():\n",
        "    test_output = result_int4.model.generate(**test_input, max_new_tokens=5)\n",
        "print(f\"   Test output: {result_int4.tokenizer.decode(test_output[0], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary: Memory Comparison\n",
        "print(\"=\" * 60)\n",
        "print(\"Memory Comparison Summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fp16_mem = load_results[\"FP16\"].memory_mb\n",
        "\n",
        "summary_data = []\n",
        "for name, result in load_results.items():\n",
        "    bits = {\"FP16\": 16, \"INT8\": 8, \"INT4-NF4\": 4}.get(name, 16)\n",
        "    summary_data.append({\n",
        "        \"Precision\": name,\n",
        "        \"Bits\": bits,\n",
        "        \"Memory (MB)\": result.memory_mb,\n",
        "        \"Load Time (s)\": result.load_time_seconds,\n",
        "        \"Compression\": f\"{fp16_mem / result.memory_mb:.1f}x\",\n",
        "    })\n",
        "\n",
        "df_memory = pd.DataFrame(summary_data)\n",
        "display(df_memory)\n",
        "\n",
        "# Bar chart\n",
        "fig = px.bar(\n",
        "    df_memory,\n",
        "    x=\"Precision\",\n",
        "    y=\"Memory (MB)\",\n",
        "    title=\"GPU Memory Usage by Quantization Level\",\n",
        "    text=\"Memory (MB)\",\n",
        "    color=\"Precision\",\n",
        ")\n",
        "fig.update_traces(texttemplate=\"%{text:.0f} MB\", textposition=\"outside\")\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Benchmarking - Latency and Perplexity\n",
        "\n",
        "Now we'll measure:\n",
        "1. **Generation Speed** - tokens per second\n",
        "2. **Perplexity** - objective quality metric (lower = better)\n",
        "\n",
        "We need to reload all models for fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload all models for benchmarking\n",
        "# (We deleted them earlier to measure memory accurately)\n",
        "\n",
        "print(\"\ud83d\udd04 Reloading all models for benchmarking...\")\n",
        "print(\"   This may take a few minutes.\\n\")\n",
        "\n",
        "# Clear first\n",
        "models = {}\n",
        "clear_gpu_memory()\n",
        "\n",
        "# We'll keep only INT4 loaded (smallest) and load others as needed\n",
        "# Or reload all if you have enough VRAM\n",
        "\n",
        "# For machines with limited VRAM, we benchmark one at a time\n",
        "BENCHMARK_ONE_AT_A_TIME = True\n",
        "\n",
        "if not BENCHMARK_ONE_AT_A_TIME:\n",
        "    # Load all (needs ~5GB VRAM)\n",
        "    print(\"Loading FP16...\")\n",
        "    result = load_model_fp16(MODEL_NAME)\n",
        "    models[\"FP16\"] = (result.model, result.tokenizer)\n",
        "    \n",
        "    print(\"Loading INT8...\")\n",
        "    result = load_model_int8(MODEL_NAME)\n",
        "    models[\"INT8\"] = (result.model, result.tokenizer)\n",
        "    \n",
        "    print(\"Loading INT4-NF4...\")\n",
        "    result = load_model_int4(MODEL_NAME)\n",
        "    models[\"INT4-NF4\"] = (result.model, result.tokenizer)\n",
        "    \n",
        "    print(\"\\n\u2705 All models loaded!\")\n",
        "else:\n",
        "    print(\"Will benchmark models one at a time (memory-efficient mode)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark Function\n",
        "\n",
        "def run_full_benchmark(model, tokenizer, precision: str, bits: float) -> QuantizationBenchmark:\n",
        "    \"\"\"\n",
        "    Run complete benchmark for a model.\n",
        "    \n",
        "    Returns QuantizationBenchmark with all metrics.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Benchmarking {precision}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    memory_mb = get_gpu_memory_mb()\n",
        "    print(f\"Current memory: {memory_mb:.1f} MB\")\n",
        "    \n",
        "    # 1. Perplexity\n",
        "    print(\"\\n\ud83d\udcca Calculating perplexity...\")\n",
        "    ppl = calculate_perplexity_simple(model, tokenizer, CALIBRATION_TEXT)\n",
        "    print(f\"   Perplexity: {ppl:.2f}\")\n",
        "    \n",
        "    # 2. Generation speed\n",
        "    print(\"\\n\u26a1 Measuring generation speed...\")\n",
        "    speeds = []\n",
        "    sample_outputs = {}\n",
        "    \n",
        "    for prompt in BENCHMARK_PROMPTS[:3]:  # Use 3 prompts for speed\n",
        "        result = benchmark_generation(\n",
        "            model, tokenizer, prompt,\n",
        "            max_new_tokens=50,\n",
        "            num_runs=2,\n",
        "            warmup_runs=1,\n",
        "        )\n",
        "        speeds.append(result.tokens_per_second)\n",
        "        sample_outputs[prompt[:30]] = result.output[:100]\n",
        "    \n",
        "    avg_speed = sum(speeds) / len(speeds)\n",
        "    print(f\"   Avg speed: {avg_speed:.1f} tokens/sec\")\n",
        "    \n",
        "    return QuantizationBenchmark(\n",
        "        precision=precision,\n",
        "        bits_per_weight=bits,\n",
        "        memory_mb=memory_mb,\n",
        "        load_time_s=load_results.get(precision, load_results.get(\"FP16\")).load_time_seconds,\n",
        "        perplexity=ppl,\n",
        "        tokens_per_second=avg_speed,\n",
        "        sample_outputs=sample_outputs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run benchmarks one at a time\n",
        "\n",
        "benchmarks = []\n",
        "\n",
        "# FP16\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading and benchmarking FP16...\")\n",
        "print(\"=\"*60)\n",
        "clear_gpu_memory()\n",
        "result = load_model_fp16(MODEL_NAME)\n",
        "benchmark = run_full_benchmark(result.model, result.tokenizer, \"FP16\", 16.0)\n",
        "benchmark.memory_mb = result.memory_mb  # Use accurate load-time memory\n",
        "benchmark.load_time_s = result.load_time_seconds\n",
        "benchmarks.append(benchmark)\n",
        "\n",
        "# Keep FP16 for comparison later\n",
        "models[\"FP16\"] = (result.model, result.tokenizer)\n",
        "\n",
        "# Free and load INT8\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading and benchmarking INT8...\")\n",
        "print(\"=\"*60)\n",
        "if \"FP16\" in models:\n",
        "    del models[\"FP16\"]\n",
        "clear_gpu_memory()\n",
        "result = load_model_int8(MODEL_NAME)\n",
        "benchmark = run_full_benchmark(result.model, result.tokenizer, \"INT8\", 8.0)\n",
        "benchmark.memory_mb = result.memory_mb\n",
        "benchmark.load_time_s = result.load_time_seconds\n",
        "benchmarks.append(benchmark)\n",
        "\n",
        "# Free and load INT4\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading and benchmarking INT4-NF4...\")\n",
        "print(\"=\"*60)\n",
        "# Free the previous model and its reference\n",
        "del result\n",
        "clear_gpu_memory()\n",
        "result = load_model_int4(MODEL_NAME, quant_type=\"nf4\")\n",
        "benchmark = run_full_benchmark(result.model, result.tokenizer, \"INT4-NF4\", 4.0)\n",
        "benchmark.memory_mb = result.memory_mb\n",
        "benchmark.load_time_s = result.load_time_seconds\n",
        "benchmarks.append(benchmark)\n",
        "\n",
        "# Keep INT4 loaded (smallest)\n",
        "models[\"INT4-NF4\"] = (result.model, result.tokenizer)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 All benchmarks complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results Summary\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                    BENCHMARK RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create DataFrame\n",
        "results_data = []\n",
        "for b in benchmarks:\n",
        "    results_data.append({\n",
        "        \"Precision\": b.precision,\n",
        "        \"Bits\": b.bits_per_weight,\n",
        "        \"Memory (MB)\": f\"{b.memory_mb:.0f}\",\n",
        "        \"Load Time (s)\": f\"{b.load_time_s:.1f}\",\n",
        "        \"Perplexity\": f\"{b.perplexity:.2f}\",\n",
        "        \"Speed (tok/s)\": f\"{b.tokens_per_second:.1f}\",\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results_data)\n",
        "display(df_results)\n",
        "\n",
        "# Calculate relative metrics\n",
        "fp16_ppl = benchmarks[0].perplexity\n",
        "fp16_speed = benchmarks[0].tokens_per_second\n",
        "fp16_mem = benchmarks[0].memory_mb\n",
        "\n",
        "print(\"\\n\ud83d\udcca Relative to FP16 Baseline:\")\n",
        "for b in benchmarks:\n",
        "    ppl_change = ((b.perplexity - fp16_ppl) / fp16_ppl) * 100\n",
        "    speed_change = ((b.tokens_per_second - fp16_speed) / fp16_speed) * 100\n",
        "    mem_ratio = fp16_mem / b.memory_mb\n",
        "    \n",
        "    print(f\"\\n{b.precision}:\")\n",
        "    print(f\"   Memory: {mem_ratio:.1f}x smaller\")\n",
        "    print(f\"   Speed: {speed_change:+.1f}% {'faster' if speed_change > 0 else 'slower'}\")\n",
        "    print(f\"   Quality: {ppl_change:+.1f}% perplexity change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Summary Dashboard\n",
        "\n",
        "fig = create_summary_dashboard(benchmarks)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Individual Charts\n",
        "\n",
        "# Memory Chart\n",
        "fig_mem = create_memory_chart(benchmarks)\n",
        "fig_mem.show()\n",
        "\n",
        "# Speed Chart\n",
        "fig_speed = create_speed_chart(benchmarks)\n",
        "fig_speed.show()\n",
        "\n",
        "# Perplexity Chart\n",
        "fig_ppl = create_perplexity_chart(benchmarks)\n",
        "fig_ppl.show()\n",
        "\n",
        "# Tradeoff Chart\n",
        "fig_trade = create_tradeoff_chart(benchmarks)\n",
        "fig_trade.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Quality Arena - Side-by-Side Comparison\n",
        "\n",
        "Numbers are great, but can you actually *see* the quality difference?\n",
        "\n",
        "In this section, we'll:\n",
        "1. Generate outputs from all quantization levels for the same prompts\n",
        "2. Display them side-by-side\n",
        "3. Vote on which is best (blind voting available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload all models for comparison (if not already loaded)\n",
        "\n",
        "print(\"\ud83d\udd04 Loading all models for quality arena...\")\n",
        "\n",
        "# Clear everything\n",
        "models = {}\n",
        "clear_gpu_memory()\n",
        "\n",
        "# Load all three\n",
        "print(\"Loading FP16...\")\n",
        "result = load_model_fp16(MODEL_NAME)\n",
        "models[\"FP16\"] = (result.model, result.tokenizer)\n",
        "\n",
        "print(\"Loading INT8...\")\n",
        "result = load_model_int8(MODEL_NAME)\n",
        "models[\"INT8\"] = (result.model, result.tokenizer)\n",
        "\n",
        "print(\"Loading INT4-NF4...\")\n",
        "result = load_model_int4(MODEL_NAME)\n",
        "models[\"INT4-NF4\"] = (result.model, result.tokenizer)\n",
        "\n",
        "print(f\"\\n\u2705 All models loaded! Current GPU memory: {get_gpu_memory_mb():.0f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality Arena Class\n",
        "\n",
        "class QualityArena:\n",
        "    \"\"\"Interactive side-by-side quality comparison.\"\"\"\n",
        "    \n",
        "    def __init__(self, models_dict: dict, prompts: dict):\n",
        "        self.models = models_dict\n",
        "        self.prompts = prompts\n",
        "        self.responses = {}  # {prompt: {precision: response}}\n",
        "        self.votes = {p: 0 for p in models_dict.keys()}  # Vote counts\n",
        "        self.current_prompt_idx = 0\n",
        "        self.prompt_names = list(prompts.keys())\n",
        "        self.blind_mode = False\n",
        "        \n",
        "    def generate_all_responses(self, prompt: str, max_new_tokens: int = 150):\n",
        "        \"\"\"Generate responses from all models for a prompt.\"\"\"\n",
        "        responses = {}\n",
        "        \n",
        "        for precision, (model, tokenizer) in self.models.items():\n",
        "            device = next(model.parameters()).device\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "                )\n",
        "            \n",
        "            response = tokenizer.decode(\n",
        "                outputs[0][inputs.input_ids.shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "            responses[precision] = response.strip()\n",
        "        \n",
        "        return responses\n",
        "    \n",
        "    def display_comparison(self, prompt_name: str):\n",
        "        \"\"\"Display side-by-side comparison for a prompt.\"\"\"\n",
        "        prompt = self.prompts[prompt_name]\n",
        "        \n",
        "        if prompt not in self.responses:\n",
        "            print(f\"\\n\u23f3 Generating responses for: {prompt_name}\")\n",
        "            self.responses[prompt] = self.generate_all_responses(prompt)\n",
        "        \n",
        "        responses = self.responses[prompt]\n",
        "        \n",
        "        # Display\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Task: {prompt_name}\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        if self.blind_mode:\n",
        "            # Shuffle for blind comparison\n",
        "            import random\n",
        "            items = list(responses.items())\n",
        "            random.shuffle(items)\n",
        "            labels = [\"Option A\", \"Option B\", \"Option C\"]\n",
        "            \n",
        "            for label, (precision, response) in zip(labels, items):\n",
        "                print(f\"\\n\ud83d\udcdd {label}:\")\n",
        "                print(\"-\" * 40)\n",
        "                print(response[:500])\n",
        "                print()\n",
        "            \n",
        "            print(\"\\n\ud83d\udd12 Reveal: \" + \", \".join([f\"{l}={p}\" for l, (p, _) in zip(labels, items)]))\n",
        "        else:\n",
        "            for precision, response in responses.items():\n",
        "                print(f\"\\n\ud83d\udcdd {precision}:\")\n",
        "                print(\"-\" * 40)\n",
        "                print(response[:500])\n",
        "                print()\n",
        "    \n",
        "    def record_vote(self, precision: str):\n",
        "        \"\"\"Record a vote for a precision level.\"\"\"\n",
        "        if precision in self.votes:\n",
        "            self.votes[precision] += 1\n",
        "            print(f\"\u2705 Vote recorded for {precision}\")\n",
        "    \n",
        "    def show_results(self):\n",
        "        \"\"\"Display voting results.\"\"\"\n",
        "        total = sum(self.votes.values())\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        QUALITY ARENA RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        if total == 0:\n",
        "            print(\"No votes recorded yet.\")\n",
        "            return\n",
        "        \n",
        "        sorted_votes = sorted(self.votes.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        for i, (precision, count) in enumerate(sorted_votes, 1):\n",
        "            pct = (count / total) * 100\n",
        "            bar = \"\u2588\" * int(pct / 5)\n",
        "            medal = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\"][i-1] if i <= 3 else \"  \"\n",
        "            print(f\"{medal} {precision}: {bar} {count} votes ({pct:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\nTotal votes: {total}\")\n",
        "\n",
        "\n",
        "# Create arena\n",
        "arena = QualityArena(models, ARENA_PROMPTS)\n",
        "print(\"\u2705 Quality Arena ready!\")\n",
        "print(f\"   Prompts: {list(ARENA_PROMPTS.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Arena Widget\n",
        "\n",
        "# Prompt selector\n",
        "w_prompt = widgets.Dropdown(\n",
        "    options=list(ARENA_PROMPTS.keys()),\n",
        "    description=\"Task:\",\n",
        "    style={\"description_width\": \"initial\"}\n",
        ")\n",
        "\n",
        "# Blind mode toggle\n",
        "w_blind = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Blind Mode (shuffle labels)\",\n",
        ")\n",
        "\n",
        "# Generate button\n",
        "w_generate = widgets.Button(\n",
        "    description=\"Generate Comparisons\",\n",
        "    button_style=\"primary\",\n",
        ")\n",
        "\n",
        "# Vote buttons\n",
        "w_vote_fp16 = widgets.Button(description=\"Vote FP16\", button_style=\"info\")\n",
        "w_vote_int8 = widgets.Button(description=\"Vote INT8\", button_style=\"warning\")\n",
        "w_vote_int4 = widgets.Button(description=\"Vote INT4\", button_style=\"success\")\n",
        "\n",
        "# Results button\n",
        "w_results = widgets.Button(description=\"Show Results\", button_style=\"\")\n",
        "\n",
        "# Output\n",
        "w_output = widgets.Output()\n",
        "\n",
        "def on_generate(b):\n",
        "    arena.blind_mode = w_blind.value\n",
        "    with w_output:\n",
        "        clear_output()\n",
        "        arena.display_comparison(w_prompt.value)\n",
        "\n",
        "def on_vote_fp16(b):\n",
        "    arena.record_vote(\"FP16\")\n",
        "    \n",
        "def on_vote_int8(b):\n",
        "    arena.record_vote(\"INT8\")\n",
        "    \n",
        "def on_vote_int4(b):\n",
        "    arena.record_vote(\"INT4-NF4\")\n",
        "\n",
        "def on_results(b):\n",
        "    with w_output:\n",
        "        arena.show_results()\n",
        "\n",
        "w_generate.on_click(on_generate)\n",
        "w_vote_fp16.on_click(on_vote_fp16)\n",
        "w_vote_int8.on_click(on_vote_int8)\n",
        "w_vote_int4.on_click(on_vote_int4)\n",
        "w_results.on_click(on_results)\n",
        "\n",
        "# Display\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>\ud83c\udfc6 Quality Arena</h3>\"),\n",
        "    widgets.HBox([w_prompt, w_blind]),\n",
        "    w_generate,\n",
        "    widgets.HTML(\"<br><b>Vote for the best response:</b>\"),\n",
        "    widgets.HBox([w_vote_fp16, w_vote_int8, w_vote_int4, w_results]),\n",
        "    w_output,\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual comparison (non-widget fallback)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"           SIDE-BY-SIDE QUALITY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for task_name, prompt in ARENA_PROMPTS.items():\n",
        "    print(f\"\\n\\n{'#'*70}\")\n",
        "    print(f\"# Task: {task_name}\")\n",
        "    print(f\"# Prompt: {prompt}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "    \n",
        "    responses = arena.generate_all_responses(prompt, max_new_tokens=100)\n",
        "    arena.responses[prompt] = responses\n",
        "    \n",
        "    for precision, response in responses.items():\n",
        "        print(f\"\\n\ud83d\udcdd [{precision}]:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(response[:400])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 All comparisons generated!\")\n",
        "print(\"   Use the voting widget above or call arena.record_vote('FP16')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Exercises\n",
        "\n",
        "Test your understanding with these challenges!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Memory Budget Challenge\n",
        "\n",
        "**Scenario:** You have a GPU with only **2GB** of available VRAM. \n",
        "\n",
        "**Question:** Which quantization level(s) can fit, and what's the tradeoff?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Check which models fit in 2GB\n",
        "\n",
        "VRAM_BUDGET_MB = 2000  # 2GB\n",
        "\n",
        "print(f\"VRAM Budget: {VRAM_BUDGET_MB} MB\\n\")\n",
        "\n",
        "for b in benchmarks:\n",
        "    fits = \"\u2705 FITS\" if b.memory_mb <= VRAM_BUDGET_MB else \"\u274c TOO BIG\"\n",
        "    print(f\"{b.precision}: {b.memory_mb:.0f} MB - {fits}\")\n",
        "\n",
        "# Your analysis:\n",
        "# TODO: Which would you choose and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Quality Threshold\n",
        "\n",
        "**Scenario:** Your application requires perplexity to stay within **10%** of the FP16 baseline.\n",
        "\n",
        "**Question:** What's the most aggressive quantization you can use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Find lowest precision meeting quality threshold\n",
        "\n",
        "MAX_PPL_INCREASE = 0.10  # 10% max increase\n",
        "\n",
        "fp16_ppl = benchmarks[0].perplexity\n",
        "threshold = fp16_ppl * (1 + MAX_PPL_INCREASE)\n",
        "\n",
        "print(f\"FP16 Baseline Perplexity: {fp16_ppl:.2f}\")\n",
        "print(f\"Max Allowed Perplexity: {threshold:.2f} (+{MAX_PPL_INCREASE*100:.0f}%)\\n\")\n",
        "\n",
        "for b in benchmarks:\n",
        "    ppl_change = ((b.perplexity - fp16_ppl) / fp16_ppl) * 100\n",
        "    meets = \"\u2705 MEETS\" if b.perplexity <= threshold else \"\u274c EXCEEDS\"\n",
        "    print(f\"{b.precision}: PPL={b.perplexity:.2f} ({ppl_change:+.1f}%) - {meets}\")\n",
        "\n",
        "# Your analysis:\n",
        "# TODO: Which precision would you recommend?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Pareto Frontier\n",
        "\n",
        "**Question:** Looking at the tradeoff chart, which quantization level is on the \"Pareto frontier\" (best tradeoff between speed and quality)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Analyze the Pareto frontier\n",
        "\n",
        "# Recreate tradeoff chart for analysis\n",
        "fig = create_tradeoff_chart(benchmarks)\n",
        "fig.show()\n",
        "\n",
        "# Analysis helper\n",
        "print(\"\\nSpeed vs Quality Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "for b in benchmarks:\n",
        "    # Calculate \"efficiency score\" (higher speed, lower perplexity = better)\n",
        "    efficiency = b.tokens_per_second / b.perplexity\n",
        "    print(f\"{b.precision}: Speed={b.tokens_per_second:.1f} tok/s, PPL={b.perplexity:.2f}, Efficiency={efficiency:.2f}\")\n",
        "\n",
        "# Your analysis:\n",
        "# TODO: Which point(s) are on the Pareto frontier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Blind Test\n",
        "\n",
        "**Challenge:** Can you tell the difference between FP16 and INT4 outputs?\n",
        "\n",
        "Enable blind mode in the Quality Arena and try to guess which is which!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Blind comparison\n",
        "\n",
        "import random\n",
        "\n",
        "print(\"\ud83d\udd12 BLIND TEST MODE\")\n",
        "print(\"=\"*50)\n",
        "print(\"Can you tell which output is FP16 vs INT4?\\n\")\n",
        "\n",
        "test_prompt = \"Explain compound interest in simple terms.\"\n",
        "\n",
        "# Generate responses\n",
        "responses = {}\n",
        "for precision, (model, tokenizer) in models.items():\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    responses[precision] = response.strip()\n",
        "\n",
        "# Shuffle\n",
        "items = list(responses.items())\n",
        "random.shuffle(items)\n",
        "labels = [\"Option A\", \"Option B\", \"Option C\"]\n",
        "label_map = {l: p for l, (p, _) in zip(labels, items)}\n",
        "\n",
        "print(f\"Prompt: {test_prompt}\\n\")\n",
        "\n",
        "for label, (precision, response) in zip(labels, items):\n",
        "    print(f\"\\n\ud83d\udcdd {label}:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(response[:500])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Which is FP16? Which is INT4? Make your guess!\")\n",
        "print(\"\\nRun the next cell to reveal the answer...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reveal blind test answers\n",
        "print(\"\ud83d\udd13 REVEAL\")\n",
        "print(\"=\"*50)\n",
        "for label, precision in label_map.items():\n",
        "    print(f\"{label} = {precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary & Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Quantization trades precision for efficiency**\n",
        "   - FP16 \u2192 INT8: ~2x memory reduction, minimal quality loss\n",
        "   - FP16 \u2192 INT4: ~4x memory reduction, some quality degradation\n",
        "\n",
        "2. **The right choice depends on your constraints**\n",
        "   - Limited VRAM? \u2192 INT4 or INT8\n",
        "   - Quality-critical? \u2192 FP16 or INT8\n",
        "   - Cost-sensitive? \u2192 INT4 with careful validation\n",
        "\n",
        "3. **Perplexity is a good objective metric**\n",
        "   - But always validate with task-specific evaluation\n",
        "   - Human preference matters for subjective tasks\n",
        "\n",
        "4. **Modern quantization is surprisingly good**\n",
        "   - NF4 (Normal Float 4-bit) preserves quality well\n",
        "   - For many tasks, users can't tell the difference\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "| Precision | When to Use |\n",
        "|-----------|-------------|\n",
        "| FP16 | Development, quality-critical production |\n",
        "| INT8 | Production with quality requirements |\n",
        "| INT4-NF4 | Edge deployment, cost optimization |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore **GPTQ** and **AWQ** for even better 4-bit quality\n",
        "- Try **quantization-aware training (QAT)** for critical applications\n",
        "- Investigate **speculative decoding** for faster inference\n",
        "- Consider **mixed precision** for optimal tradeoffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"           QUANTIZATION LAB - FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Show final benchmark table\n",
        "display(df_results)\n",
        "\n",
        "# Show voting results if any\n",
        "if sum(arena.votes.values()) > 0:\n",
        "    print(\"\\n\ud83d\udcca Quality Arena Voting Results:\")\n",
        "    arena.show_results()\n",
        "\n",
        "print(\"\\n\u2705 Lab complete! You now understand:\")\n",
        "print(\"   \u2022 How quantization affects model size\")\n",
        "print(\"   \u2022 The memory-quality-speed tradeoffs\")\n",
        "print(\"   \u2022 How to measure quality with perplexity\")\n",
        "print(\"   \u2022 How to choose the right precision for your use case\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "\n",
        "# Uncomment to free GPU memory when done:\n",
        "# for name in list(models.keys()):\n",
        "#     del models[name]\n",
        "# clear_gpu_memory()\n",
        "# print(f\"GPU memory after cleanup: {get_gpu_memory_mb():.0f} MB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}