{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 — Evals & Grading (RAGAS)\n",
    "\n",
    "**Audience:** senior cloud engineers.\n",
    "\n",
    "**Goal:** build a real evaluation harness for a RAG system.\n",
    "\n",
    "- **System under test (SUT):** *local* RAG pipeline execution\n",
    "- **Judge:** **local chat model (Ollama)** (OpenAI-compatible), used by RAGAS to grade answers\n",
    "\n",
    "## What you’ll do\n",
    "- Build a small, grounded **eval set** (cached)\n",
    "- Run the local RAG SUT to produce `response` + `retrieved_contexts`\n",
    "- Compute core RAGAS metrics:\n",
    "  - `faithfulness`\n",
    "  - `answer_relevancy`\n",
    "- Turn metrics into a **grade** (pass/fail gate)\n",
    "- Run a small knob sweep and observe quality/latency tradeoffs\n",
    "\n",
    "## Key env vars\n",
    "- `NIM_BASE_URL` (default `http://localhost:8000`) — NIM gateway (embeddings/rerank)\n",
    "- `NIM_CHAT_BASE_URL` (default `http://localhost:11434`) — Ollama (judge/chat)\n",
    "- `NIM_JUDGE_MODEL` (defaults to `NIM_GEN_MODEL`, then `llama3.1:8b`)\n",
    "- `NIM_API_KEY` (optional)\n",
    "\n",
    "**Prereq:**\n",
    "- NIM gateway running for embeddings (`./scripts/start_nims.sh`)\n",
    "- Ollama running for the judge (`./scripts/start_ollama.sh`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup + preflight\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from nim_clients import NIMClient, NIMConfig\n",
    "from rag_pipeline import (\n",
    "    Chunk,\n",
    "    Timer,\n",
    "    VectorIndex,\n",
    "    chunk_text,\n",
    "    make_prompt,\n",
    "    clean_answer,\n",
    ")\n",
    "\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "# ---- Env ----\n",
    "NIM_BASE_URL = os.environ.get(\"NIM_BASE_URL\", \"http://localhost:8000\").rstrip(\"/\")\n",
    "NIM_JUDGE_MODEL = (\n",
    "    os.environ.get(\"NIM_JUDGE_MODEL\")\n",
    "    or os.environ.get(\"NIM_GEN_MODEL\")\n",
    "    or \"llama3.1:8b\"\n",
    ")\n",
    "NIM_CHAT_BASE_URL = os.environ.get(\"NIM_CHAT_BASE_URL\", \"http://localhost:11434\").rstrip(\"/\")\n",
    "\n",
    "# Local SUT models (keep small-ish for workshop speed)\n",
    "LOCAL_EMBED_MODEL = os.environ.get(\"LOCAL_EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "LOCAL_GEN_MODEL = os.environ.get(\"LOCAL_GEN_MODEL\", \"llama3.1:8b\")\n",
    "\n",
    "RANDOM_SEED = int(os.environ.get(\"RANDOM_SEED\", \"42\"))\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "CACHE_DIR = Path(os.environ.get(\"EVAL_CACHE_DIR\", str(Path(\".module_eval_cache\")))).resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "print(\"NIM_BASE_URL:\", NIM_BASE_URL)\n",
    "print(\"NIM_JUDGE_MODEL:\", NIM_JUDGE_MODEL)\n",
    "print(\"LOCAL_EMBED_MODEL:\", LOCAL_EMBED_MODEL)\n",
    "print(\"LOCAL_GEN_MODEL:\", LOCAL_GEN_MODEL)\n",
    "print(\"CACHE_DIR:\", CACHE_DIR)\n",
    "\n",
    "\n",
    "def nim_ok() -> bool:\n",
    "    \"\"\"Quick reachability check using a tiny chat call.\"\"\"\n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": NIM_JUDGE_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Reply with a single word: ok\"}],\n",
    "            \"max_tokens\": 4,\n",
    "            \"temperature\": 0.0,\n",
    "        }\n",
    "        r = requests.post(NIM_CHAT_BASE_URL + \"/v1/chat/completions\", json=payload, timeout=10)\n",
    "        if r.status_code >= 400:\n",
    "            print(\"NIM judge call failed:\", r.status_code)\n",
    "            print((r.text or \"\").strip()[:400])\n",
    "            return False\n",
    "        j = r.json()\n",
    "        content = (((j.get(\"choices\") or [{}])[0].get(\"message\") or {}).get(\"content\") or \"\").strip()\n",
    "        print(\"NIM judge sample response:\", repr(content[:80]))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"NIM judge not reachable:\", type(e).__name__, str(e)[:200])\n",
    "        return False\n",
    "\n",
    "\n",
    "_ = nim_ok()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus loader (prefer newest run)\n",
    "\n",
    "\n",
    "def find_fico_dir() -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for c in [here, *here.parents]:\n",
    "        if (c / \"nim_clients.py\").exists() and (c / \"rag_pipeline.py\").exists():\n",
    "            return c\n",
    "        if (c / \"fico\" / \"nim_clients.py\").exists() and (c / \"fico\" / \"rag_pipeline.py\").exists():\n",
    "            return c / \"fico\"\n",
    "    raise RuntimeError(\"Could not locate fico/ directory\")\n",
    "\n",
    "\n",
    "FICO_DIR = find_fico_dir()\n",
    "print(\"FICO_DIR:\", FICO_DIR)\n",
    "\n",
    "\n",
    "def newest_corpus_csv() -> Path:\n",
    "    runs_dir = FICO_DIR / \"corpus_runs\"\n",
    "    if runs_dir.exists():\n",
    "        candidates = list(runs_dir.glob(\"*/fico_corpus_embedded.csv\"))\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda p: p.stat().st_mtime)\n",
    "            return candidates[-1]\n",
    "\n",
    "    for name in [\"fico_corpus_embedded_smoke.csv\", \"fico_corpus_embedded.csv\"]:\n",
    "        p = FICO_DIR / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    raise FileNotFoundError(\"No corpus CSV found.\")\n",
    "\n",
    "\n",
    "CORPUS_CSV = newest_corpus_csv()\n",
    "print(\"CORPUS_CSV:\", CORPUS_CSV)\n",
    "\n",
    "df_docs = pd.read_csv(CORPUS_CSV)\n",
    "print(\"docs:\", len(df_docs))\n",
    "print(\"columns:\", list(df_docs.columns))\n",
    "display(df_docs.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a local retrieval index (cached)\n",
    "\n",
    "SUT_CHUNK_SIZE = int(os.environ.get(\"SUT_CHUNK_SIZE\", \"900\"))\n",
    "SUT_OVERLAP = int(os.environ.get(\"SUT_CHUNK_OVERLAP\", \"150\"))\n",
    "\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in (s or \"\"))[:80]\n",
    "\n",
    "\n",
    "def build_chunk_rows(chunk_size: int, overlap: int) -> list[dict[str, Any]]:\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    for _, row in df_docs.iterrows():\n",
    "        doc_id = str(row.get(\"doc_id\"))\n",
    "        title = row.get(\"title\")\n",
    "        body = row.get(\"body_redacted\") or row.get(\"body\") or \"\"\n",
    "        parts = chunk_text(str(body), chunk_size=int(chunk_size), overlap=int(overlap))\n",
    "        for j, part in enumerate(parts):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}::c{j:03d}\",\n",
    "                    \"title\": str(title) if title is not None else None,\n",
    "                    \"text\": part,\n",
    "                }\n",
    "            )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def embed_local(texts: list[str]) -> np.ndarray:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer(LOCAL_EMBED_MODEL)\n",
    "    E = model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return np.asarray(E, dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_index(*, chunk_size: int, overlap: int) -> tuple[list[dict[str, Any]], VectorIndex]:\n",
    "    h = hashlib.sha256((LOCAL_EMBED_MODEL + f\"|cs={chunk_size}|ov={overlap}\").encode(\"utf-8\")).hexdigest()[:12]\n",
    "    p_rows = CACHE_DIR / f\"sut_chunks_{_slug(LOCAL_EMBED_MODEL)}_{h}.jsonl\"\n",
    "    p_emb = CACHE_DIR / f\"sut_emb_{_slug(LOCAL_EMBED_MODEL)}_{h}.npy\"\n",
    "\n",
    "    if p_rows.exists() and p_emb.exists():\n",
    "        rows = []\n",
    "        with open(p_rows, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        E = np.load(p_emb)\n",
    "        print(\"Loaded cached index:\", p_rows.name, p_emb.name)\n",
    "    else:\n",
    "        rows = build_chunk_rows(chunk_size=int(chunk_size), overlap=int(overlap))\n",
    "        texts = [r[\"text\"] for r in rows]\n",
    "        E = embed_local(texts)\n",
    "        with open(p_rows, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in rows:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "        np.save(p_emb, E)\n",
    "        print(\"Built index and cached:\", p_rows.name, p_emb.name)\n",
    "\n",
    "    index = VectorIndex(E.astype(np.float32), use_faiss=True)\n",
    "    return rows, index\n",
    "\n",
    "\n",
    "rows, index = get_index(chunk_size=SUT_CHUNK_SIZE, overlap=SUT_OVERLAP)\n",
    "print(\"chunks:\", len(rows), \"| embed_model:\", LOCAL_EMBED_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tiny eval set (grounded questions, cached)\n",
    "\n",
    "N_EVAL_QUESTIONS = int(os.environ.get(\"N_EVAL_QUESTIONS\", \"12\"))\n",
    "\n",
    "\n",
    "def _hash_for_evalset(n_questions: int) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(str(CORPUS_CSV).encode(\"utf-8\"))\n",
    "    h.update(str(int(CORPUS_CSV.stat().st_mtime)).encode(\"utf-8\"))\n",
    "    h.update(str(RANDOM_SEED).encode(\"utf-8\"))\n",
    "    h.update(str(int(n_questions)).encode(\"utf-8\"))\n",
    "    return h.hexdigest()[:12]\n",
    "\n",
    "\n",
    "def _extract_json_obj(s: str) -> dict[str, Any] | None:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        o = json.loads(s)\n",
    "        return o if isinstance(o, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        try:\n",
    "            o = json.loads(s[start : end + 1])\n",
    "            return o if isinstance(o, dict) else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_eval_questions(*, n_questions: int) -> pd.DataFrame:\n",
    "    out_path = CACHE_DIR / f\"eval_questions_{_hash_for_evalset(n_questions)}.json\"\n",
    "    if out_path.exists():\n",
    "        rows = json.loads(out_path.read_text(encoding=\"utf-8\"))\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    sample = df_docs.sample(n=min(int(n_questions), len(df_docs)), random_state=RANDOM_SEED)\n",
    "\n",
    "    cfg = NIMConfig(base_url=NIM_BASE_URL, timeout_s=60.0, gen_model=NIM_JUDGE_MODEL)\n",
    "    nim = NIMClient(cfg)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in sample.iterrows():\n",
    "        ctx = str(r.get(\"body_redacted\") or r.get(\"body\") or \"\").strip()[:1800]\n",
    "        title = str(r.get(\"title\") or \"\")\n",
    "        prompt = (\n",
    "            \"Create ONE question that can be answered ONLY from the provided context. \"\n",
    "            \"Return ONLY JSON with key 'question'.\\n\\n\"\n",
    "            f\"TITLE: {title}\\n\\nCONTEXT:\\n{ctx}\\n\\nJSON:\" \n",
    "        )\n",
    "        try:\n",
    "            txt, _ = nim.chat(prompt, max_tokens=96, temperature=0.0)\n",
    "            obj = _extract_json_obj(txt) or {}\n",
    "            q = str(obj.get(\"question\") or \"\").strip()\n",
    "        except Exception:\n",
    "            q = \"\"\n",
    "\n",
    "        q = (q.splitlines()[0] if q else \"\").strip()\n",
    "        if not q:\n",
    "            q = f\"What does the document titled '{title}' describe?\"\n",
    "        if not q.endswith(\"?\"):\n",
    "            q = q + \"?\"\n",
    "\n",
    "        rows.append({\"user_input\": q, \"doc_id\": str(r.get(\"doc_id\"))})\n",
    "\n",
    "    out_path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_q = make_eval_questions(n_questions=N_EVAL_QUESTIONS)\n",
    "print(\"questions:\", len(df_q))\n",
    "display(df_q.head(8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the SUT (local RAG) to produce evaluation records\n",
    "\n",
    "For each question we store:\n",
    "- `user_input`\n",
    "- `response`\n",
    "- `retrieved_contexts` (list of strings)\n",
    "\n",
    "And debugging you’ll care about in production:\n",
    "- per-stage timings (embed/query/index/gen)\n",
    "- retrieval knobs (`top_k`, `max_context_chars`)\n",
    "\n",
    "**Escape hatch:** if local generation is too heavy, set `SUT_GEN_MODE=nim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local RAG SUT implementation + record generation\n",
    "\n",
    "SUT_GEN_MODE = os.environ.get(\"SUT_GEN_MODE\", \"local\").lower()  # local | nim\n",
    "SUT_TOP_K = int(os.environ.get(\"SUT_TOP_K\", \"10\"))\n",
    "SUT_MAX_CONTEXT_CHARS = int(os.environ.get(\"SUT_MAX_CONTEXT_CHARS\", \"6000\"))\n",
    "\n",
    "# If LOCAL_GEN_MODEL is an Ollama id like \"llama3.1:8b\", Transformers can't load it.\n",
    "# In that case, automatically switch the SUT generator to the NIM/Ollama path.\n",
    "if SUT_GEN_MODE == \"local\" and \":\" in str(LOCAL_GEN_MODEL):\n",
    "    print(f\"LOCAL_GEN_MODEL={LOCAL_GEN_MODEL!r} looks like an Ollama model id; switching SUT_GEN_MODE -> 'nim'\")\n",
    "    SUT_GEN_MODE = \"nim\"\n",
    "\n",
    "# Optional: rerank only (ordering) using NIM reranker\n",
    "SUT_USE_NIM_RERANK = os.environ.get(\"SUT_USE_NIM_RERANK\", \"0\") == \"1\"\n",
    "SUT_RERANK_K = int(os.environ.get(\"SUT_RERANK_K\", \"5\"))\n",
    "\n",
    "print(\"SUT_GEN_MODE:\", SUT_GEN_MODE)\n",
    "print(\"SUT_TOP_K:\", SUT_TOP_K)\n",
    "print(\"SUT_MAX_CONTEXT_CHARS:\", SUT_MAX_CONTEXT_CHARS)\n",
    "print(\"SUT_USE_NIM_RERANK:\", SUT_USE_NIM_RERANK)\n",
    "print(\"SUT_RERANK_K:\", SUT_RERANK_K)\n",
    "\n",
    "\n",
    "def embed_query_local(text: str) -> np.ndarray:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer(LOCAL_EMBED_MODEL)\n",
    "    v = model.encode([text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return np.asarray(v[0], dtype=np.float32)\n",
    "\n",
    "\n",
    "_LOCAL_GEN = {\"tok\": None, \"model\": None}\n",
    "\n",
    "\n",
    "def ensure_local_generator():\n",
    "    if _LOCAL_GEN[\"tok\"] is not None and _LOCAL_GEN[\"model\"] is not None:\n",
    "        return _LOCAL_GEN[\"tok\"], _LOCAL_GEN[\"model\"]\n",
    "\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(LOCAL_GEN_MODEL, use_fast=True)\n",
    "    tok.padding_side = \"left\"\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LOCAL_GEN_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    _LOCAL_GEN[\"tok\"] = tok\n",
    "    _LOCAL_GEN[\"model\"] = model\n",
    "    return tok, model\n",
    "\n",
    "\n",
    "def gen_sut(prompt: str, *, max_new_tokens: int = 180, temperature: float = 0.2) -> tuple[str, float]:\n",
    "    if SUT_GEN_MODE == \"nim\":\n",
    "        # Use Ollama for generation (via NIMClient's chat path) and NIM gateway for everything else.\n",
    "        cfg = NIMConfig(\n",
    "            base_url=NIM_BASE_URL,\n",
    "            chat_base_url=NIM_CHAT_BASE_URL,\n",
    "            gen_model=os.environ.get(\"NIM_GEN_MODEL\") or str(LOCAL_GEN_MODEL),\n",
    "            timeout_s=120.0,\n",
    "        )\n",
    "        nim = NIMClient(cfg)\n",
    "        txt, dt = nim.chat(prompt, max_tokens=max_new_tokens, temperature=temperature)\n",
    "        return clean_answer(txt), dt\n",
    "\n",
    "    import torch\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    tok, model = ensure_local_generator()\n",
    "    inputs = tok([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    do_sample = float(temperature) > 1e-6\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=do_sample,\n",
    "            temperature=float(temperature) if do_sample else None,\n",
    "            top_p=0.95,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    gen_ids = out[0, inputs[\"input_ids\"].shape[1] :]\n",
    "    txt = tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return clean_answer(txt), (time.perf_counter() - t0)\n",
    "\n",
    "\n",
    "def retrieve_contexts(question: str, *, top_k: int, max_context_chars: int) -> tuple[list[Chunk], dict[str, float]]:\n",
    "    t = Timer()\n",
    "\n",
    "    qv = t.time(\"embed_query_s\", lambda: embed_query_local(question))\n",
    "    idxs, sims = t.time(\"index_search_s\", lambda: index.search(qv, k=int(top_k)))\n",
    "\n",
    "    chunks = [\n",
    "        Chunk(\n",
    "            doc_id=str(rows[i][\"doc_id\"]),\n",
    "            chunk_id=str(rows[i][\"chunk_id\"]),\n",
    "            title=rows[i].get(\"title\"),\n",
    "            text=str(rows[i][\"text\"]),\n",
    "        )\n",
    "        for i in idxs\n",
    "    ]\n",
    "\n",
    "    if SUT_USE_NIM_RERANK and chunks:\n",
    "        try:\n",
    "            cfg = NIMConfig(base_url=NIM_BASE_URL, timeout_s=60.0)\n",
    "            nim = NIMClient(cfg)\n",
    "            doc_texts = [c.text for c in chunks]\n",
    "            reranked, dt = nim.rerank(question, doc_texts, top_n=min(int(SUT_RERANK_K), len(doc_texts)))\n",
    "            t.timings[\"rerank_s\"] = dt\n",
    "            first = [chunks[i] for i in reranked if 0 <= i < len(chunks)]\n",
    "            rest = [c for j, c in enumerate(chunks) if j not in set(reranked)]\n",
    "            chunks = first + rest\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    packed: list[Chunk] = []\n",
    "    total = 0\n",
    "    for c in chunks:\n",
    "        if not c.text:\n",
    "            continue\n",
    "        if total >= int(max_context_chars):\n",
    "            break\n",
    "        take = c.text\n",
    "        remaining = int(max_context_chars) - total\n",
    "        if len(take) > remaining:\n",
    "            take = take[:remaining]\n",
    "        packed.append(Chunk(doc_id=c.doc_id, chunk_id=c.chunk_id, title=c.title, text=take))\n",
    "        total += len(take)\n",
    "\n",
    "    return packed, t.timings\n",
    "\n",
    "\n",
    "def run_sut(question: str, *, top_k: int, max_context_chars: int) -> dict[str, Any]:\n",
    "    ctx, timings = retrieve_contexts(question, top_k=top_k, max_context_chars=max_context_chars)\n",
    "    prompt = make_prompt(question, ctx)\n",
    "    ans, gen_dt = gen_sut(prompt, max_new_tokens=180, temperature=0.2)\n",
    "    timings = {**timings, \"gen_s\": float(gen_dt)}\n",
    "\n",
    "    return {\n",
    "        \"user_input\": question,\n",
    "        \"response\": ans,\n",
    "        \"retrieved_contexts\": [c.text for c in ctx],\n",
    "        \"debug\": {\n",
    "            \"top_k\": int(top_k),\n",
    "            \"max_context_chars\": int(max_context_chars),\n",
    "            \"n_contexts\": len(ctx),\n",
    "            **timings,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def make_sut_records(questions: list[str], *, top_k: int, max_context_chars: int) -> pd.DataFrame:\n",
    "    h = hashlib.sha256()\n",
    "    h.update(str(CORPUS_CSV).encode(\"utf-8\"))\n",
    "    h.update(str(int(CORPUS_CSV.stat().st_mtime)).encode(\"utf-8\"))\n",
    "    h.update(str(RANDOM_SEED).encode(\"utf-8\"))\n",
    "    h.update(str(SUT_CHUNK_SIZE).encode(\"utf-8\"))\n",
    "    h.update(str(SUT_OVERLAP).encode(\"utf-8\"))\n",
    "    h.update(str(top_k).encode(\"utf-8\"))\n",
    "    h.update(str(max_context_chars).encode(\"utf-8\"))\n",
    "    h.update(str(SUT_GEN_MODE).encode(\"utf-8\"))\n",
    "    key = h.hexdigest()[:12]\n",
    "\n",
    "    out_path = CACHE_DIR / f\"sut_records_{key}.jsonl\"\n",
    "\n",
    "    # Cache read (robust): if the file exists but is empty/corrupt (e.g., a previous run crashed\n",
    "    # mid-write), treat it as a cache miss and regenerate.\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        rows_in: list[dict[str, Any]] = []\n",
    "        try:\n",
    "            with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    obj = json.loads(line)\n",
    "                    if isinstance(obj, dict):\n",
    "                        rows_in.append(obj)\n",
    "            if rows_in:\n",
    "                return pd.DataFrame(rows_in)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Cache write (atomic): write to a temp file then replace.\n",
    "    tmp_path = out_path.with_suffix(out_path.suffix + \".tmp\")\n",
    "    rows_out: list[dict[str, Any]] = []\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for q in questions:\n",
    "            rec = run_sut(q, top_k=top_k, max_context_chars=max_context_chars)\n",
    "            rows_out.append(rec)\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    tmp_path.replace(out_path)\n",
    "\n",
    "    return pd.DataFrame(rows_out)\n",
    "\n",
    "\n",
    "questions = df_q[\"user_input\"].tolist()\n",
    "df_sut = make_sut_records(questions, top_k=SUT_TOP_K, max_context_chars=SUT_MAX_CONTEXT_CHARS)\n",
    "print(\"records:\", len(df_sut))\n",
    "display(df_sut[[\"user_input\", \"response\"]].head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS evaluation (judge = NIM)\n",
    "\n",
    "RAGAS needs an **LLM judge**. We’ll use `langchain-openai` to point it at NIM’s OpenAI-compatible API:\n",
    "\n",
    "- `base_url = NIM_BASE_URL + \"/v1\"`\n",
    "- `model = NIM_JUDGE_MODEL`\n",
    "\n",
    "If your Ollama server doesn’t require auth, leave `NIM_API_KEY` unset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS eval using a NIM-hosted judge (OpenAI-compatible)\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, AnswerRelevancy\n",
    "from ragas.run_config import RunConfig\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "ANSWER_RELEVANCY_STRICTNESS = int(os.environ.get(\"RAGAS_ANSWER_RELEVANCY_STRICTNESS\", \"1\"))\n",
    "\n",
    "\n",
    "def make_judge_llm():\n",
    "    \"\"\"Judge LLM: always Ollama on :11434 (never the nginx gateway on :8000).\"\"\"\n",
    "\n",
    "    api_key = os.environ.get(\"NIM_API_KEY\") or \"nim\"\n",
    "    judge_base_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "    # Ensure any implicit OpenAI clients also target Ollama.\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\") or api_key\n",
    "    for k in [\"OPENAI_BASE_URL\", \"OPENAI_API_BASE\", \"OPENAI_API_BASE_URL\"]:\n",
    "        os.environ[k] = judge_base_url\n",
    "\n",
    "    try:\n",
    "        from openai import AsyncOpenAI\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Missing dependency: openai\") from e\n",
    "\n",
    "    # Pass an explicit root async client so LangChain cannot accidentally use a cached client\n",
    "    # pointing at the gateway.\n",
    "    root_async_client = AsyncOpenAI(api_key=api_key, base_url=judge_base_url)\n",
    "\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Missing dependency: langchain-openai. Install it with: pip install langchain-openai\"\n",
    "        ) from e\n",
    "\n",
    "    return ChatOpenAI(\n",
    "        model=NIM_JUDGE_MODEL,\n",
    "        base_url=judge_base_url,\n",
    "        api_key=api_key,\n",
    "        root_async_client=root_async_client,\n",
    "        # Enforce structured outputs for RAGAS parsers (Ollama supports this).\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    "        n=1,\n",
    "        use_responses_api=False,\n",
    "        temperature=0.0,\n",
    "        timeout=60,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "\n",
    "class NIMRagasEmbeddings(Embeddings):\n",
    "    \"\"\"Use NIM embeddings via NIMClient (gateway on :8000 by default).\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: NIMConfig):\n",
    "        self.client = NIMClient(cfg)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        embs, _ = self.client.embed_many(texts, input_type=\"passage\")\n",
    "        return embs\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        embs, _ = self.client.embed([text], input_type=\"query\")\n",
    "        return embs[0]\n",
    "\n",
    "\n",
    "def make_ragas_embeddings():\n",
    "    # Force embeddings to the NIM gateway (avoid accidentally using Ollama for /v1/embeddings)\n",
    "    cfg = NIMConfig(\n",
    "        base_url=NIM_BASE_URL,\n",
    "        embed_base_url=NIM_BASE_URL,\n",
    "        rerank_base_url=NIM_BASE_URL,\n",
    "        chat_base_url=NIM_CHAT_BASE_URL,\n",
    "    )\n",
    "    return NIMRagasEmbeddings(cfg)\n",
    "\n",
    "\n",
    "def ragas_report(df_records: pd.DataFrame) -> pd.DataFrame:\n",
    "    ds = Dataset.from_pandas(df_records[[\"user_input\", \"response\", \"retrieved_contexts\"]])\n",
    "    llm = make_judge_llm()\n",
    "    embeddings = make_ragas_embeddings()\n",
    "\n",
    "    # Ollama only returns 1 completion; keep AnswerRelevancy strictness aligned.\n",
    "    answer_relevancy_metric = AnswerRelevancy(strictness=int(ANSWER_RELEVANCY_STRICTNESS))\n",
    "\n",
    "    report = evaluate(\n",
    "        ds,\n",
    "        metrics=[faithfulness, answer_relevancy_metric],\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        run_config=RunConfig(max_workers=1),\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return report.to_pandas()\n",
    "\n",
    "\n",
    "df_report = ragas_report(df_sut)\n",
    "print(\"rows:\", len(df_report), \"cols:\", list(df_report.columns))\n",
    "display(df_report[[\"user_input\", \"faithfulness\", \"answer_relevancy\"]].head(8))\n",
    "\n",
    "print(\"\\nAggregate metrics:\")\n",
    "print(df_report[[\"faithfulness\", \"answer_relevancy\"]].agg([\"mean\", \"median\"]))\n",
    "\n",
    "fig = px.histogram(df_report, x=\"faithfulness\", nbins=20, title=\"Faithfulness distribution\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade (pass/fail) + small knob sweep\n",
    "\n",
    "A practical way to ship this:\n",
    "\n",
    "- Pick thresholds (example only):\n",
    "  - **faithfulness mean ≥ 0.80**\n",
    "  - **answer_relevancy mean ≥ 0.70**\n",
    "- Add a “tail” guardrail:\n",
    "  - p10 faithfulness ≥ 0.60\n",
    "\n",
    "Then sweep a couple knobs to answer:\n",
    "- “How much quality do we buy with more retrieval/context?”\n",
    "- “What does it cost in latency?”\n",
    "\n",
    "Tip: Keep `N_EVAL_QUESTIONS` small during workshops; do bigger runs overnight in CI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade + sweep helper\n",
    "\n",
    "\n",
    "def summarize_timings(df_records: pd.DataFrame) -> dict[str, float]:\n",
    "    # debug is a dict per-row\n",
    "    dbg = [r if isinstance(r, dict) else {} for r in df_records.get(\"debug\", [])]\n",
    "    df_dbg = pd.DataFrame(dbg).fillna(0.0)\n",
    "    out = {}\n",
    "    for k in [\"embed_query_s\", \"index_search_s\", \"rerank_s\", \"gen_s\"]:\n",
    "        if k in df_dbg.columns:\n",
    "            out[k] = float(df_dbg[k].mean())\n",
    "    out[\"total_s\"] = float(sum(out.values()))\n",
    "    return out\n",
    "\n",
    "\n",
    "def grade_report(df_report: pd.DataFrame, *, f_mean: float = 0.80, ar_mean: float = 0.70, f_p10: float = 0.60) -> dict[str, Any]:\n",
    "    f = df_report[\"faithfulness\"].astype(float)\n",
    "    ar = df_report[\"answer_relevancy\"].astype(float)\n",
    "    stats = {\n",
    "        \"faithfulness_mean\": float(f.mean()),\n",
    "        \"faithfulness_p10\": float(np.quantile(f, 0.10)),\n",
    "        \"answer_relevancy_mean\": float(ar.mean()),\n",
    "    }\n",
    "    stats[\"pass\"] = bool(\n",
    "        stats[\"faithfulness_mean\"] >= float(f_mean)\n",
    "        and stats[\"answer_relevancy_mean\"] >= float(ar_mean)\n",
    "        and stats[\"faithfulness_p10\"] >= float(f_p10)\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Baseline grade\n",
    "baseline_grade = grade_report(df_report)\n",
    "print(\"Baseline grade:\")\n",
    "print(json.dumps(baseline_grade, indent=2))\n",
    "\n",
    "# Small sweep (keep tiny for workshop speed)\n",
    "TOP_KS = [5, 10, 15]\n",
    "MAX_CTXS = [2000, 6000]\n",
    "\n",
    "rows_sweep = []\n",
    "for top_k in TOP_KS:\n",
    "    for max_ctx in MAX_CTXS:\n",
    "        df_rec = make_sut_records(questions, top_k=int(top_k), max_context_chars=int(max_ctx))\n",
    "        df_rep = ragas_report(df_rec)\n",
    "        g = grade_report(df_rep)\n",
    "        t = summarize_timings(df_rec)\n",
    "        rows_sweep.append({\n",
    "            \"top_k\": int(top_k),\n",
    "            \"max_context_chars\": int(max_ctx),\n",
    "            **g,\n",
    "            **t,\n",
    "        })\n",
    "\n",
    "\n",
    "df_sweep = pd.DataFrame(rows_sweep)\n",
    "print(\"\\nSweep summary:\")\n",
    "display(df_sweep.sort_values([\"pass\", \"faithfulness_mean\", \"answer_relevancy_mean\"], ascending=False))\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sweep,\n",
    "    x=\"total_s\",\n",
    "    y=\"faithfulness_mean\",\n",
    "    color=\"pass\",\n",
    "    symbol=\"top_k\",\n",
    "    hover_data=[\"max_context_chars\", \"answer_relevancy_mean\", \"faithfulness_p10\"],\n",
    "    title=\"Quality vs latency (sweep)\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways + consulting drills\n",
    "\n",
    "### What to ship to a customer\n",
    "- A versioned **golden set** (questions)\n",
    "- A repeatable **harness** that produces `response` + `retrieved_contexts`\n",
    "- A **grade gate** (thresholds + rationale)\n",
    "- A simple report artifact (CSV/JSON) and a CI job that runs it\n",
    "\n",
    "### Drills\n",
    "1. **Quality regression:** lower `max_context_chars` and explain why faithfulness moves.\n",
    "2. **Latency SLO:** choose the fastest passing config from the sweep.\n",
    "3. **Metric gaming:** manually inspect a few “high score” outputs — do you actually like them?\n",
    "\n",
    "### Next step\n",
    "Add online monitoring (logs/metrics/traces) and correlate quality drops to retrieval changes, indexing issues, or upstream model changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of notebook.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
