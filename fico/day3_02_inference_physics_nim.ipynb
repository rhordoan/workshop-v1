{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 — Inference Physics (NIM-first)\n",
    "\n",
    "**Goal:** Build intuition for throughput, tail latency, queueing, and why batching/concurrency matter — using NIM as the serving target.\n",
    "\n",
    "**Bonus (if you have an H200):** we’ll run a few “hardware physics experiments” so engineers can *see* why VRAM, bandwidth, and KV cache dominate real-world inference.\n",
    "\n",
    "**Target:** NIM gateway (OpenAI-compatible) at `NIM_BASE_URL` (default `http://localhost:8000`).\n",
    "\n",
    "**Outputs (what you’ll see):**\n",
    "- GPU “engine check” + simple bandwidth sanity test\n",
    "- KV-cache concurrency cliff visualization\n",
    "- Latency distribution (p50/p95/p99)\n",
    "- Latency vs concurrency curves (the p95 cliff)\n",
    "- Throughput vs concurrency curve\n",
    "- Latency vs token budget curve\n",
    "\n",
    "**Timebox:** 60–90 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup + preflight\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "NIM_BASE_URL = os.environ.get(\"NIM_BASE_URL\", \"http://localhost:8000\").rstrip(\"/\")\n",
    "NIM_CHAT_PATH = os.environ.get(\"NIM_CHAT_PATH\", \"/v1/chat/completions\")\n",
    "NIM_GEN_MODEL = os.environ.get(\"NIM_GEN_MODEL\", \"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "print(\"NIM_BASE_URL:\", NIM_BASE_URL)\n",
    "print(\"NIM_CHAT_PATH:\", NIM_CHAT_PATH)\n",
    "print(\"NIM_GEN_MODEL:\", NIM_GEN_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: The engine check (know your metal)\n",
    "\n",
    "If you’re on an H200, you’re holding a rare card. Let’s prove what we’re running on.\n",
    "\n",
    "Key idea: **inference is often memory-bandwidth-bound**, not FLOPs-bound.\n",
    "\n",
    "We’ll:\n",
    "- confirm GPU model + total VRAM via NVML\n",
    "- show a quick “back-of-the-envelope” bandwidth comparison (A100 vs H100 vs H200)\n",
    "- run a tiny GPU memory-copy benchmark to sanity-check that we can move data fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVML-based GPU identity + VRAM\n",
    "\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlShutdown,\n",
    "    nvmlDeviceGetCount,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetName,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetDriverVersion,\n",
    ")\n",
    "\n",
    "\n",
    "def _bytes_gib(x: float) -> float:\n",
    "    return float(x) / (1024.0**3)\n",
    "\n",
    "\n",
    "def gpu_engine_check() -> dict:\n",
    "    nvmlInit()\n",
    "    try:\n",
    "        n = nvmlDeviceGetCount()\n",
    "        if n < 1:\n",
    "            raise RuntimeError(\"No NVIDIA GPUs visible to NVML\")\n",
    "\n",
    "        h = nvmlDeviceGetHandleByIndex(0)\n",
    "        name = nvmlDeviceGetName(h).decode(\"utf-8\", errors=\"ignore\")\n",
    "        info = nvmlDeviceGetMemoryInfo(h)\n",
    "        drv = nvmlDeviceGetDriverVersion().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        out = {\n",
    "            \"gpu_name\": name,\n",
    "            \"driver\": drv,\n",
    "            \"vram_total_gib\": _bytes_gib(info.total),\n",
    "            \"vram_used_gib\": _bytes_gib(info.used),\n",
    "            \"vram_free_gib\": _bytes_gib(info.free),\n",
    "        }\n",
    "        return out\n",
    "    finally:\n",
    "        try:\n",
    "            nvmlShutdown()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "info = gpu_engine_check()\n",
    "print(\"GPU detected:\", info[\"gpu_name\"])\n",
    "print(\"Driver:\", info[\"driver\"])\n",
    "print(f\"Total VRAM: {info['vram_total_gib']:.2f} GiB\")\n",
    "print(f\"Used VRAM:  {info['vram_used_gib']:.2f} GiB\")\n",
    "\n",
    "print(\"\\n--- Bandwidth context (spec-level, for intuition) ---\")\n",
    "print(\"A100 (80GB HBM2e):  ~2.0 TB/s\")\n",
    "print(\"H100 (80GB HBM3):   ~3.3 TB/s\")\n",
    "print(\"H200 (141GB HBM3e): ~4.8 TB/s\")\n",
    "print(\"\\nSpeaker note: VRAM size is nice; bandwidth is the real villain/hero for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny memory-bandwidth sanity check (GPU memcpy benchmark)\n",
    "# Caveat: microbenchmarks lie. But they lie consistently.\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def memcpy_gbps(*, mib: int = 2048, iters: int = 50) -> float:\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA not available\")\n",
    "\n",
    "    n_bytes = int(mib) * 1024 * 1024\n",
    "    n = n_bytes // 4  # float32\n",
    "\n",
    "    a = torch.empty(n, device=\"cuda\", dtype=torch.float32)\n",
    "    b = torch.empty_like(a)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        b.copy_(a)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(int(iters)):\n",
    "        b.copy_(a)\n",
    "    torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    total = n_bytes * int(iters)\n",
    "    gbps = (total / dt) / 1e9\n",
    "    return float(gbps)\n",
    "\n",
    "\n",
    "try:\n",
    "    gbps = memcpy_gbps(mib=1024, iters=80)\n",
    "    print(f\"Approx device copy throughput: {gbps:.1f} GB/s\")\n",
    "    print(\"Interpretation: higher is better; compare runs on the same machine.\")\n",
    "except Exception as e:\n",
    "    print(\"Memcpy benchmark skipped:\", type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live VRAM watcher (sample NVML + plot). Useful for demos and debugging.\n",
    "\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlShutdown,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetUtilizationRates,\n",
    "    nvmlDeviceGetPowerUsage,\n",
    "    nvmlDeviceGetTemperature,\n",
    "    NVML_TEMPERATURE_GPU,\n",
    ")\n",
    "\n",
    "\n",
    "def nvml_snapshot() -> dict[str, float]:\n",
    "    nvmlInit()\n",
    "    try:\n",
    "        h = nvmlDeviceGetHandleByIndex(0)\n",
    "        mem = nvmlDeviceGetMemoryInfo(h)\n",
    "        util = nvmlDeviceGetUtilizationRates(h)\n",
    "        pwr_w = float(nvmlDeviceGetPowerUsage(h)) / 1000.0\n",
    "        temp_c = float(nvmlDeviceGetTemperature(h, NVML_TEMPERATURE_GPU))\n",
    "        return {\n",
    "            \"vram_used_gib\": _bytes_gib(mem.used),\n",
    "            \"vram_total_gib\": _bytes_gib(mem.total),\n",
    "            \"gpu_util_pct\": float(util.gpu),\n",
    "            \"mem_util_pct\": float(util.memory),\n",
    "            \"power_w\": float(pwr_w),\n",
    "            \"temp_c\": float(temp_c),\n",
    "        }\n",
    "    finally:\n",
    "        try:\n",
    "            nvmlShutdown()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def watch_gpu(*, seconds: float = 10.0, interval_s: float = 0.25) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    t0 = time.perf_counter()\n",
    "    while True:\n",
    "        t = time.perf_counter() - t0\n",
    "        if t > float(seconds):\n",
    "            break\n",
    "        try:\n",
    "            snap = nvml_snapshot()\n",
    "        except Exception as e:\n",
    "            snap = {\"error\": 1.0}\n",
    "            snap[\"vram_used_gib\"] = float(\"nan\")\n",
    "            snap[\"gpu_util_pct\"] = float(\"nan\")\n",
    "            snap[\"power_w\"] = float(\"nan\")\n",
    "            snap[\"temp_c\"] = float(\"nan\")\n",
    "        snap[\"t_s\"] = float(t)\n",
    "        rows.append(snap)\n",
    "        time.sleep(float(interval_s))\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Quick demo: a short watch window (should be mostly idle).\n",
    "df_watch = watch_gpu(seconds=3.0, interval_s=0.25)\n",
    "display(df_watch.tail(5))\n",
    "\n",
    "if not df_watch.empty and \"vram_used_gib\" in df_watch.columns:\n",
    "    fig = px.line(df_watch, x=\"t_s\", y=[\"vram_used_gib\", \"gpu_util_pct\", \"power_w\"], title=\"GPU watch (sampled via NVML)\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: The KV cache physics experiment (why concurrency eats VRAM)\n",
    "\n",
    "This is the most important “inference physics” mental model:\n",
    "\n",
    "- Model weights are mostly fixed.\n",
    "- The **KV cache** grows with:\n",
    "  - number of concurrent sequences (users)\n",
    "  - context length (prefill tokens)\n",
    "  - number of layers\n",
    "  - hidden size\n",
    "  - precision (FP16 vs FP8)\n",
    "\n",
    "If you’ve ever wondered why a serving system falls off a cliff at p95, KV cache is often holding the cliff sign.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV cache VRAM cliff: interactive visualization\n",
    "\n",
    "\n",
    "def kv_cache_gib(*, batch: int, seq_len: int, layers: int, hidden: int, bytes_per: int) -> float:\n",
    "    # Formula: 2 (K + V) * layers * hidden * seq_len * batch * bytes_per\n",
    "    cache_per_token = 2 * int(layers) * int(hidden) * int(bytes_per)\n",
    "    total_bytes = int(batch) * int(seq_len) * int(cache_per_token)\n",
    "    return float(total_bytes) / (1024.0**3)\n",
    "\n",
    "\n",
    "w_users = widgets.IntSlider(value=20, min=1, max=200, step=1, description=\"Concurrent users\", style={\"description_width\": \"initial\"})\n",
    "w_seq = widgets.IntSlider(value=8192, min=512, max=16384, step=512, description=\"Context length (tokens)\", style={\"description_width\": \"initial\"})\n",
    "w_layers = widgets.IntSlider(value=80, min=12, max=120, step=4, description=\"Layers\", style={\"description_width\": \"initial\"})\n",
    "w_hidden = widgets.IntSlider(value=8192, min=1024, max=16384, step=1024, description=\"Hidden dim\", style={\"description_width\": \"initial\"})\n",
    "w_bytes = widgets.Dropdown(options=[(\"FP16 (2 bytes)\", 2), (\"FP8 (1 byte)\", 1)], value=2, description=\"KV precision\", style={\"description_width\": \"initial\"})\n",
    "\n",
    "w_weights = widgets.FloatSlider(value=130.0, min=1.0, max=200.0, step=1.0, description=\"Model weights (GiB)\", style={\"description_width\": \"initial\"})\n",
    "\n",
    "w_line_a100 = widgets.FloatSlider(value=80.0, min=40.0, max=160.0, step=1.0, description=\"Line: A100/H100 VRAM (GiB)\", style={\"description_width\": \"initial\"})\n",
    "w_line_h200 = widgets.FloatSlider(value=141.0, min=80.0, max=200.0, step=1.0, description=\"Line: H200 VRAM (GiB)\", style={\"description_width\": \"initial\"})\n",
    "\n",
    "out_kv = widgets.Output()\n",
    "\n",
    "\n",
    "def _render_kv(*_):\n",
    "    with out_kv:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        users_max = int(w_users.value)\n",
    "        xs = list(range(1, users_max + 1))\n",
    "\n",
    "        v_kv = [kv_cache_gib(batch=u, seq_len=int(w_seq.value), layers=int(w_layers.value), hidden=int(w_hidden.value), bytes_per=int(w_bytes.value)) for u in xs]\n",
    "        v_total = [float(w_weights.value) + v for v in v_kv]\n",
    "\n",
    "        df = pd.DataFrame({\"users\": xs, \"kv_cache_gib\": v_kv, \"total_vram_gib\": v_total})\n",
    "\n",
    "        fig = px.line(df, x=\"users\", y=\"total_vram_gib\", title=f\"KV cache cliff (seq_len={int(w_seq.value)} tokens)\")\n",
    "        fig.add_hline(y=float(w_line_a100.value), line_dash=\"dash\", annotation_text=\"~80 GiB line\")\n",
    "        fig.add_hline(y=float(w_line_h200.value), line_dash=\"dash\", annotation_text=\"~141 GiB line\")\n",
    "        fig.update_layout(xaxis_title=\"concurrent users\", yaxis_title=\"GiB (weights + KV cache)\")\n",
    "        fig.show()\n",
    "\n",
    "        # Where do we crash?\n",
    "        def first_over(limit: float):\n",
    "            over = df[df[\"total_vram_gib\"] > float(limit)]\n",
    "            if over.empty:\n",
    "                return None\n",
    "            return int(over.iloc[0][\"users\"])\n",
    "\n",
    "        crash_80 = first_over(float(w_line_a100.value))\n",
    "        crash_141 = first_over(float(w_line_h200.value))\n",
    "\n",
    "        print(\"=== Back-of-the-envelope ===\")\n",
    "        print(f\"Weights: {float(w_weights.value):.1f} GiB\")\n",
    "        print(f\"KV precision bytes: {int(w_bytes.value)}\")\n",
    "        print(f\"KV at 1 user: {float(v_kv[0]):.2f} GiB\")\n",
    "        print(f\"KV at {users_max} users: {float(v_kv[-1]):.2f} GiB\")\n",
    "        print(\"\")\n",
    "        print(\"Crash estimate (first user count over the line):\")\n",
    "        print(\"- ~80 GiB line:\", crash_80)\n",
    "        print(\"- ~141 GiB line:\", crash_141)\n",
    "\n",
    "        print(\"\\nSpeaker note:\")\n",
    "        print(\"- This is why long context + concurrency is the real VRAM tax.\")\n",
    "        print(\"- Batching is good until it becomes KV cache debt.\")\n",
    "\n",
    "\n",
    "for w in [w_users, w_seq, w_layers, w_hidden, w_bytes, w_weights, w_line_a100, w_line_h200]:\n",
    "    w.observe(_render_kv, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([w_users, w_seq]),\n",
    "    widgets.HBox([w_layers, w_hidden, w_bytes]),\n",
    "    widgets.HBox([w_weights, w_line_a100, w_line_h200]),\n",
    "    out_kv,\n",
    "]))\n",
    "\n",
    "_render_kv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FP8 \"magic trick\" (capacity math + optional real run)\n",
    "# We keep this safe: if vLLM isn't installed (or models aren't available), we still teach the concept.\n",
    "\n",
    "\n",
    "def estimate_weight_gib(*, params_b: float, bytes_per: int) -> float:\n",
    "    # Back-of-the-envelope: params * bytes.\n",
    "    total_bytes = float(params_b) * 1e9 * float(bytes_per)\n",
    "    return total_bytes / (1024.0**3)\n",
    "\n",
    "\n",
    "print(\"=== FP16 vs FP8 memory intuition (very rough) ===\")\n",
    "\n",
    "# Example: 70B params (order-of-magnitude). Actual weights depend on architecture + overhead.\n",
    "params_b = 70.0\n",
    "w_fp16 = estimate_weight_gib(params_b=params_b, bytes_per=2)\n",
    "w_fp8 = estimate_weight_gib(params_b=params_b, bytes_per=1)\n",
    "\n",
    "print(f\"Model size (params ~{params_b:.0f}B):\")\n",
    "print(f\"- FP16 weights ~ {w_fp16:.1f} GiB\")\n",
    "print(f\"- FP8  weights ~ {w_fp8:.1f} GiB\")\n",
    "print(\"\\nTakeaway: FP8 can roughly halve weight memory (and often improves throughput on modern tensor cores).\")\n",
    "\n",
    "print(\"\\nOptional: if you have vLLM installed, you can try a real FP16 vs FP8 load/run.\")\n",
    "print(\"(This is optional because large models require access and disk/cache; the math demo above always works.)\")\n",
    "\n",
    "try:\n",
    "    import vllm  # type: ignore\n",
    "\n",
    "    print(\"vLLM detected:\", getattr(vllm, \"__version__\", \"?\"))\n",
    "    print(\"If you want to run vLLM here, consider a smaller model unless you're sure the 70B weights are cached.\")\n",
    "except Exception as e:\n",
    "    print(\"vLLM not available (ok):\", type(e).__name__, str(e)[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput test (NIM): approximate tokens/sec\n",
    "# Note: OpenAI-compatible servers often return usage tokens, but not always. We'll use usage if present, else a crude proxy.\n",
    "\n",
    "\n",
    "def nim_chat_raw(*, prompt: str, max_tokens: int = 128, temperature: float = 0.0, timeout_s: float = 60.0) -> tuple[dict, float]:\n",
    "    url = f\"{NIM_BASE_URL}{NIM_CHAT_PATH}\"\n",
    "    payload = {\n",
    "        \"model\": NIM_GEN_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"temperature\": float(temperature),\n",
    "    }\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(url, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=float(timeout_s))\n",
    "    dt = time.perf_counter() - t0\n",
    "    r.raise_for_status()\n",
    "    return r.json(), float(dt)\n",
    "\n",
    "\n",
    "def _count_tokens_from_usage(j: dict) -> int | None:\n",
    "    u = j.get(\"usage\") or {}\n",
    "    # OpenAI-ish: prompt_tokens / completion_tokens / total_tokens\n",
    "    ct = u.get(\"completion_tokens\")\n",
    "    if isinstance(ct, (int, float)):\n",
    "        return int(ct)\n",
    "    return None\n",
    "\n",
    "\n",
    "PROMPT = \"Explain quantum physics in one sentence.\"  # intentionally small\n",
    "N_REQ = 40\n",
    "MAX_TOKENS = 64\n",
    "\n",
    "lat = []\n",
    "out_tokens = []\n",
    "\n",
    "print(f\"Running {N_REQ} requests...\")\n",
    "for _ in range(N_REQ):\n",
    "    j, dt = nim_chat_raw(prompt=PROMPT, max_tokens=MAX_TOKENS, temperature=0.0, timeout_s=60)\n",
    "    lat.append(float(dt))\n",
    "    t = _count_tokens_from_usage(j)\n",
    "    if t is None:\n",
    "        # crude proxy: words (not real tokens, but directionally useful)\n",
    "        txt = (((j.get(\"choices\") or [{}])[0].get(\"message\") or {}).get(\"content\") or \"\")\n",
    "        t = max(1, len(str(txt).split()))\n",
    "    out_tokens.append(int(t))\n",
    "\n",
    "sec = float(sum(lat))\n",
    "tokens = int(sum(out_tokens))\n",
    "print(\"Total time (s):\", f\"{sec:.2f}\")\n",
    "print(\"Total completion tokens (approx):\", tokens)\n",
    "print(\"Throughput (tokens/s):\", f\"{tokens/sec:.1f}\")\n",
    "\n",
    "# Show p50/p95 just for feel\n",
    "lat_sorted = sorted(lat)\n",
    "print(\"p50 latency (s):\", f\"{lat_sorted[int(0.5*len(lat_sorted))]:.3f}\")\n",
    "print(\"p95 latency (s):\", f\"{lat_sorted[int(0.95*len(lat_sorted))-1]:.3f}\")\n",
    "\n",
    "# Optional: watch GPU while doing a short burst\n",
    "print(\"\\nGPU watch during a short burst (if NVML works):\")\n",
    "df_burst = watch_gpu(seconds=2.5, interval_s=0.25)\n",
    "if not df_burst.empty:\n",
    "    px.line(df_burst, x=\"t_s\", y=[\"vram_used_gib\", \"gpu_util_pct\", \"power_w\"], title=\"GPU during burst\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dollars & cents (toy ROI calculator)\n",
    "# This is deliberately hand-wavy. The purpose is to force the conversation: \"cost per concurrent user\".\n",
    "\n",
    "w_users_a100 = widgets.IntSlider(value=5, min=1, max=100, step=1, description=\"Users on A100/H100\", style={\"description_width\": \"initial\"})\n",
    "w_users_h200 = widgets.IntSlider(value=20, min=1, max=200, step=1, description=\"Users on H200\", style={\"description_width\": \"initial\"})\n",
    "\n",
    "w_price_a100 = widgets.IntSlider(value=12000, min=2000, max=80000, step=500, description=\"Price A100/H100 ($)\", style={\"description_width\": \"initial\"})\n",
    "w_price_h200 = widgets.IntSlider(value=25000, min=5000, max=120000, step=500, description=\"Price H200 ($)\", style={\"description_width\": \"initial\"})\n",
    "\n",
    "out_roi = widgets.Output()\n",
    "\n",
    "\n",
    "def _render_roi(*_):\n",
    "    with out_roi:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        a = float(w_price_a100.value) / max(1, int(w_users_a100.value))\n",
    "        h = float(w_price_h200.value) / max(1, int(w_users_h200.value))\n",
    "\n",
    "        print(\"=== Cost per concurrent user (toy) ===\")\n",
    "        print(f\"A100/H100: ${a:,.0f} per user\")\n",
    "        print(f\"H200:      ${h:,.0f} per user\")\n",
    "\n",
    "        if h < a:\n",
    "            print(\"\\nTakeaway: the expensive GPU can be cheaper per user if it buys enough concurrency.\")\n",
    "        else:\n",
    "            print(\"\\nTakeaway: if concurrency doesn't increase, you're just buying a nicer space heater.\")\n",
    "\n",
    "\n",
    "for w in [w_users_a100, w_users_h200, w_price_a100, w_price_h200]:\n",
    "    w.observe(_render_roi, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([w_users_a100, w_users_h200]), widgets.HBox([w_price_a100, w_price_h200]), out_roi]))\n",
    "\n",
    "_render_roi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_chat_once(*, prompt: str, max_tokens: int = 32, temperature: float = 0.0, timeout_s: float = 60.0) -> tuple[str, float]:\n",
    "    \"\"\"One OpenAI-style chat request to NIM; returns (text, latency_s).\"\"\"\n",
    "    url = f\"{NIM_BASE_URL}{NIM_CHAT_PATH}\"\n",
    "    payload = {\n",
    "        \"model\": NIM_GEN_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"temperature\": float(temperature),\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(url, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=float(timeout_s))\n",
    "    dt = time.perf_counter() - t0\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "\n",
    "    choices = j.get(\"choices\") or []\n",
    "    msg = (choices[0].get(\"message\") if choices else {}) or {}\n",
    "    content = msg.get(\"content\")\n",
    "    if content is None:\n",
    "        content = (choices[0].get(\"text\") if choices else \"\")\n",
    "\n",
    "    return str(content or \"\").strip(), float(dt)\n",
    "\n",
    "\n",
    "def nim_preflight() -> bool:\n",
    "    print(\"\\n=== NIM preflight ===\")\n",
    "    try:\n",
    "        txt, dt = nim_chat_once(prompt=\"Reply with only: OK\", max_tokens=4, temperature=0.0, timeout_s=20)\n",
    "        print(f\"✅ NIM reachable: {dt:.3f}s | sample={txt!r}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ NIM not reachable at {NIM_BASE_URL}: {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"\\nTo start local NIMs:\")\n",
    "        print(\"  cd fico\")\n",
    "        print(\"  export NGC_API_KEY=...   # needed to pull nvcr.io images\")\n",
    "        print(\"  ./scripts/start_nims.sh\")\n",
    "        print(\"\\nThen re-run this cell.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if not nim_preflight():\n",
    "    raise RuntimeError(\"NIM preflight failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental model (pretty short)\n",
    "\n",
    "End-to-end latency is roughly:\n",
    "\n",
    "\\[\n",
    "T = T_{queue} + T_{net} + T_{compute}\n",
    "\\]\n",
    "\n",
    "- Under light load, **p50** is mostly \\(T_{net} + T_{compute}\\).\n",
    "- Under contention, **p95/p99** are dominated by \\(T_{queue}\\) (waiting).\n",
    "\n",
    "A useful intuition (Little’s Law):\n",
    "\n",
    "\\[\n",
    "L = \\lambda W\n",
    "\\]\n",
    "\n",
    "- \\(L\\): average number of requests in the system\n",
    "- \\(\\lambda\\): throughput (req/s)\n",
    "- \\(W\\): time-in-system (seconds)\n",
    "\n",
    "As you push concurrency up, you often increase \\(L\\), which increases \\(W\\) (latency).\n",
    "\n",
    "Your goal is to find the **knee**: throughput stops improving, but p95 explodes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: single-request latency distribution\n",
    "\n",
    "We send the same request repeatedly (sequentially) to estimate the **noise floor** before adding concurrency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct(xs: list[float], p: float) -> float:\n",
    "    if not xs:\n",
    "        return float(\"nan\")\n",
    "    return float(np.percentile(np.array(xs, dtype=np.float64), p))\n",
    "\n",
    "\n",
    "def summarize_latencies(lat_s: list[float]) -> dict[str, float]:\n",
    "    return {\n",
    "        \"n\": float(len(lat_s)),\n",
    "        \"mean_s\": float(np.mean(lat_s)) if lat_s else float(\"nan\"),\n",
    "        \"p50_s\": pct(lat_s, 50),\n",
    "        \"p95_s\": pct(lat_s, 95),\n",
    "        \"p99_s\": pct(lat_s, 99),\n",
    "        \"min_s\": float(min(lat_s)) if lat_s else float(\"nan\"),\n",
    "        \"max_s\": float(max(lat_s)) if lat_s else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "BASE_PROMPT = \"Summarize why batching affects throughput in one short paragraph.\"\n",
    "BASE_MAX_TOKENS = 96\n",
    "BASE_TEMP = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BASELINE = 12\n",
    "\n",
    "lat = []\n",
    "for _ in range(N_BASELINE):\n",
    "    _, dt = nim_chat_once(prompt=BASE_PROMPT, max_tokens=BASE_MAX_TOKENS, temperature=BASE_TEMP, timeout_s=60)\n",
    "    lat.append(float(dt))\n",
    "\n",
    "display(pd.DataFrame([summarize_latencies(lat)]))\n",
    "\n",
    "df_lat = pd.DataFrame({\"latency_s\": lat})\n",
    "\n",
    "fig_h = px.histogram(df_lat, x=\"latency_s\", nbins=12, title=\"Baseline latency histogram\", labels={\"latency_s\": \"seconds\"})\n",
    "fig_h.show()\n",
    "\n",
    "fig_b = px.box(df_lat, y=\"latency_s\", title=\"Baseline latency box plot\", labels={\"latency_s\": \"seconds\"})\n",
    "fig_b.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency sweep: the p95 cliff\n",
    "\n",
    "We’ll run a small load test by issuing many requests while limiting **in-flight concurrency**.\n",
    "\n",
    "What to look for:\n",
    "- Throughput rises, then saturates.\n",
    "- p95/p99 often blow up near saturation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunResult:\n",
    "    ok: bool\n",
    "    latency_s: float\n",
    "    error: str | None = None\n",
    "\n",
    "\n",
    "def _sync_one(timeout_s: float) -> RunResult:\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        nim_chat_once(prompt=BASE_PROMPT, max_tokens=BASE_MAX_TOKENS, temperature=BASE_TEMP, timeout_s=timeout_s)\n",
    "        return RunResult(ok=True, latency_s=float(time.perf_counter() - t0), error=None)\n",
    "    except Exception as e:\n",
    "        return RunResult(ok=False, latency_s=float(time.perf_counter() - t0), error=f\"{type(e).__name__}: {str(e)[:160]}\")\n",
    "\n",
    "\n",
    "def run_load(*, total_requests: int, concurrency: int, timeout_s: float) -> dict[str, Any]:\n",
    "    \"\"\"Threadpool-based loadgen: portable and works in notebooks.\"\"\"\n",
    "    total_requests = int(total_requests)\n",
    "    concurrency = int(concurrency)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    results: list[RunResult] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "        futs = [ex.submit(_sync_one, float(timeout_s)) for _ in range(total_requests)]\n",
    "        for f in as_completed(futs):\n",
    "            results.append(f.result())\n",
    "\n",
    "    wall_s = float(time.perf_counter() - t0)\n",
    "    lat_ok = [r.latency_s for r in results if r.ok]\n",
    "    err = [r for r in results if not r.ok]\n",
    "\n",
    "    throughput = (len(results) / wall_s) if wall_s > 0 else float(\"nan\")\n",
    "    err_rate = (len(err) / len(results)) if results else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"concurrency\": concurrency,\n",
    "        \"total_requests\": total_requests,\n",
    "        \"timeout_s\": float(timeout_s),\n",
    "        \"wall_s\": wall_s,\n",
    "        \"throughput_rps\": float(throughput),\n",
    "        \"error_rate\": float(err_rate),\n",
    "        \"p50_s\": pct(lat_ok, 50),\n",
    "        \"p95_s\": pct(lat_ok, 95),\n",
    "        \"p99_s\": pct(lat_ok, 99),\n",
    "        \"mean_s\": float(np.mean(lat_ok)) if lat_ok else float(\"nan\"),\n",
    "        \"ok\": int(len(lat_ok)),\n",
    "        \"err\": int(len(err)),\n",
    "        \"sample_error\": (err[0].error if err else None),\n",
    "    }\n",
    "\n",
    "\n",
    "CONCURRENCY_LEVELS = [1, 2, 4, 8, 16]\n",
    "TOTAL_REQ = 80\n",
    "TIMEOUT_S = 60.0\n",
    "\n",
    "rows = [run_load(total_requests=TOTAL_REQ, concurrency=c, timeout_s=TIMEOUT_S) for c in CONCURRENCY_LEVELS]\n",
    "df_c = pd.DataFrame(rows).sort_values(\"concurrency\")\n",
    "display(df_c)\n",
    "\n",
    "fig_lat = px.line(\n",
    "    df_c,\n",
    "    x=\"concurrency\",\n",
    "    y=[\"p50_s\", \"p95_s\", \"p99_s\"],\n",
    "    markers=True,\n",
    "    title=\"Latency vs concurrency (p50/p95/p99)\",\n",
    "    labels={\"value\": \"seconds\"},\n",
    ")\n",
    "fig_lat.show()\n",
    "\n",
    "fig_tp = px.line(df_c, x=\"concurrency\", y=\"throughput_rps\", markers=True, title=\"Throughput vs concurrency\", labels={\"throughput_rps\": \"req/s\"})\n",
    "fig_tp.show()\n",
    "\n",
    "fig_err = px.bar(df_c, x=\"concurrency\", y=\"error_rate\", title=\"Error rate vs concurrency\")\n",
    "fig_err.update_yaxes(range=[0, max(0.05, float(df_c[\"error_rate\"].max()) * 1.2 if len(df_c) else 0.1)])\n",
    "fig_err.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token budget: generation dominates\n",
    "\n",
    "Roughly, generation cost scales with:\n",
    "- output tokens (`max_tokens`)\n",
    "- and often input size (prompt length)\n",
    "\n",
    "We’ll vary both and plot how p95 latency responds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(chars: int) -> str:\n",
    "    base = \"Summarize the following text in one sentence.\\n\\n\"\n",
    "    filler = (\"lorem ipsum \") * 5000\n",
    "    return base + filler[: int(chars)]\n",
    "\n",
    "\n",
    "def measure_p95(*, prompt: str, max_tokens: int, n: int = 6) -> dict[str, Any]:\n",
    "    lats = []\n",
    "    for _ in range(int(n)):\n",
    "        _, dt = nim_chat_once(prompt=prompt, max_tokens=int(max_tokens), temperature=0.2, timeout_s=60)\n",
    "        lats.append(float(dt))\n",
    "    return {\n",
    "        \"prompt_chars\": int(len(prompt)),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"p95_s\": pct(lats, 95),\n",
    "        \"p50_s\": pct(lats, 50),\n",
    "    }\n",
    "\n",
    "\n",
    "PROMPT_SIZES = [200, 800, 2000]\n",
    "MAX_TOKENS_GRID = [32, 64, 128, 256]\n",
    "\n",
    "rows = []\n",
    "for pc in PROMPT_SIZES:\n",
    "    p = make_prompt(pc)\n",
    "    for mt in MAX_TOKENS_GRID:\n",
    "        rows.append(measure_p95(prompt=p, max_tokens=mt, n=6))\n",
    "\n",
    "df_tok = pd.DataFrame(rows)\n",
    "display(df_tok)\n",
    "\n",
    "fig_tok = px.line(\n",
    "    df_tok,\n",
    "    x=\"max_tokens\",\n",
    "    y=\"p95_s\",\n",
    "    color=df_tok[\"prompt_chars\"].astype(str),\n",
    "    markers=True,\n",
    "    title=\"p95 latency vs max_tokens (colored by input size)\",\n",
    "    labels={\"p95_s\": \"p95 seconds\", \"color\": \"prompt_chars\"},\n",
    ")\n",
    "fig_tok.show()\n",
    "\n",
    "hm = df_tok.pivot_table(index=\"prompt_chars\", columns=\"max_tokens\", values=\"p95_s\", aggfunc=\"mean\")\n",
    "fig_hm = px.imshow(hm, title=\"Heatmap: p95 latency (seconds)\", labels={\"x\": \"max_tokens\", \"y\": \"prompt_chars\", \"color\": \"p95 seconds\"})\n",
    "fig_hm.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpressure + timeouts (the failure mode)\n",
    "\n",
    "When you overload an inference server:\n",
    "- p95/p99 get worse (queueing)\n",
    "- then requests start timing out / failing\n",
    "\n",
    "A practical rule: **cap concurrency** and set a timeout that matches your SLO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overload by lowering timeout and increasing concurrency\n",
    "\n",
    "OVERLOAD_TIMEOUT_S = 5.0\n",
    "OVERLOAD_TOTAL_REQ = 60\n",
    "OVERLOAD_CONCURRENCY = [4, 8, 16, 32]\n",
    "\n",
    "rows = [run_load(total_requests=OVERLOAD_TOTAL_REQ, concurrency=c, timeout_s=OVERLOAD_TIMEOUT_S) for c in OVERLOAD_CONCURRENCY]\n",
    "df_over = pd.DataFrame(rows).sort_values(\"concurrency\")\n",
    "display(df_over)\n",
    "\n",
    "fig = px.line(\n",
    "    df_over,\n",
    "    x=\"concurrency\",\n",
    "    y=[\"p95_s\", \"error_rate\"],\n",
    "    markers=True,\n",
    "    title=f\"Overload demo (timeout={OVERLOAD_TIMEOUT_S}s)\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical tuning checklist (NIM / TensorRT-style serving)\n",
    "\n",
    "- **Start with a baseline**: measure p50/p95 at concurrency=1.\n",
    "- **Find the knee**: increase concurrency until throughput stops improving.\n",
    "- **Protect tail latency**:\n",
    "  - cap concurrency below the knee\n",
    "  - set timeouts intentionally (match your SLO)\n",
    "- **Control generation cost**:\n",
    "  - reduce `max_tokens`\n",
    "  - keep prompts short; avoid dumping huge context\n",
    "- **Operationally**:\n",
    "  - watch error rate\n",
    "  - avoid blind retries under overload (use backoff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional appendix: mapping the same ideas to vLLM (concepts only)\n",
    "\n",
    "Even though we used NIM here, the same “physics” shows up in any serving stack:\n",
    "- **Batching** improves GPU utilization (higher throughput) but can add queueing delay.\n",
    "- **KV-cache** makes long generations expensive; output tokens dominate cost.\n",
    "- **Scheduling** decisions move you along the latency/throughput frontier.\n",
    "\n",
    "If you later benchmark vLLM, you’ll typically see the same p95 cliff as you approach saturation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fico)",
   "language": "python",
   "name": "fico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
