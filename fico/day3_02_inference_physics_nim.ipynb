{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 — Inference Physics (NIM-first)\n",
    "\n",
    "**Goal:** Build intuition for throughput, tail latency, queueing, and why batching/concurrency matter.\n",
    "\n",
    "**Target:** NIM gateway (OpenAI-compatible) at `NIM_BASE_URL` (default `http://localhost:8000`).\n",
    "\n",
    "**Outputs (what you’ll see):**\n",
    "- Latency distribution (p50/p95/p99)\n",
    "- Latency vs concurrency curves\n",
    "- Throughput vs concurrency curve\n",
    "- Latency vs token budget curve\n",
    "\n",
    "**Timebox:** 60–90 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup + preflight\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "\n",
    "NIM_BASE_URL = os.environ.get(\"NIM_BASE_URL\", \"http://localhost:8000\").rstrip(\"/\")\n",
    "NIM_CHAT_PATH = os.environ.get(\"NIM_CHAT_PATH\", \"/v1/chat/completions\")\n",
    "NIM_GEN_MODEL = os.environ.get(\"NIM_GEN_MODEL\", \"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "print(\"NIM_BASE_URL:\", NIM_BASE_URL)\n",
    "print(\"NIM_CHAT_PATH:\", NIM_CHAT_PATH)\n",
    "print(\"NIM_GEN_MODEL:\", NIM_GEN_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_chat_once(*, prompt: str, max_tokens: int = 32, temperature: float = 0.0, timeout_s: float = 60.0) -> tuple[str, float]:\n",
    "    \"\"\"One OpenAI-style chat request to NIM; returns (text, latency_s).\"\"\"\n",
    "    url = f\"{NIM_BASE_URL}{NIM_CHAT_PATH}\"\n",
    "    payload = {\n",
    "        \"model\": NIM_GEN_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"temperature\": float(temperature),\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(url, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=float(timeout_s))\n",
    "    dt = time.perf_counter() - t0\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "\n",
    "    choices = j.get(\"choices\") or []\n",
    "    msg = (choices[0].get(\"message\") if choices else {}) or {}\n",
    "    content = msg.get(\"content\")\n",
    "    if content is None:\n",
    "        content = (choices[0].get(\"text\") if choices else \"\")\n",
    "\n",
    "    return str(content or \"\").strip(), float(dt)\n",
    "\n",
    "\n",
    "def nim_preflight() -> bool:\n",
    "    print(\"\\n=== NIM preflight ===\")\n",
    "    try:\n",
    "        txt, dt = nim_chat_once(prompt=\"Reply with only: OK\", max_tokens=4, temperature=0.0, timeout_s=20)\n",
    "        print(f\"✅ NIM reachable: {dt:.3f}s | sample={txt!r}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ NIM not reachable at {NIM_BASE_URL}: {type(e).__name__}: {str(e)[:200]}\")\n",
    "        print(\"\\nTo start local NIMs:\")\n",
    "        print(\"  cd fico\")\n",
    "        print(\"  export NGC_API_KEY=...   # needed to pull nvcr.io images\")\n",
    "        print(\"  ./scripts/start_nims.sh\")\n",
    "        print(\"\\nThen re-run this cell.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if not nim_preflight():\n",
    "    raise RuntimeError(\"NIM preflight failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental model (pretty short)\n",
    "\n",
    "End-to-end latency is roughly:\n",
    "\n",
    "\\[\n",
    "T = T_{queue} + T_{net} + T_{compute}\n",
    "\\]\n",
    "\n",
    "- Under light load, **p50** is mostly \\(T_{net} + T_{compute}\\).\n",
    "- Under contention, **p95/p99** are dominated by \\(T_{queue}\\) (waiting).\n",
    "\n",
    "A useful intuition (Little’s Law):\n",
    "\n",
    "\\[\n",
    "L = \\lambda W\n",
    "\\]\n",
    "\n",
    "- \\(L\\): average number of requests in the system\n",
    "- \\(\\lambda\\): throughput (req/s)\n",
    "- \\(W\\): time-in-system (seconds)\n",
    "\n",
    "As you push concurrency up, you often increase \\(L\\), which increases \\(W\\) (latency).\n",
    "\n",
    "Your goal is to find the **knee**: throughput stops improving, but p95 explodes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: single-request latency distribution\n",
    "\n",
    "We send the same request repeatedly (sequentially) to estimate the **noise floor** before adding concurrency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct(xs: list[float], p: float) -> float:\n",
    "    if not xs:\n",
    "        return float(\"nan\")\n",
    "    return float(np.percentile(np.array(xs, dtype=np.float64), p))\n",
    "\n",
    "\n",
    "def summarize_latencies(lat_s: list[float]) -> dict[str, float]:\n",
    "    return {\n",
    "        \"n\": float(len(lat_s)),\n",
    "        \"mean_s\": float(np.mean(lat_s)) if lat_s else float(\"nan\"),\n",
    "        \"p50_s\": pct(lat_s, 50),\n",
    "        \"p95_s\": pct(lat_s, 95),\n",
    "        \"p99_s\": pct(lat_s, 99),\n",
    "        \"min_s\": float(min(lat_s)) if lat_s else float(\"nan\"),\n",
    "        \"max_s\": float(max(lat_s)) if lat_s else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "BASE_PROMPT = \"Summarize why batching affects throughput in one short paragraph.\"\n",
    "BASE_MAX_TOKENS = 96\n",
    "BASE_TEMP = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BASELINE = 12\n",
    "\n",
    "lat = []\n",
    "for _ in range(N_BASELINE):\n",
    "    _, dt = nim_chat_once(prompt=BASE_PROMPT, max_tokens=BASE_MAX_TOKENS, temperature=BASE_TEMP, timeout_s=60)\n",
    "    lat.append(float(dt))\n",
    "\n",
    "display(pd.DataFrame([summarize_latencies(lat)]))\n",
    "\n",
    "df_lat = pd.DataFrame({\"latency_s\": lat})\n",
    "\n",
    "fig_h = px.histogram(df_lat, x=\"latency_s\", nbins=12, title=\"Baseline latency histogram\", labels={\"latency_s\": \"seconds\"})\n",
    "fig_h.show()\n",
    "\n",
    "fig_b = px.box(df_lat, y=\"latency_s\", title=\"Baseline latency box plot\", labels={\"latency_s\": \"seconds\"})\n",
    "fig_b.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency sweep: the p95 cliff\n",
    "\n",
    "We’ll run a small load test by issuing many requests while limiting **in-flight concurrency**.\n",
    "\n",
    "What to look for:\n",
    "- Throughput rises, then saturates.\n",
    "- p95/p99 often blow up near saturation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunResult:\n",
    "    ok: bool\n",
    "    latency_s: float\n",
    "    error: str | None = None\n",
    "\n",
    "\n",
    "def _sync_one(timeout_s: float) -> RunResult:\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        nim_chat_once(prompt=BASE_PROMPT, max_tokens=BASE_MAX_TOKENS, temperature=BASE_TEMP, timeout_s=timeout_s)\n",
    "        return RunResult(ok=True, latency_s=float(time.perf_counter() - t0), error=None)\n",
    "    except Exception as e:\n",
    "        return RunResult(ok=False, latency_s=float(time.perf_counter() - t0), error=f\"{type(e).__name__}: {str(e)[:160]}\")\n",
    "\n",
    "\n",
    "def run_load(*, total_requests: int, concurrency: int, timeout_s: float) -> dict[str, Any]:\n",
    "    \"\"\"Threadpool-based loadgen: portable and works in notebooks.\"\"\"\n",
    "    total_requests = int(total_requests)\n",
    "    concurrency = int(concurrency)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    results: list[RunResult] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "        futs = [ex.submit(_sync_one, float(timeout_s)) for _ in range(total_requests)]\n",
    "        for f in as_completed(futs):\n",
    "            results.append(f.result())\n",
    "\n",
    "    wall_s = float(time.perf_counter() - t0)\n",
    "    lat_ok = [r.latency_s for r in results if r.ok]\n",
    "    err = [r for r in results if not r.ok]\n",
    "\n",
    "    throughput = (len(results) / wall_s) if wall_s > 0 else float(\"nan\")\n",
    "    err_rate = (len(err) / len(results)) if results else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"concurrency\": concurrency,\n",
    "        \"total_requests\": total_requests,\n",
    "        \"timeout_s\": float(timeout_s),\n",
    "        \"wall_s\": wall_s,\n",
    "        \"throughput_rps\": float(throughput),\n",
    "        \"error_rate\": float(err_rate),\n",
    "        \"p50_s\": pct(lat_ok, 50),\n",
    "        \"p95_s\": pct(lat_ok, 95),\n",
    "        \"p99_s\": pct(lat_ok, 99),\n",
    "        \"mean_s\": float(np.mean(lat_ok)) if lat_ok else float(\"nan\"),\n",
    "        \"ok\": int(len(lat_ok)),\n",
    "        \"err\": int(len(err)),\n",
    "        \"sample_error\": (err[0].error if err else None),\n",
    "    }\n",
    "\n",
    "\n",
    "CONCURRENCY_LEVELS = [1, 2, 4, 8, 16]\n",
    "TOTAL_REQ = 80\n",
    "TIMEOUT_S = 60.0\n",
    "\n",
    "rows = [run_load(total_requests=TOTAL_REQ, concurrency=c, timeout_s=TIMEOUT_S) for c in CONCURRENCY_LEVELS]\n",
    "df_c = pd.DataFrame(rows).sort_values(\"concurrency\")\n",
    "display(df_c)\n",
    "\n",
    "fig_lat = px.line(\n",
    "    df_c,\n",
    "    x=\"concurrency\",\n",
    "    y=[\"p50_s\", \"p95_s\", \"p99_s\"],\n",
    "    markers=True,\n",
    "    title=\"Latency vs concurrency (p50/p95/p99)\",\n",
    "    labels={\"value\": \"seconds\"},\n",
    ")\n",
    "fig_lat.show()\n",
    "\n",
    "fig_tp = px.line(df_c, x=\"concurrency\", y=\"throughput_rps\", markers=True, title=\"Throughput vs concurrency\", labels={\"throughput_rps\": \"req/s\"})\n",
    "fig_tp.show()\n",
    "\n",
    "fig_err = px.bar(df_c, x=\"concurrency\", y=\"error_rate\", title=\"Error rate vs concurrency\")\n",
    "fig_err.update_yaxes(range=[0, max(0.05, float(df_c[\"error_rate\"].max()) * 1.2 if len(df_c) else 0.1)])\n",
    "fig_err.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token budget: generation dominates\n",
    "\n",
    "Roughly, generation cost scales with:\n",
    "- output tokens (`max_tokens`)\n",
    "- and often input size (prompt length)\n",
    "\n",
    "We’ll vary both and plot how p95 latency responds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(chars: int) -> str:\n",
    "    base = \"Summarize the following text in one sentence.\\n\\n\"\n",
    "    filler = (\"lorem ipsum \") * 5000\n",
    "    return base + filler[: int(chars)]\n",
    "\n",
    "\n",
    "def measure_p95(*, prompt: str, max_tokens: int, n: int = 6) -> dict[str, Any]:\n",
    "    lats = []\n",
    "    for _ in range(int(n)):\n",
    "        _, dt = nim_chat_once(prompt=prompt, max_tokens=int(max_tokens), temperature=0.2, timeout_s=60)\n",
    "        lats.append(float(dt))\n",
    "    return {\n",
    "        \"prompt_chars\": int(len(prompt)),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"p95_s\": pct(lats, 95),\n",
    "        \"p50_s\": pct(lats, 50),\n",
    "    }\n",
    "\n",
    "\n",
    "PROMPT_SIZES = [200, 800, 2000]\n",
    "MAX_TOKENS_GRID = [32, 64, 128, 256]\n",
    "\n",
    "rows = []\n",
    "for pc in PROMPT_SIZES:\n",
    "    p = make_prompt(pc)\n",
    "    for mt in MAX_TOKENS_GRID:\n",
    "        rows.append(measure_p95(prompt=p, max_tokens=mt, n=6))\n",
    "\n",
    "df_tok = pd.DataFrame(rows)\n",
    "display(df_tok)\n",
    "\n",
    "fig_tok = px.line(\n",
    "    df_tok,\n",
    "    x=\"max_tokens\",\n",
    "    y=\"p95_s\",\n",
    "    color=df_tok[\"prompt_chars\"].astype(str),\n",
    "    markers=True,\n",
    "    title=\"p95 latency vs max_tokens (colored by input size)\",\n",
    "    labels={\"p95_s\": \"p95 seconds\", \"color\": \"prompt_chars\"},\n",
    ")\n",
    "fig_tok.show()\n",
    "\n",
    "hm = df_tok.pivot_table(index=\"prompt_chars\", columns=\"max_tokens\", values=\"p95_s\", aggfunc=\"mean\")\n",
    "fig_hm = px.imshow(hm, title=\"Heatmap: p95 latency (seconds)\", labels={\"x\": \"max_tokens\", \"y\": \"prompt_chars\", \"color\": \"p95 seconds\"})\n",
    "fig_hm.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpressure + timeouts (the failure mode)\n",
    "\n",
    "When you overload an inference server:\n",
    "- p95/p99 get worse (queueing)\n",
    "- then requests start timing out / failing\n",
    "\n",
    "A practical rule: **cap concurrency** and set a timeout that matches your SLO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overload by lowering timeout and increasing concurrency\n",
    "\n",
    "OVERLOAD_TIMEOUT_S = 5.0\n",
    "OVERLOAD_TOTAL_REQ = 60\n",
    "OVERLOAD_CONCURRENCY = [4, 8, 16, 32]\n",
    "\n",
    "rows = [run_load(total_requests=OVERLOAD_TOTAL_REQ, concurrency=c, timeout_s=OVERLOAD_TIMEOUT_S) for c in OVERLOAD_CONCURRENCY]\n",
    "df_over = pd.DataFrame(rows).sort_values(\"concurrency\")\n",
    "display(df_over)\n",
    "\n",
    "fig = px.line(\n",
    "    df_over,\n",
    "    x=\"concurrency\",\n",
    "    y=[\"p95_s\", \"error_rate\"],\n",
    "    markers=True,\n",
    "    title=f\"Overload demo (timeout={OVERLOAD_TIMEOUT_S}s)\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical tuning checklist (NIM / TensorRT-style serving)\n",
    "\n",
    "- **Start with a baseline**: measure p50/p95 at concurrency=1.\n",
    "- **Find the knee**: increase concurrency until throughput stops improving.\n",
    "- **Protect tail latency**:\n",
    "  - cap concurrency below the knee\n",
    "  - set timeouts intentionally (match your SLO)\n",
    "- **Control generation cost**:\n",
    "  - reduce `max_tokens`\n",
    "  - keep prompts short; avoid dumping huge context\n",
    "- **Operationally**:\n",
    "  - watch error rate\n",
    "  - avoid blind retries under overload (use backoff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional appendix: mapping the same ideas to vLLM (concepts only)\n",
    "\n",
    "Even though we used NIM here, the same “physics” shows up in any serving stack:\n",
    "- **Batching** improves GPU utilization (higher throughput) but can add queueing delay.\n",
    "- **KV-cache** makes long generations expensive; output tokens dominate cost.\n",
    "- **Scheduling** decisions move you along the latency/throughput frontier.\n",
    "\n",
    "If you later benchmark vLLM, you’ll typically see the same p95 cliff as you approach saturation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fico)",
   "language": "python",
   "name": "fico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
